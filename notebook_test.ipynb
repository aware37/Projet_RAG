{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6ca9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: docling in ./venv/lib/python3.10/site-packages (2.68.0)\n",
      "Requirement already satisfied: langchain in ./venv/lib/python3.10/site-packages (1.2.0)\n",
      "Requirement already satisfied: langchain-community in ./venv/lib/python3.10/site-packages (0.4.1)\n",
      "Requirement already satisfied: langchain-openai in ./venv/lib/python3.10/site-packages (1.1.6)\n",
      "Requirement already satisfied: faiss-cpu in ./venv/lib/python3.10/site-packages (1.13.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in ./venv/lib/python3.10/site-packages (from docling) (2.12.5)\n",
      "Requirement already satisfied: docling-core<3.0.0,>=2.50.1 in ./venv/lib/python3.10/site-packages (from docling-core[chunking]<3.0.0,>=2.50.1->docling) (2.59.0)\n",
      "Requirement already satisfied: docling-parse<5.0.0,>=4.7.0 in ./venv/lib/python3.10/site-packages (from docling) (4.7.3)\n",
      "Requirement already satisfied: docling-ibm-models<4,>=3.9.1 in ./venv/lib/python3.10/site-packages (from docling) (3.10.3)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in ./venv/lib/python3.10/site-packages (from docling) (1.2.0)\n",
      "Requirement already satisfied: pypdfium2!=4.30.1,<5.0.0,>=4.30.0 in ./venv/lib/python3.10/site-packages (from docling) (4.30.0)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.3.0 in ./venv/lib/python3.10/site-packages (from docling) (2.12.0)\n",
      "Requirement already satisfied: huggingface_hub<1,>=0.23 in ./venv/lib/python3.10/site-packages (from docling) (0.36.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.2 in ./venv/lib/python3.10/site-packages (from docling) (2.32.5)\n",
      "Requirement already satisfied: rapidocr<4.0.0,>=3.3 in ./venv/lib/python3.10/site-packages (from docling) (3.5.0)\n",
      "Requirement already satisfied: certifi>=2024.7.4 in ./venv/lib/python3.10/site-packages (from docling) (2025.11.12)\n",
      "Requirement already satisfied: rtree<2.0.0,>=1.3.0 in ./venv/lib/python3.10/site-packages (from docling) (1.4.1)\n",
      "Requirement already satisfied: typer<0.20.0,>=0.12.5 in ./venv/lib/python3.10/site-packages (from docling) (0.19.2)\n",
      "Requirement already satisfied: python-docx<2.0.0,>=1.1.2 in ./venv/lib/python3.10/site-packages (from docling) (1.2.0)\n",
      "Requirement already satisfied: python-pptx<2.0.0,>=1.0.2 in ./venv/lib/python3.10/site-packages (from docling) (1.0.2)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in ./venv/lib/python3.10/site-packages (from docling) (4.14.3)\n",
      "Requirement already satisfied: pandas<3.0.0,>=2.1.4 in ./venv/lib/python3.10/site-packages (from docling) (2.3.3)\n",
      "Requirement already satisfied: marko<3.0.0,>=2.1.2 in ./venv/lib/python3.10/site-packages (from docling) (2.2.2)\n",
      "Requirement already satisfied: openpyxl<4.0.0,>=3.1.5 in ./venv/lib/python3.10/site-packages (from docling) (3.1.5)\n",
      "Requirement already satisfied: lxml<7.0.0,>=4.0.0 in ./venv/lib/python3.10/site-packages (from docling) (6.0.2)\n",
      "Requirement already satisfied: pillow<12.0.0,>=10.0.0 in ./venv/lib/python3.10/site-packages (from docling) (11.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.65.0 in ./venv/lib/python3.10/site-packages (from docling) (4.67.1)\n",
      "Requirement already satisfied: pluggy<2.0.0,>=1.0.0 in ./venv/lib/python3.10/site-packages (from docling) (1.6.0)\n",
      "Requirement already satisfied: pylatexenc<3.0,>=2.10 in ./venv/lib/python3.10/site-packages (from docling) (2.10)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.6.0 in ./venv/lib/python3.10/site-packages (from docling) (1.15.3)\n",
      "Requirement already satisfied: accelerate<2,>=1.0.0 in ./venv/lib/python3.10/site-packages (from docling) (1.12.0)\n",
      "Requirement already satisfied: polyfactory>=2.22.2 in ./venv/lib/python3.10/site-packages (from docling) (3.2.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.10/site-packages (from accelerate<2,>=1.0.0->docling) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.10/site-packages (from accelerate<2,>=1.0.0->docling) (25.0)\n",
      "Requirement already satisfied: psutil in ./venv/lib/python3.10/site-packages (from accelerate<2,>=1.0.0->docling) (7.2.1)\n",
      "Requirement already satisfied: pyyaml in ./venv/lib/python3.10/site-packages (from accelerate<2,>=1.0.0->docling) (6.0.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./venv/lib/python3.10/site-packages (from accelerate<2,>=1.0.0->docling) (2.9.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./venv/lib/python3.10/site-packages (from accelerate<2,>=1.0.0->docling) (0.7.0)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in ./venv/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling) (2.8.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in ./venv/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling) (4.15.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.16.0 in ./venv/lib/python3.10/site-packages (from docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling) (4.25.1)\n",
      "Requirement already satisfied: jsonref<2.0.0,>=1.1.0 in ./venv/lib/python3.10/site-packages (from docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling) (1.1.0)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in ./venv/lib/python3.10/site-packages (from docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.9.0)\n",
      "Requirement already satisfied: latex2mathml<4.0.0,>=3.77.0 in ./venv/lib/python3.10/site-packages (from docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling) (3.78.1)\n",
      "Requirement already satisfied: semchunk<3.0.0,>=2.2.0 in ./venv/lib/python3.10/site-packages (from docling-core[chunking]<3.0.0,>=2.50.1->docling) (2.2.2)\n",
      "Requirement already satisfied: tree-sitter<1.0.0,>=0.23.2 in ./venv/lib/python3.10/site-packages (from docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.25.2)\n",
      "Requirement already satisfied: tree-sitter-python<1.0.0,>=0.23.6 in ./venv/lib/python3.10/site-packages (from docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.25.0)\n",
      "Requirement already satisfied: tree-sitter-c<1.0.0,>=0.23.4 in ./venv/lib/python3.10/site-packages (from docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.24.1)\n",
      "Requirement already satisfied: tree-sitter-javascript<1.0.0,>=0.23.1 in ./venv/lib/python3.10/site-packages (from docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.25.0)\n",
      "Requirement already satisfied: tree-sitter-typescript<1.0.0,>=0.23.2 in ./venv/lib/python3.10/site-packages (from docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.23.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in ./venv/lib/python3.10/site-packages (from docling-core[chunking]<3.0.0,>=2.50.1->docling) (4.57.3)\n",
      "Requirement already satisfied: torchvision<1,>=0 in ./venv/lib/python3.10/site-packages (from docling-ibm-models<4,>=3.9.1->docling) (0.24.1)\n",
      "Requirement already satisfied: jsonlines<5.0.0,>=3.1.0 in ./venv/lib/python3.10/site-packages (from docling-ibm-models<4,>=3.9.1->docling) (4.0.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from huggingface_hub<1,>=0.23->docling) (3.20.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.10/site-packages (from huggingface_hub<1,>=0.23->docling) (2025.12.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./venv/lib/python3.10/site-packages (from huggingface_hub<1,>=0.23->docling) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in ./venv/lib/python3.10/site-packages (from jsonlines<5.0.0,>=3.1.0->docling-ibm-models<4,>=3.9.1->docling) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./venv/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./venv/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./venv/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.30.0)\n",
      "Requirement already satisfied: et-xmlfile in ./venv/lib/python3.10/site-packages (from openpyxl<4.0.0,>=3.1.5->docling) (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.10/site-packages (from pandas<3.0.0,>=2.1.4->docling) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.10/site-packages (from pandas<3.0.0,>=2.1.4->docling) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.10/site-packages (from pandas<3.0.0,>=2.1.4->docling) (2025.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.0.0->docling) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in ./venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.0.0->docling) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in ./venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.0.0->docling) (0.4.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in ./venv/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.3.0->docling) (1.2.1)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in ./venv/lib/python3.10/site-packages (from python-pptx<2.0.0,>=1.0.2->docling) (3.2.9)\n",
      "Requirement already satisfied: pyclipper>=1.2.0 in ./venv/lib/python3.10/site-packages (from rapidocr<4.0.0,>=3.3->docling) (1.4.0)\n",
      "Requirement already satisfied: opencv_python>=4.5.1.48 in ./venv/lib/python3.10/site-packages (from rapidocr<4.0.0,>=3.3->docling) (4.12.0.88)\n",
      "Requirement already satisfied: six>=1.15.0 in ./venv/lib/python3.10/site-packages (from rapidocr<4.0.0,>=3.3->docling) (1.17.0)\n",
      "Requirement already satisfied: Shapely!=2.0.4,>=1.7.1 in ./venv/lib/python3.10/site-packages (from rapidocr<4.0.0,>=3.3->docling) (2.1.2)\n",
      "Requirement already satisfied: omegaconf in ./venv/lib/python3.10/site-packages (from rapidocr<4.0.0,>=3.3->docling) (2.3.0)\n",
      "Requirement already satisfied: colorlog in ./venv/lib/python3.10/site-packages (from rapidocr<4.0.0,>=3.3->docling) (6.10.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.32.2->docling) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.32.2->docling) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.32.2->docling) (2.3.0)\n",
      "Requirement already satisfied: mpire[dill] in ./venv/lib/python3.10/site-packages (from semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.50.1->docling) (2.10.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in ./venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in ./venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.5.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.50.1->docling) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.22.1)\n",
      "Requirement already satisfied: click>=8.0.0 in ./venv/lib/python3.10/site-packages (from typer<0.20.0,>=0.12.5->docling) (8.3.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./venv/lib/python3.10/site-packages (from typer<0.20.0,>=0.12.5->docling) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./venv/lib/python3.10/site-packages (from typer<0.20.0,>=0.12.5->docling) (14.2.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.2.1 in ./venv/lib/python3.10/site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in ./venv/lib/python3.10/site-packages (from langchain) (1.0.5)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in ./venv/lib/python3.10/site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in ./venv/lib/python3.10/site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.5.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./venv/lib/python3.10/site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (9.1.2)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in ./venv/lib/python3.10/site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.12.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./venv/lib/python3.10/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain) (3.0.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in ./venv/lib/python3.10/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in ./venv/lib/python3.10/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
      "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in ./venv/lib/python3.10/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.3.1)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in ./venv/lib/python3.10/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in ./venv/lib/python3.10/site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.1)\n",
      "Requirement already satisfied: httpx>=0.25.2 in ./venv/lib/python3.10/site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in ./venv/lib/python3.10/site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in ./venv/lib/python3.10/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in ./venv/lib/python3.10/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (0.25.0)\n",
      "Requirement already satisfied: anyio in ./venv/lib/python3.10/site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (4.12.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.10/site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in ./venv/lib/python3.10/site-packages (from langchain-community) (1.0.0)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in ./venv/lib/python3.10/site-packages (from langchain-community) (2.0.45)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./venv/lib/python3.10/site-packages (from langchain-community) (3.13.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in ./venv/lib/python3.10/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in ./venv/lib/python3.10/site-packages (from langchain-community) (0.4.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./venv/lib/python3.10/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./venv/lib/python3.10/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in ./venv/lib/python3.10/site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: greenlet>=1 in ./venv/lib/python3.10/site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./venv/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: openai<3.0.0,>=1.109.1 in ./venv/lib/python3.10/site-packages (from langchain-openai) (2.14.0)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in ./venv/lib/python3.10/site-packages (from langchain-openai) (0.12.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./venv/lib/python3.10/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in ./venv/lib/python3.10/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.12.0)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.10/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./venv/lib/python3.10/site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (1.3.1)\n",
      "Requirement already satisfied: faker>=5.0.0 in ./venv/lib/python3.10/site-packages (from polyfactory>=2.22.2->docling) (40.1.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./venv/lib/python3.10/site-packages (from rich>=10.11.0->typer<0.20.0,>=0.12.5->docling) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/lib/python3.10/site-packages (from rich>=10.11.0->typer<0.20.0,>=0.12.5->docling) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<0.20.0,>=0.12.5->docling) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate<2,>=1.0.0->docling) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.0.3)\n",
      "Requirement already satisfied: multiprocess in ./venv/lib/python3.10/site-packages (from mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.70.18)\n",
      "Requirement already satisfied: dill>=0.4.0 in ./venv/lib/python3.10/site-packages (from multiprocess->mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.4.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in ./venv/lib/python3.10/site-packages (from omegaconf->rapidocr<4.0.0,>=3.3->docling) (4.9.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install docling langchain langchain-community langchain-openai faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86395a8d",
   "metadata": {},
   "source": [
    "## Fonctions Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2b0995c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cytech/Ing3/NLP/Projet_RAG/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter, Language\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.pipeline.vlm_pipeline import VlmPipeline\n",
    "from docling.datamodel.pipeline_options import VlmPipelineOptions\n",
    "from docling.datamodel.pipeline_options_vlm_model import ApiVlmOptions, ResponseFormat\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv(\"LLAMA_CLOUD_API_KEY\")\n",
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = api_key\n",
    "\n",
    "SERVER = \"http://localhost:11434\"\n",
    "MODEL = \"mistral\"\n",
    "#VISION_MODEL = \"llava\"\n",
    "EMBEDDING_MODEL = \"nomic-embed-text\"\n",
    "DB_PATH = \"./db_local\"\n",
    "\n",
    "def query_ollama(model: str, prompt: str) -> str:\n",
    "    \"\"\"Envoie une requête à Ollama et récupère la réponse\"\"\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"stream\": False\n",
    "    }\n",
    "    try:\n",
    "        r = requests.post(f\"{SERVER}/api/chat\", json=payload)\n",
    "        r.raise_for_status()\n",
    "        response = r.json()\n",
    "        return response.get(\"message\", {}).get(\"content\", \"\").strip()\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"Erreur : Ollama n'est pas lancé\")\n",
    "        return \"\"\n",
    "    \n",
    "def ollama_vlm_options(model: str, prompt: str):\n",
    "    options = ApiVlmOptions(\n",
    "        url=\"http://localhost:11434/v1/chat/completions\",\n",
    "        params=dict(\n",
    "            model=model,\n",
    "        ),\n",
    "        prompt=prompt,\n",
    "        timeout=120,\n",
    "        scale=1.0,\n",
    "        response_format=ResponseFormat.MARKDOWN,\n",
    "    )\n",
    "    return options\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a366b1",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eab8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_parse import LlamaParse\n",
    "\n",
    "def step1_llamaparse(pdf_path, output_md_path):\n",
    "    print(f\"Envoi du fichier {pdf_path} à LlamaParse...\")\n",
    "    \n",
    "    # Multimodal pour décrive images\n",
    "    parser = LlamaParse(\n",
    "        result_type=\"markdown\",\n",
    "        premium_mode=True,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Conversion PDF -> Markdown\n",
    "    documents = parser.load_data(pdf_path)\n",
    "    \n",
    "    # LlamaParse renvoie une liste de documents\n",
    "    full_markdown = \"\\n\\n\".join([doc.text for doc in documents])\n",
    "    \n",
    "    # Sauvegarde\n",
    "    os.makedirs(os.path.dirname(output_md_path), exist_ok=True)\n",
    "    with open(output_md_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(full_markdown)\n",
    "        \n",
    "    print(f\"Terminé ! Markdown sauvegardé dans : {output_md_path}\")\n",
    "    # Apreçu du markdown\n",
    "    print(full_markdown[:500])\n",
    "    return full_markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaf5d337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Envoi du fichier data/test/raw/2512.02906v2.pdf à LlamaParse...\n",
      "Started parsing the file under job_id 770f1b0a-9ec9-424d-9c65-920b996bbc50\n",
      "Terminé ! Markdown sauvegardé dans : data/test/processed/document.md\n",
      "\n",
      "# MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding\n",
      "\n",
      "| Fan Yang<br/>Harbin Institute of Technology (Shenzhen)<br/>25b951055\\@stu.hit.edu.cn | Kaihao Zhang<br/>Harbin Institute of Technology (Shenzhen)<br/>super.khzhang\\@gmail.com |\n",
      "| ------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------- |\n",
      "\n",
      "\n",
      "## Abstract\n",
      "\n",
      "*Understanding high-resolution ima\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding\\n\\n| Fan Yang<br/>Harbin Institute of Technology (Shenzhen)<br/>25b951055\\\\@stu.hit.edu.cn | Kaihao Zhang<br/>Harbin Institute of Technology (Shenzhen)<br/>super.khzhang\\\\@gmail.com |\\n| ------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------- |\\n\\n\\n## Abstract\\n\\n*Understanding high-resolution images remains a significant challenge for multimodal large language models (MLLMs). Recent study address this issue by dividing the image into smaller crops and computing the semantic similarity between each crop and a query using a pretrained retrieval-augmented generation (RAG) model. The most relevant crops are then selected to localize the target object and suppress irrelevant information. However, such crop-based processing can fragment complete objects across multiple crops, thereby disrupting the computation of semantic similarity. In our experiments, we find that image crops of objects with different sizes are better handled at different resolutions. Based on this observation, we propose Multi-resolution Retrieval-Detection (MRD), a trainingfree framework for high-resolution image understanding. To address the issue of semantic similarity bias caused by objects being split across different image crops, we propose a multi-resolution semantic fusion method, which integrates semantic similarity maps obtained at different resolutions to produce more accurate semantic information and preserve the integrity of target objects. Furthermore, to achieve direct localization of target objects at a global scale, we introduce an open-vocabulary object detection (OVD) model that identifies object regions using a sliding-window approach.Experiments on high-resolution image understanding benchmarks using different MLLMs demonstrate the effectiveness of our approach.*\\n\\n## 1. Introduction\\n\\nMultimodal Large Language Models (MLLMs) have demonstrated significant advancements in integrating and interpreting visual and linguistic information, enabling robust capabilities in vision-language understanding, reasoning, and interactive tasks [25]. By leveraging visual signals, these models can process and decipher complex visual information, forming a bridge between pixel-level data and\\n\\n| **Query: Is the red vehicle on the left or right side of the image?**<br/>**High-Resolution Image** |                                                                          | **Semantic Map**<br/>**RAG** |                   |\\n| --------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------ | ---------------------------- | ----------------- |\\n| MLLM                                                                                                | OVD                                                                      |                              | **Detection Map** |\\n|                                                                                                     | The red vehicle is on the<br/>right side of the image.<br/>**RE-Search** |                              |                   |\\n\\n\\nFigure 1. Overview of the proposed Multi-resolution Retrieval-Detection framework, which uses RAG and OVD to obtain semantic similarity map and detection confidence map respectively. By integrating the two, the target objects can be localized more accurately.\\n\\nsemantic interpretation [17, 20]. However, a common practice among most MLLMs is to process input images at fixed and pre-defined resolutions [1, 13, 14]. While this uniform input pipeline simplifies model architecture and reduces computational overhead, it introduces substantial limitations. Specifically, resizing high-resolution (HR) real-world images to a fixed low resolution often leads to shape distortion and blurring, which degrades the quality of fine-grained visual details. Recent studies [11, 19, 22, 24, 29, 30] indicate that existing methods remain unsatisfactory for highresolution image tasks. This is clearly demonstrated by their suboptimal results on dedicated high-resolution image understanding benchmarks [21, 24].\\n\\nTo address this limitation, improving the high-resolution image perception capability of MLLMs has become an emerging research focus. A common \"locate-and-zoomin\" strategy is widely adopted to enhance detail perception in models. Although training-based approaches such as Supervised Fine-Tuning (SFT) [18] and Reinforcement\\n\\n\\n\\n\\nLearning (RL) [30] can effectively identify relevant regions, they are hampered by critical limitations, including high computational costs, long training cycles, and poor cross-architecture transferability, which curtails their scalability and practical application. In contrast, training-free methods [11, 19, 29] automatically locate regions by using attention mechanisms or tree-based search, without requiring the construction of the dataset or the fine-tuning of the model. Despite employing a top-down search strategy from high to low resolution, these methods face significant limitations. A primary issue is the model\\'s inadequate perception of small objects during the initial search stage [19, 21], which frequently leads to the generation of erroneous search paths.\\n\\nRecently, Inspired by the success of Retrieval-Augmented Generation (RAG) for enabling long-context understanding in general LLMs [9], Wang et al. [22] introduced Retrieval-Augmented Perception (RAP) — a training-free framework designed to enhance MLLMs\\' perception of high-resolution images. RAP extends this paradigm to the visual domain, achieving significant performance improvement on high-resolution benchmarks. The RAP framework consists of three key components: First, the Visual Retrieval module employs a pre-trained vision RAG model, VisRAG, to compute semantic similarity between the query and different image regions (crops), retrieving the most relevant ones to reduce noise. Next, the Spatial-Awareness Layout module preserves the original relative spatial relationships among the retrieved crops when composing them into the model input, maintaining spatial coherence. Finally, the Adaptive Retrieval-Exploration Search (RE-Search) module dynamically determines the optimal number of retrieved crops by constructing a Retrieval-Exploration Tree (RE-Tree), balancing information sufficiency with computational efficiency.\\n\\nDespite its promise, the RAP method suffers from several inherent limitations. First, the patching operation can fragment large objects across multiple disjointed crops, disrupting their holistic semantics and leading to biased similarity calculations. Our empirical observations confirm that some patches semantically irrelevant to the query can obtain abnormally high similarity scores. Second, the patch resolution is a critical yet difficult-to-tune hyperparameter: overly large patches introduce redundant background information, while overly small ones exacerbate object fragmentation. Experiments show that the choice of resolution significantly impacts performance. Third, in high-resolution images with cluttered backgrounds, the similarity measure is prone to false positives, where background regions may attain higher similarity than those containing the actual target objects, severely hampering recognition.\\n\\nTo tackle these challenges, we propose a novel Multi-resolution Retrieval-Detection (MRD) framework, based on RAP to improve retrieval quality and localization accuracy through two key techniques:\\n\\n• Multi-resolution Semantic Fusion: To mitigate the bias inherent in single-resolution patching, we design a simple yet effective fusion strategy. It computes semantic similarities across multiple proportional resolutions and performs consistency-based fusion to calibrate the results, yielding a more robust and accurate relevance estimation that alleviates semantic deviations caused by object fragmentation.\\n\\n• Open-vocabulary Detector Enhancement: For more precise target localization, we incorporate an advanced open-vocabulary object detector, LLMDet [7]. First, we leverage the in-context learning capability of LLMs to extract target concepts from the query, defining the categories for the detector. Subsequently, a sliding window mechanism is employed to traverse the entire high-resolution image, detecting target objects within each window to generate a confidence map indicating target presence.\\n\\nFinally, the calibrated multi-resolution semantic similarity is augmented by the object detection confidence. This synergistic fusion effectively amplifies the response in true target regions, enabling faster and more accurate localization of critical areas during subsequent retrieval, thereby guiding the MLLM toward more reliable inference.\\n\\nWe conduct extensive experiments on several high-resolution benchmarks, including V* [24], HRBench-4K, and HRBench-8K [21], utilizing various MLLMs such as LLaVA-ov and LLaVA-v1.5. The results demonstrate that our MRD framework surpasses all existing training-free methods and achieves state-of-the-art performance on both single-object and multi-object retrieval and recognition tasks, with particularly notable gains on single-object tasks.\\n\\nOur contributions are summarized as follows:\\n\\n• To the best of our knowledge, this is the first work that systematically leverages an open-vocabulary object detector to enhance MLLMs\\' understanding of high-resolution images. Experiments validate that the detector provides precise target localization, effectively suppressing interference from irrelevant regions.\\n\\n• We propose MRD, a training-free and generic framework. It innovatively corrects semantic similarity via a multi-resolution fusion strategy and integrates open-set detection results to enhance target regions, creating a synergistic effect.\\n\\n• Comprehensive experiments validate the effectiveness and generalization of our method. It achieves leading performance on both single-object and multi-object tasks across different MLLMs and high-resolution benchmarks.\\n\\n\\n\\n\\n## 2. Related Work\\n\\n### 2.1. Multimodal Large Language Models\\n\\nMLLMs have rapidly advanced as powerful foundation models capable of understanding and generating multimodal content across diverse vision-language tasks [25, 28]. Early MLLM architectures generally adopt fixed-resolution vision encoders—such as 224 × 224 or 448 × 448 ViTs [2, 12–14]. While this design simplifies training and computation, it inevitably requires resizing or cropping high-resolution (HR) images, thereby discarding fine-grained visual details crucial for tasks such as fine-grained recognition, dense reasoning, or detecting small objects.\\n\\nTo enhance high-resolution image understanding without proportionally increasing the computational burden from visual tokens, several studies have integrated high-resolution visual encoders into MLLMs. For example, Vary [23] and Deepseek-VL [15] incorporate the SAM encoder [10] to improve model performance on HR images.\\n\\nAlternatively, another line of work introduced Native/Dynamic-Resolution MLLMs that processes images at their native resolution. The core idea is to generate a variable-length sequence of visual tokens that adapts to the original dimensions of the input image, thereby preserving spatial fidelity and high-frequency details. These models employ various mechanisms to handle the resulting long sequences and computational complexity, including sliding window attention, dynamic masking, and patch-based encoding strategies. Representative works in this category have demonstrated significant progress. For instance, the InternVL series [4, 6, 31] adopts a strategy of splitting a high-resolution image into multiple fixed-size patches. In contrast, models like Qwen2.5-VL [2, 3] take an end-to-end approach by training a ViT directly on native-resolution images. This allows the model to embed the entire image into a single, coherent token sequence in one forward pass, potentially leading to better global context understanding.\\n\\n### 2.2. High-Resolution Image Understanding\\n\\nMultimodal large language models (MLLMs) have made substantial progress in recent years; however, they continue to face challenges in accurately recognizing and interpreting fine-grained details within high-resolution (HR) images [21?]. To enhance the capability of MLLMs in high-resolution image understanding, existing studies generally follow two main directions. Training-based approaches rely on supervised fine-tuning (SFT) [18, 24] or reinforcement learning (RL) [30], but such methods often compromise the model\\'s generalization ability on broad vision–language tasks. In contrast, training-free approaches [11, 19, 21, 22]typically perform hierarchical or tree-based search to localize target regions. However, these methods tend to suffer from low efficiency and may fail to retain all target objects during the search process, particularly in multi-object scenarios.\\n\\n## 3. Preliminary\\n\\nIn this section, We first conduct an analysis of the relationship between the resolution of image crops and the performance of MLLMs in subsection 3.2. The experimental results indicate that using different resolution has a significant impact on MLLMs to analyze HR images. Objects of different sizes are suitable for different resolutions. Inspired by this we propose the MRD framework.\\n\\n### 3.1. Semantic Similarity\\n\\nThis section presents the pipeline for integrating Retrieval-Augmented Generation (RAG) into Multimodal Large Language Models (MLLMs) to calculate the semantic similarity scores between the query embedding and images crops from HR images. Given an HR image, we first partition it collection of image patches, denoted as $$P = \\\\{p_1, p_2, \\\\ldots, p_n\\\\}$$, where $$n$$ is the total number of image crops. Following the approach of Yu et al. [27], the textual query and each image crop are encoded independently using the text and image encoders of a Vision-Language Model (VLM), producing a sequence of hidden representations. The semantic similarity score between the query embedding and each image crop embedding is then computed. Specifically, the similarity score $$s(q, p_i)$$ for the i-th crop is calculated by the cosine similarity of the query and image crop embeddings:\\n\\n$$s(q, p_i) = \\\\frac{1}{2} \\\\cdot (1 + \\\\frac{q \\\\cdot p_i}{\\\\|q\\\\| \\\\cdot \\\\|p_i\\\\|})$$\\n(1)\\n\\nBased on these scores, the top $$K$$ most relevant image crops are selected and provided to the MLLM to support detailed understanding of the high-resolution input.\\n\\n### 3.2. Impact of the Resolution of Image Crops\\n\\nIn this section, we conduct an analysis to investigates the relationship between the resolution of image crops and the performance of MLLMs in HR image understanding.\\n\\n**Experimental setting.** We analyze the relation between performance and the resolution of image crops, using LLaVA-ov and LLaVA-v1.5 on V * benmark.\\n\\n**Observations.** We visualize the relationship between the resolution of image crops and performance of MLLMs. As shown in Figure 3, from the overall accuracy obtained by using different resolutions, when the resolution of image crops is set to 112, using different MLLM achieved the highest accuracy rates in both the single-object task and the multi-object task. This indicates that setting the resolution to 112 might be the optimal choice. However, when we take\\n\\n\\n\\n\\n\\n| ![Street scene with half-timbered buildings and baby carriage](image1)                                                                                                                                                                   | ![Building facade detail](image2) | ![Building entrance with baby carriage](image3) | **Semantic Similarity**<br/>![Semantic similarity heatmap showing building in blue tones](heatmap) |                                                     | ![Building detail in green/blue tones](image4) |\\n| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------- | ----------------------------------------------- | -------------------------------------------------------------------------------------------------- | --------------------------------------------------- | ---------------------------------------------- |\\n| **What is the color of the baby carriage?**<br/>A. The color of the baby carriage is black.<br/>B. The color of the baby carriage is green.<br/>C. The color of the baby carriage is red.<br/>D. The color of the baby carriage is blue. |                                   | **Answer: B**<br/>**Resolution=96**             | **Answer: B**<br/>**Resolution=112**                                                               | **Resolution =112**<br/>![Scale from 0 to 1](scale) | ![Sad face emoji](sad_face)                    |\\n|                                                                                                                                                                                                                                          | ![Street scene detail](image5)    | **Answer: B**<br/>**Resolution =144**           | **Answer: A**<br/>**Resolution =224**                                                              |                                                     |                                                |\\n\\n\\nFigure 2. Setting the resolution of image crops to 112 causes complete objects to be split across different regions, which disrupts the semantic information of the target objects.\\n\\n| **Attribute**                                                                                      |   |   | **Spatial**                                |   |   |\\n| -------------------------------------------------------------------------------------------------- | - | - | ------------------------------------------ | - | - |\\n| ![Two line graphs showing Accuracy vs Resolution of Image Crop for different LLaVA models](graphs) |   |   |                                            |   |   |\\n| LLaVA-ov-0.5B LLaVA-v1.5-7B LLaVA-v1.5-13B                                                         |   |   | LLaVA-ov-0.5B LLaVA-v1.5-7B LLaVA-v1.5-13B |   |   |\\n\\n\\nFigure 3. The effect of the resolution of retrieved image crops on model performance. Attribute and Spatial represent the attribute recognition and spatial reasoning in $$V^*$$ Bench.\\n\\na closer look at the results of each sample, we find that in some cases, choosing a different resolution actually leads to more accurate results compared to when the resolution is 112, as shown in Figure 3. The results of the image crops selected based on different resolutions and the visualization of the semantic similarity map can provide a very intuitive analysis of the reasons: Since the complete object is divided into different crops, some parts of it have a higher semantic similarity calculated by VisRAG, while other parts have a lower semantic similarity. After screening, only the parts with higher similarity are retained. However, this will damage the integrity of the target object and cause interference to the judgment of MLLM.\\n\\n## 4. Method\\n\\nIn this section, we propose a novel framework named Multi-resolution Retrieval Detection (MRD). The core design of MRD lies in its multi-resolution approach at different scales to better localize regions containing target objects. This enables subsequent search processes to more easily identify image crops corresponding to the target objects, eliminating irrelevant distractions and enhancing the perceptual understanding of HR images by MLLMs. Based on the findings in subsection 3.2, we argue that using different resolutions for semantic similarity computation is more suitable for objects of varying sizes and locations. Inspired by this idea, we first introduce a simple yet effective Multi-resolution Semantic Fusion method, which computes semantic similarity maps at different resolutions on a local scale and performs consistency-based fusion to refine the semantic similarity and improve its accuracy. To more directly localize target objects, we incorporate an Open-vocabulary object detection model that traverses the entire HR image globally using a sliding window approach, generating confidence scores for regions containing target objects. Finally, by integrating the detection confidence scores with the multi-resolution semantic similarity maps, our method not only improves localization of target regions but also distinguishes fine-grained differences among crops in these regions, thereby assisting subsequent search processes in more accurately identifying key areas. The following sections will provide detailed explanations of each component.\\n\\n### 4.1. Multi-resolution Semantic Fusion\\n\\nIn subsection 3.2, we observe that image crops of different resolutions are suitable for objects of varying sizes and locations in different cases. Compared to the semantic similarity map obtained using a single resolution, those derived from multiple resolutions exhibit respective advantages. Therefore, we first propose a Multi-Resolution Semantic Fusion method. As shown in the top part of Figure 4, we partition the HR image using proportional resolutions, with the low resolution set to $$l$$ and the high resolution set to $$\\\\hat{l}$$, where\\n\\n\\n\\n\\n\\n\\n\\nFigure 4. Detailed information of our proposed *MRD*. First, We use VisRAG with different resolution of image crops to obtain multi-resolution semantic similarity map. We then employ an open-set object detection model, LLMDet, to localize the target objects extracted from the query within the high-resolution image using a sliding-window approach, yielding a global detection confidence map. Finally, the obtained multi-resolution semantic similarity map is linearly fused with the detection confidence map, and the fused scores are used to guide the subsequent search to select image crops containing the target objects.\\n\\n$$\\\\hat{l} = k \\\\cdot l$$. The set of image patches at high resolution is denoted as $$\\\\hat{P} = \\\\{\\\\hat{p}_1, \\\\hat{p}_2, \\\\ldots, \\\\hat{p}_m\\\\}$$, and at low resolution as $$P = \\\\{p_1, p_2, \\\\ldots, p_n\\\\}$$. Due to the proportional relationship between high and low resolutions, we have $$n = k^2 \\\\cdot m$$, and each high-resolution patch $$\\\\hat{p}_i$$ corresponds to $$k^2$$ low-resolution patches:\\n\\n$$\\\\hat{p}_i = \\\\begin{bmatrix} \\\\tilde{p}_{i,1} & \\\\tilde{p}_{i,2} & \\\\cdots & \\\\tilde{p}_{i,k} \\\\\\\\ \\\\tilde{p}_{i,(k+1)} & \\\\tilde{p}_{i,(k+2)} & \\\\cdots & \\\\tilde{p}_{i,(2k)} \\\\\\\\ \\\\vdots & \\\\vdots & \\\\ddots & \\\\vdots \\\\\\\\ \\\\tilde{p}_{i,(k(k-1)+1)} & \\\\tilde{p}_{i,(k(k-1)+2)} & \\\\cdots & \\\\tilde{p}_{i,k^2} \\\\end{bmatrix}$$ (2)\\n\\nwhere $$\\\\tilde{p}_{ij} \\\\in P, i \\\\in \\\\{1, 2, \\\\ldots, m\\\\}, \\\\quad j \\\\in \\\\{1, 2, \\\\ldots, k^2\\\\}$$.\\n\\nThen, using Equation 1, we compute the cosine similarity between the query and image crop embeddings to obtain the semantic similarity scores for high and low resolutions, respectively: $$\\\\hat{S} = \\\\{\\\\hat{s}_1, \\\\hat{s}_2, \\\\ldots, \\\\hat{s}_m\\\\}$$ and $$S = \\\\{s_1, s_2, \\\\ldots, s_n\\\\}$$:\\n\\n$$\\\\hat{s}_i = s(f(q), g(\\\\hat{p}_i)), \\\\quad s_j = s(f(q), g(p_j))$$ (3)\\n\\nwhere $$f(\\\\cdot)$$ and $$g(\\\\cdot)$$ denote the embedding operations for the query and image crop, respectively, with $$i \\\\in \\\\{1, 2, \\\\ldots, m\\\\}$$ and $$j \\\\in \\\\{1, 2, \\\\ldots, n\\\\}$$. According to the mapping from $$\\\\hat{p}_i$$ to $$p_j$$ in (2), we can map the semantic similarity $$\\\\hat{s}$$ obtained from each high-resolution $$\\\\hat{p}$$ and the query to the corresponding $$k^2$$ low-resolution $$p$$ positions. The mapping operation can be expressed as:\\n\\n$$\\\\tilde{S} = H(\\\\hat{S})$$ (4)\\n\\nwhere $$\\\\tilde{S} = \\\\{\\\\tilde{s}_1, \\\\tilde{s}_2, \\\\ldots, \\\\tilde{s}_n\\\\}$$. After obtaining $$\\\\tilde{S}$$ and $$S$$, we perform consistency fusion on the semantic similarities at corresponding positions to obtain the multi-resolution semantic similarity $$S^f = \\\\{s^f_1, s^f_2, \\\\ldots, s^f_n\\\\}$$, which can be expressed as:\\n\\n$$s^f_t = \\\\sqrt{\\\\tilde{s}_t \\\\cdot s_t}, \\\\quad t \\\\in 1, 2, \\\\ldots, n$$ (5)\\n\\nFinally, we can transform the semantic similarity scores into a two-dimensional semantic similarity map $$s^f(i, j)$$ with $$i \\\\in \\\\{1, 2, \\\\ldots, H\\\\}$$ and $$j \\\\in \\\\{1, 2, \\\\ldots, W\\\\}$$. The total number of low resolution image crop $$n = H \\\\times W$$.\\n\\nFusing the multi-resolution semantic similarity scores from high and low resolutions enables correction of the low-resolution similarities when a complete object is split across different patches in the low-resolution view. This enhances the similarity of various parts of the object, thereby preserving the integrity of the object as much as possible during subsequent search processes and improving the recognition accuracy of the MLLM.\\n\\n## 4.2. Open-vocabulary Detector Enhancement\\n\\nVisRAG divides the HR image into patches and computes semantic similarity between the query and each image crop,\\n\\n\\n\\n\\nenabling localized object retrieval at a small scale. However, it struggles to accurately localize larger objects. To address this limitation and achieve more direct and large-scale object localization, we introduce an advanced open-vocabulary object detection model—LLMDet—to directly locate regions containing target objects. First, we employ in-context learning with a Large Language Model (LLM) to extract the primary target objects from the query, which serve as the target categories for LLMDet. Due to the extremely high resolution of HR images in datasets such as HR-Bench, we adopt a sliding window strategy for object localization. To align with the semantic similarity map derived from image crops, we assign the detection confidence scores of the target bounding boxes to their corresponding image patches, thereby generating a detection map that reflects the confidence of target object presence in each patch. This detection map offers a more intuitive localization representation compared to the semantic similarity map. In the following, we provide a detailed introduction to the proposed method.\\n\\n**Object Extraction.** To enable open-vocabulary object detection, we first leverages Large Language Models (LLMs) to dynamically identify target objects from textual queries. Given an input query $Q$, we employ in-context learning to extract the primary object entities that serve as detection targets for our LLMDet framework.\\n\\nFormally, we define the object extraction process as:\\n\\n$$O = \\\\text{LLM}(\\\\mathcal{P}_{\\\\text{system}}, \\\\mathcal{E}_{\\\\text{examples}}, Q) \\\\tag{6}$$\\n\\nwhere $O$ represents the set of extracted objects, $\\\\mathcal{P}_{\\\\text{system}}$ denotes the system prompt containing extraction guidelines, and $\\\\mathcal{E}_{\\\\text{examples}}$ constitutes the demonstration examples.\\n\\n**Sliding-window Object Detection.** In order to get a global detection confidence map align with the previously semantic similarity map, we similarly partition the HR image into a grid of $H \\\\times W$ non-overlapping patches where total number of patches $n = H \\\\times W$. A sliding window of size $h \\\\times w$ patches (where $h < H$ and $w < W$) traverses the entire image with a predefined stride. In this way, We can obtain $T$ sliding windows $W = \\\\{W_1, W_2, ..., W_T\\\\}$.\\n\\nAfter obtaining multiple sliding windows using the sliding window method, we use LLMDet to detect objects within each sliding window. The detector generates a set of bounding boxes $\\\\mathcal{B}_t = \\\\{b_1, b_2, \\\\ldots, b_{K_t}\\\\}$ and the corresponding confidence scores $s_k$ indicating the likelihood of containing a target object where $t$ denotes the $t$-th sliding window.\\n\\nWe apply a confidence threshold $\\\\tau$ to filter out low-quality detections:\\n\\n$$\\\\mathcal{B}_t^{\\\\text{filter}} = \\\\{b_k \\\\in \\\\mathcal{B}_t \\\\mid s_k > \\\\tau\\\\} \\\\tag{7}$$\\n\\nSubsequently, we generate a window detection confidence map $\\\\mathbf{c}_t^w \\\\in \\\\mathbb{R}^{h \\\\times w}$ for the current window. The value\\n\\nat patch coordinate $(p, q)$ within this local window is assigned the maximum confidence score among all bounding boxes in $\\\\mathcal{B}_t^{\\\\text{filter}}$ that contain this patch. If no box covers the patch, the confidence is set to 0:\\n\\n$$\\\\mathbf{c}_t^w(p, q) = \\\\max_{b_k \\\\in \\\\mathcal{B}_t^{\\\\text{filter}}} \\\\{s_k \\\\cdot \\\\mathbb{I}[(m, n) \\\\in b_k]\\\\} \\\\tag{8}$$\\n\\nwhere $\\\\mathbb{I}[\\\\cdot]$ is the indicator function that equals 1 if the patch $(p, q)$ is inside the bounding box $b_k$, and 0 otherwise.\\n\\nTo aggregate information from all sliding windows and form a global, unified detection confidence map $\\\\mathbf{c}^g \\\\in \\\\mathbb{R}^{H \\\\times W}$ for the entire high-resolution image, we employ an averaging fusion strategy. For a global patch at coordinate $(p, q)$, its final confidence score is computed as the average of all confidence scores assigned to it from every sliding window that contained it.\\n\\nFor the patch $I(i, j)$ at position $(i, j)$ in the HR image, if it is contained in the $t$-th sliding window, we denote its position in the $t$-th sliding window $W_t$ as $(t_i, t_j)$, which can be expressed as\\n\\n$$I(i, j) = W_t(t_i, t_j), \\\\quad t \\\\in \\\\mathcal{T}_{i,j} \\\\tag{9}$$\\n\\nwhere $\\\\mathcal{T}_{ij}$ denotes the set of sliding windows that contain $I(i, j)$. Now, we can obtain the global detection confidence map of the whole HR image, which can be computed as:\\n\\n$$\\\\mathbf{c}^g(i, j) = \\\\frac{1}{|\\\\mathcal{T}_{i,j}|} \\\\sum_{t \\\\in \\\\mathcal{T}_{i,j}} \\\\mathbf{c}_t^w(t_i, t_j) \\\\tag{10}$$\\n\\nwhere $i \\\\in \\\\{1, 2, \\\\ldots, H\\\\}$ and $j \\\\in \\\\{1, 2, \\\\ldots, W\\\\}$.\\n\\nThe detection confidence map provides effective localization of target regions on a global scale, offering direct spatial guidance but lacking the ability to distinguish fine-grained differences within the target object. To address this limitation, we integrate the detection confidence with multi-resolution semantic similarity through linear combination, which can be expressed as:\\n\\n$$\\\\mathbf{s}^F(i, j) = (1 - w) \\\\cdot \\\\mathbf{s}^f(i, j) + w \\\\cdot \\\\mathbf{c}^g(i, j) \\\\tag{11}$$\\n\\nThis synergistic fusion enables precise target localization while effectively highlighting intra-object variations, thereby facilitating more accurate extraction of key regions in subsequent search processes. The details of the subsequent Retrieved-Exploration Search process can be found in paper [22].\\n\\n## 5. Experiments\\n\\n**Evaluated benchmark.** We evaluate our *MRD* on two high-resolution benchmarks. The first is $V^*$ *Bench* [24], with an average resolution of $2246 \\\\times 1582$, consists of two sub-tasks: attribute recognition and spatial reasoning. The Second is HRBench which includes two sub-task Fine-grained Single-instance Perception (FSP) and Fine-grained Cross-instance Perception (FCP).\\n\\n\\n\\n\\nTable 1. Comparison of *MRD* with existing works on high-resolution benchmarks\\n\\n| Method                       | V\\\\* Bench<br/>Attribute | V\\\\* Bench<br/>Spatial | V\\\\* Bench<br/>Overall | HR-Bench 4K<br/>FSP | HR-Bench 4K<br/>FCP | HR-Bench 4K<br/>Overall | HR-Bench 8K<br/>FSP | HR-Bench 8K<br/>FCP | HR-Bench 8K<br/>Overall |\\n| ---------------------------- | ----------------------- | --------------------- | --------------------- | ------------------- | ------------------- | ----------------------- | ------------------- | ------------------- | ----------------------- |\\n| *Open-source MLLMs*          |                         |                       |                       |                     |                     |                         |                     |                     |                         |\\n| LLaVA-v1.6-7B \\\\[14]          | 60.9                    | 63.2                  | 61.8                  | 49.0                | 46.8                | 47.9                    | 37.3                | 44.3                | 40.8                    |\\n| LLaVA-v1.6-13B \\\\[14]         | 60.0                    | 64.5                  | 61.8                  | 49.8                | 41.3                | 45.5                    | 38.0                | 38.3                | 38.1                    |\\n| LLaVA-v1.6-34B \\\\[14]         | -                       | -                     | -                     | 55.3                | 50.5                | 52.9                    | 44.5                | 50.3                | 47.4                    |\\n| LLaVA-HR-X-13B \\\\[16]         | -                       | -                     | -                     | 61.3                | 46.0                | 53.6                    | 49.5                | 44.3                | 46.9                    |\\n| LLaVA-HR-X-7B \\\\[16]          | 51.3                    | 64.5                  | 56.5                  | 57.8                | 46.3                | 52.0                    | 42.0                | 41.3                | 41.6                    |\\n| InternVl-1.5-26B \\\\[5]        | -                       | -                     | -                     | 69.5                | 51.8                | 60.6                    | 69.3                | 48.5                | 57.9                    |\\n| Yi-VL-34B \\\\[26]              | -                       | -                     | -                     | 46.0                | 42.8                | 44.4                    | 39.5                | 38.5                | 39.0                    |\\n| *Closed-source MLLMs*        |                         |                       |                       |                     |                     |                         |                     |                     |                         |\\n| GPT-4o \\\\[8]                  | -                       | -                     | 66.0                  | 70.0                | 48.0                | 59.0                    | 62.0                | 49.0                | 55.5                    |\\n| Qwen-VL-max \\\\[2]             | -                       | -                     | -                     | 65.0                | **52.0**            | 58.5                    | 54.0                | **51.0**            | 52.5                    |\\n| *Baselines and MRD*          |                         |                       |                       |                     |                     |                         |                     |                     |                         |\\n| LLaVA-v1.5-7B \\\\[14]          | 43.5                    | 56.6                  | 48.7                  | 38.5                | 33.8                | 36.1                    | 33.0                | 31.3                | 32.1                    |\\n| LLaVA-v1.5-7B-Zoom Eye \\\\[19] | 83.5                    | 82.9                  | 83.3                  | 67.8                | 38.8                | 53.3                    | 65.5                | 36.0                | 50.8                    |\\n| LLaVA-v1.5-7B-RAP \\\\[22]      | 90.4                    | **96.1**              | 91.1                  | 73.8                | 40.5                | 57.1                    | 72.3                | 35.3                | 53.8                    |\\n| **LLaVA-v1.5-7B-MRD (ours)** | **97.4**                | **96.1**              | **95.6**              | **76.8**            | 42.7                | **59.7**                | **72.6**            | 37.2                | **54.9**                |\\n| LLaVA-ov-0.5B \\\\[14]          | 63.5                    | 64.5                  | 63.9                  | 63.5                | 39.5                | 51.5                    | 47.3                | 38.3                | 42.8                    |\\n| LLaVA-ov-0.5B-Zoom Eye\\\\[19]  | 85.2                    | 73.7                  | 80.6                  | 75.5                | 39.8                | 57.6                    | 68.5                | 38.3                | 53.4                    |\\n| LLaVA-ov-0.5B-RAP \\\\[22]      | 80.0                    | 84.2                  | 83.6                  | 80.3                | 42.3                | 61.3                    | **81.8**            | 45.3                | 63.5                    |\\n| **LLaVA-ov-0.5B-MRD (ours)** | **89.6**                | **82.9**              | **88.0**              | **84.0**            | **45.2**            | **64.6**                | **81.8**            | **47.3**            | **64.5**                |\\n\\n\\n| **HR Image**                                                   | **Res = 112** | **Res = 224** | **Multi-res** |   | **Search Result<br/>(Res = 112)** | **Search Result<br/>(Multi-res)** |\\n| -------------------------------------------------------------- | ------------- | ------------- | ------------- | - | --------------------------------- | --------------------------------- |\\n| What is the color of the cyclist\\'s box?                        |               |               |               |   |                                   |                                   |\\n| Is the green bucket on the left or right side of the red tent? |               |               |               |   |                                   |                                   |\\n| **HR Image**                                                   | **Res = 112** | **OVD**       | **RAG+OVD**   |   | **Search Result<br/>(Res = 112)** | **Search Result<br/>(RAG+OVD)**   |\\n| What is the color of the telephone?                            |               |               |               |   |                                   |                                   |\\n| What is the material of the stool?                             |               |               |               |   |                                   |                                   |\\n\\n\\nFigure 5. Visualization of the Effects of Different Modules in MRD. Upper: Visualization of the Effects of the Multi-resolution Semantic Fusion Method. Lower: Visualization of the Effects of the Multi-resolution Semantic Fusion Method\\n\\n## 5.1. Main Results\\n\\nAs shown in Table 1, compared with both the baseline MLLMs and previous baseline approaches, our proposed\\n\\n\\n\\n\\nMRD framework consistently delivers substantial performance gains across all sub-tasks, datasets, and model configurations. The improvement is most pronounced on the V* dataset using the LaVA-v1.5-7B model, where MRD achieves a remarkable 46.9% absolute increase in accuracy—nearly doubling the original performance. Significant gains are also observed on HR-Bench 4K and HR-Bench 8K, with maximum improvements of 23.6% and 22.8%, respectively.\\n\\nIn comparison to the state-of-the-art baseline RAP, MRD achieves superior performance across all datasets and model settings, yielding an average improvement of 2.8%. When examining results across sub-task categories, MRD demonstrates particularly strong performance on single-object tasks. We attribute this advantage to the integration of a detection module, which provides more accurate localization for isolated objects.\\n\\nOverall, these results indicate that MRD markedly enhances the perception and understanding capabilities of MLLMs when operating on high-resolution images.\\n\\n## 5.2. Effect of the Multi-resolution Semantic Fusion\\n\\nMulti-resolution Semantic Fusion can obtain more accurate information by integrating semantic similarity maps from different resolutions. From the two cases shown in the upper part of Figure 5, we can clearly observe that incorporating multi-resolution semantic fusion allows the high-resolution semantic similarity map to correct the low-resolution map, alleviating semantic deviations caused by different parts of the target object being split across multiple patches. This helps better preserve the integrity of the target object. The results in the cases demonstrate that the approach is effective for both single-object and multi-object tasks. Overall, the experimental results indicate that Multi-resolution Semantic Fusion provides better adaptability to objects of different sizes compared to using a single resolution.\\n\\n## 5.3. Effect of Open-vocabulary Object Detection\\n\\nTo achieve more accurate and direct localization of the target object at a global scale, we introduce an open-set object detection model. As shown in lower part of Figure 5, sliding-window detection results effectively identify the target object\\'s location. By combining the detection results with semantic similarity scores, MRD amplifies the scores of patches that contain the target object while suppressing false-positive patches that also exhibit high semantic similarity. This integration facilitates a more efficient and accurate patch retrieval process in subsequent searching.\\n\\n## 5.4. Ablation Study\\n\\nTo better understand the contributions of different modules in our *MRD* framework, we conduct ablation studies\\n\\nTable 2. Ablation study of different module in *MRD*.\\n\\n|                   | V\\\\* Bench<br/>Attribute | V\\\\* Bench<br/>Spatial | V\\\\* Bench<br/>Overall | ∆↑   |\\n| ----------------- | ----------------------- | --------------------- | --------------------- | ---- |\\n| RAP               | 80.0                    | 84.2                  | 83.6                  | -    |\\n| OVD               | 84.3                    | 81.6                  | 84.9                  | +1.3 |\\n| RAP+Multi-res     | 82.9                    | 85.2                  | 85.8                  | +2.2 |\\n| RAP+OVD           | 85.2                    | 84.2                  | 86.2                  | +2.6 |\\n| RAP+OVD+Multi-Res | 90.4                    | 85.5                  | 89.3                  | +5.7 |\\n\\n\\non the V* dataset using the LLaVA-ov-0.5B model. As shown in Table 2, using the OVD model alone (second row) yields higher localization accuracy for single-object tasks, but its performance on multi-object tasks is inferior to RAP. When RAP employs multi-resolution semantic fusion (third row), performance improves on both single-object and multi-object tasks, indicating that multi-resolution semantic fusion can better handle objects of varying sizes across different scenarios.\\n\\nFusing the semantic similarity map obtained from RAP with the detection confidence map from OVD (fourth row) significantly improves performance on single-object tasks; however, the performance on multi-object tasks is even worse than using OVD alone, suggesting that some target objects may be lost during the search. By further incorporating multi-resolution semantic fusion, performance improves on both single-object and multi-object tasks, demonstrating the effectiveness of this fusion strategy.\\n\\nIn summary, introducing OVD helps localize single objects more accurately but may result in missed objects in multi-object scenarios. Multi-resolution semantic fusion corrects semantic similarity scores and preserves object completeness under different conditions, enhancing MLLM performance on both single- and multi-object tasks. The final model, which integrates all modules, achieves a 5.7% higher accuracy than RAP, demonstrating the effectiveness of MRD\\'s design in improving high-resolution image understanding for MLLMs.\\n\\n## 6. Conclusion\\n\\nIn this work, we propose a novel training-free method, Multi-resolution Retrieval-Detection (MRD), to enhance the understanding of high-resolution images by MLLMs. MRD employs multi-resolution semantic similarity to correct single-resolution similarity maps, ensuring the integrity of target objects. Moreover, to localize target objects more accurately and directly, we introduce an OVD model that identifies object regions using a sliding-window approach. We demonstrate the effectiveness of MRD across multiple high-resolution benchmarks with different MLLMs, showing its superior performance in HR image understanding.\\n\\n\\n\\n\\n## References\\n\\n[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. *arXiv preprint arXiv:2309.16609*, 2023. 1\\n\\n[2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond, 2023. 3, 7\\n\\n[3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. *arXiv preprint arXiv:2502.13923*, 2025. 3\\n\\n[4] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. *arXiv preprint arXiv:2412.05271*, 2024. 3\\n\\n[5] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. *Sci. China Inf. Sci.*, 67(12), 2024. 7\\n\\n[6] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pages 24185–24198, 2024. 3\\n\\n[7] Shenghao Fu, Qize Yang, Qijie Mo, Junkai Yan, Xihan Wei, Jingke Meng, Xiaohua Xie, and Wei-Shi Zheng. Llmdet: Learning strong open-vocabulary object detectors under the supervision of large language models. In *Proceedings of the Computer Vision and Pattern Recognition Conference*, pages 14987–14997, 2025. 2\\n\\n[8] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. *arXiv preprint arXiv:2410.21276*, 2024. 7\\n\\n[9] Bowen Jin, Jinsung Yoon, Jiawei Han, and Sercan O Arik. Long-context llms meet rag: Overcoming challenges for long inputs in rag. *arXiv preprint arXiv:2410.05983*, 2024. 2\\n\\n[10] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In *Proceedings of the IEEE/CVF international conference on computer vision*, pages 4015–4026, 2023. 3\\n\\n[11] Geng Li, Jinglin Xu, Yunzhen Zhao, and Yuxin Peng. Dyfo: A training-free dynamic focus visual search for enhancing lmms in fine-grained visual understanding. In *Proceedings of the Computer Vision and Pattern Recognition Conference*, pages 9098–9108, 2025. 1, 2, 3\\n\\n[12] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In *International conference on machine learning*, pages 19730–19742. PMLR, 2023. 3\\n\\n[13] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pages 26296–26306, 2024. 1\\n\\n[14] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llavanext: Improved reasoning, ocr, and world knowledge, 2024. 1, 3, 7\\n\\n[15] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, et al. Deepseek-vl: towards real-world vision-language understanding. *arXiv preprint arXiv:2403.05525*, 2024. 3\\n\\n[16] Gen Luo, Yiyi Zhou, Yuxin Zhang, Xiawu Zheng, Xiaoshuai Sun, and Rongrong Ji. Feast your eyes: Mixture-of-resolution adaptation for multimodal large language models. In *The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025*. OpenReview.net, 2025. 7\\n\\n[17] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In *International conference on machine learning*, pages 8748–8763. PmLR, 2021. 1\\n\\n[18] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual cot: Advancing multi-modal language models with a comprehensive dataset and benchmark for chain-of-thought reasoning. *Advances in Neural Information Processing Systems*, 37:8612–8642, 2024. 1, 3\\n\\n[19] Haozhan Shen, Kangjia Zhao, Tiancheng Zhao, Ruochen Xu, Zilun Zhang, Mingwei Zhu, and Jianwei Yin. Zoomeye: Enhancing multimodal llms with human-like zooming capabilities through tree-based image exploration. In *Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing*, pages 6613–6629, 2025. 1, 2, 3, 7\\n\\n[20] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. *arXiv preprint arXiv:2303.15389*, 2023. 1\\n\\n[21] Wenbin Wang, Liang Ding, Minyan Zeng, Xiabin Zhou, Li Shen, Yong Luo, Wei Yu, and Dacheng Tao. Divide, conquer and combine: A training-free framework for high-resolution image perception in multimodal large language models. In *Proceedings of the AAAI Conference on Artificial Intelligence*, pages 7907–7915, 2025. 1, 2, 3\\n\\n[22] Wenbin Wang, Yongcheng Jing, Liang Ding, Yingjie Wang, Li Shen, Yong Luo, Bo Du, and Dacheng Tao. Retrieval-augmented perception: High-resolution image perception meets visual rag. *arXiv preprint arXiv:2503.01222*, 2025. 1, 2, 3, 6, 7\\n\\n\\n\\n\\n[23] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng Ge, Jinrong Yang, Jianjian Sun, Chunrui Han, and Xiangyu Zhang. Vary: Scaling up the vision vocabulary for large vision-language model. In *European Conference on Computer Vision*, pages 408–424. Springer, 2024. 3\\n\\n[24] Penghao Wu and Saining Xie. V?: Guided visual search as a core mechanism in multimodal llms. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 13084–13094, 2024. 1, 2, 3, 6\\n\\n[25] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey on multimodal large language models. *National Science Review*, 11(12): nwae403, 2024. 1, 3\\n\\n[26] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open foundation models by 01.ai. *CoRR*, abs/2403.04652, 2024. 7\\n\\n[27] Shi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao Ran, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han, Zhiyuan Liu, et al. Visrag: Vision-based retrieval-augmented generation on multi-modality documents. *arXiv preprint arXiv:2410.10594*, 2024. 3\\n\\n[28] Duzhen Zhang, Yahan Yu, Jiahua Dong, Chenxing Li, Dan Su, Chenhui Chu, and Dong Yu. Mm-llms: Recent advances in multimodal large language models. In *Findings of the Association for Computational Linguistics ACL 2024*, pages 12401–12430, 2024. 3\\n\\n[29] Jiarui Zhang, Mahyar Khayatkhoei, Prateek Chhikara, and Filip Ilievski. Mllms know where to look: Training-free perception of small visual details with multimodal llms. *arXiv preprint arXiv:2502.17422*, 2025. 1, 2\\n\\n[30] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing\" thinking with images\" via reinforcement learning. *arXiv preprint arXiv:2505.14362*, 2025. 1, 2, 3\\n\\n[31] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. *arXiv preprint arXiv:2504.10479*, 2025. 3\\n\\n\\n\\n\\n# MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding\\n\\n## Supplementary Material\\n\\n## A. Implement Details of MRD\\n\\nFollowing [22], given an input HR image I, it is first partitioned into smaller image crops based on a predefined crop size, which corresponds to the preferred resolution of the retriver. According to the Resolution of HR image, we set the crop resolutions as 112, 224 and 448 for V* Bench, HR-Bench-4K and HR-Bench-8K respectively. For multi-resolution semantic fusion, the ratio between the high and low resolutions is set to k = 2 in all experiments. For Sliding Window detection, we set window size and step as 1232 and 896 for V* Bench, 2240 and 1792 for HR-Bench-4K, 3136 and 2688 for for HR-Bench-8K to balance efficiency and accuracy. The weight w of the detection confidence map is 0.4 by default in semantic detection map fusion. For the following Retrieved-Exploration Search (RE-Search) process, we adopt the same hyperparameters as the baseline method RAP [22]. In all experiments, the maximum search steps are set to 200, and the answering confidence threshold τ is set to 0.6. In the following hyperparameter studies, all other hyperparameters of our MRD use their default settings unless otherwise specified.\\n\\n## B. More Experiment Result\\n\\nTo analyze the impact of different hyperparameters on performance, we conduct experiments on MRD using various hyperparameter settings, including Crop Resolution, Maximum Search Steps, Detection Weight, and Detection Window Size. We perform these experiments on the V* Bench using both the LLaVA-ov-0.5B and LLaVA-v1.5-7B models. Unless otherwise specified, all other hyperparameters follow the default settings mentioned in section A.\\n\\n### B.1. Effect of Crop Resolution\\n\\nIn our experiments, we evaluate the effect of different crop resolutions on performance. The results are shown in Figure 6. For the Single Instance Task (Figure 6 (a)), we observe that MRD remains highly stable across different resolutions for both models, with only minor performance fluctuations, whereas RAP exhibits much larger variations, especially when using LLaVA-ov-0.5B.\\n\\nFor the Cross Instance Task, the performance gap between MRD and RAP is relatively small when using LLaVA-v1.5-7B. However, with LLaVA-ov-0.5B, MRD is largely unaffected by resolution changes, further demonstrating its robustness. Overall, MRD consistently outperforms RAP across different resolutions and model settings, highlighting the advantages of our approach.\\n\\nIn summary, the Multi-resolution Semantic Fusion and Detector Enhancement modules in MRD effectively mitigate the interference caused by fragmenting complete objects across multiple crops when using different crop resolutions. As a result, our MRD performance is only weakly influenced by crop resolution and achieves notably better results in the Single Instance Task.\\n\\n### B.2. Effect of Maximum Search Steps\\n\\nThe performance of MRD and RAP under different maximum search steps is shown in Figure 7. In Figure 7 (a), for the single-instance task, MRD consistently outperforms RAP across different max step settings on both the LLaVA-ov-0.5B and LLaVA-v1.5-7B models.\\n\\nIn Figure 7 (b), for the Cross Instance Task, MRD is slightly inferior to RAP only when using LLaVA-v1.5-7B with small max steps. However, as the max step increases, MRD surpasses RAP and maintains better performance. Overall, MRD achieves superior results compared to RAP. Notably, MRD with LLaVA-ov-0.5B performs only marginally lower than RAP with the powerful LLaVA-v1.5-7B model.\\n\\nMost importantly, MRD reaches its peak performance with a significantly smaller number of maximum search steps (Max Step = 30). This means that in practical applications, MRD can operate effectively with fewer steps, achieving high accuracy while reducing search time and improving efficiency.\"\\n\\n### B.3. Effect of Detection Weight\\n\\nThe results of using different detection weights are shown in Figure 8. We observe that relying solely on the semantic similarity map (weight = 0) or solely on the detection map (weight = 1) does not yield optimal performance for either task. In contrast, fusing the two maps leads to better results, demonstrating that the semantic similarity map and detection map provide complementary information.\\n\\nOverall (Figure 8 (c)), the optimal detection weight varies slightly across models: LLaVA-ov-0.5B achieves its best performance at weight = 0.4, while LLaVA-v1.5-7B performs best at weight = 0.2.\\n\\n### B.4. Effect of Window Size\\n\\nAs shown in Figure 9, adopting different sliding-window sizes for object detection also affects the results. Except for the Cross Instance Task with LLaVA-ov-0.5B, using a\\n\\n\\n\\n\\n\\n★ LLaVA-ov-0.5B-MRD ● LLaVA-v1.5-7B-MRD ☆ LLaVA-ov-0.5B-RAP ○ LLaVA-v1.5-7B-RAP\\n\\n| (a) Single | (b) Cross | (c) Overall |\\n|------------|-----------|-------------|\\n| ![Graph with accuracy (%) on y-axis from 75-95, resolution on x-axis (96, 112, 144, 160, 224)] | ![Graph with accuracy (%) on y-axis from 75-95, resolution on x-axis (96, 112, 144, 160, 224)] | ![Graph with accuracy (%) on y-axis from 75-95, resolution on x-axis (96, 112, 144, 160, 224)] |\\n\\nFigure 6. The effect of the resolution of image crops on model performance. Single and Cross represent the attribute recognition and spatial reasoning in V* Bench. (a) Single-instance Task. (b) Cross-instance Task. (c) Overall Performance.\\n\\n.\\n\\n★ LLaVA-ov-0.5B-MRD ● LLaVA-v1.5-7B-MRD ☆ LLaVA-ov-0.5B-RAP ○ LLaVA-v1.5-7B-RAP\\n\\n| (a) Single | (b) Cross | (c) Overall |\\n|------------|-----------|-------------|\\n| ![Graph with accuracy (%) on y-axis from 40-90, Max Step on x-axis (1, 10, 30, 50, 100, 200)] | ![Graph with accuracy (%) on y-axis from 65-95, Max Step on x-axis (1, 10, 30, 50, 100, 200)] | ![Graph with accuracy (%) on y-axis from 50-90, Max Step on x-axis (1, 10, 30, 50, 100, 200)] |\\n\\nFigure 7. The effect of the maximum search steps of *MRD* and *RAP*.\\n\\nsmaller sliding-window size (Window Size = 896) generally yields better performance. This is because a smaller window reduces background interference unrelated to the target object, leading to more accurate detection results.\\n\\nHowever, a smaller window size also means that more windows are required to scan the entire high-resolution image, resulting in increased computational complexity and longer processing time. Therefore, to balance accuracy and efficiency, we select a larger sliding-window size, Window Size = 1232, as the default setting.\\n\\n## B.5. Compared with Other HR Methods\\n\\nWe compare our *MRD* approach with three high-resolution processing baselines *RAP*, DC² and Zoom Eye. DC² is a training-free framework that improves MLLM comprehension of HR images by dividing them into crops, generating textual descriptions for each region, and aggregating these descriptions to obtain a more complete understanding. Zoom Eye, on the other hand, uses a tree-search strategy to traverse the hierarchical visual structure of an image, enabling efficient identification and extraction of relevant information.\\n\\nAs shown in Table 3, all HR processing methods yield overall performance improvements compared with the baseline. Among them, our *MRD* achieves consistently stronger results across most tasks, demonstrating its clear advantage over existing approaches.\\n\\n\\n\\n\\n\\nLLaVA-ov-0.5B-MRD LLaVA-v1.5-7B-MRD -- LLaVA-ov-0.5B-RAP -- LLaVA-v1.5-7B-RAP\\n\\n| \\\\*\\\\*Accuracy(%)\\\\*\\\\* 95.0 92.5 90.0 87.5 85.0 82.5 80.0 0 0.2 0.4 0.6 0.8 1.0 Weight \\\\*\\\\*(a) Single\\\\*\\\\* | \\\\*\\\\*Accuracy(%)\\\\*\\\\* 96 94 92 90 88 86 84 82 80 0 0.2 0.4 0.6 0.8 1.0 Weight \\\\*\\\\*(b) Cross\\\\*\\\\* | \\\\*\\\\*Accuracy(%)\\\\*\\\\* 96 94 92 90 88 86 84 0 0.2 0.4 0.6 0.8 1.0 Weight \\\\*\\\\*(c) Overall\\\\*\\\\* |\\n| ------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------- |\\n\\n\\nFigure 8. The effect of the detection weight in *MRD*.\\n\\nLLaVA-ov-0.5B-MRD LLaVA-v1.5-7B-MRD -- LLaVA-ov-0.5B-RAP -- LLaVA-v1.5-7B-RAP\\n\\n| \\\\*\\\\*Accuracy(%)\\\\*\\\\* 95.0 92.5 90.0 87.5 85.0 82.5 80.0 896 1120 1232 1456 1568 Window Size \\\\*\\\\*(a) Single\\\\*\\\\* | \\\\*\\\\*Accuracy(%)\\\\*\\\\* 96 94 92 90 88 86 84 82 896 1120 1232 1456 1568 Window Size \\\\*\\\\*(b) Cross\\\\*\\\\* | \\\\*\\\\*Accuracy(%)\\\\*\\\\* 96 94 92 90 88 86 84 896 1120 1232 1456 1568 Window Size \\\\*\\\\*(c) Overall\\\\*\\\\* |\\n| ------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------ |\\n\\n\\nFigure 9. The effect of the detection window size in *MRD*.\\n\\nTable 3. Comparison of *MRD* with existing works on high-resolution benchmarks. We conduct experiments on *V*<sup>*</sup> *Bench* and *HR-Bench* using LLaVA-v1.5 7B.\\n\\n| Method           |      |      |      |      |      |      |      |      |      | Δ(↑)  |\\n| ---------------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ----- |\\n| LLaVA-v1.5-7B    | 43.5 | 56.6 | 48.7 | 38.5 | 33.8 | 36.1 | 33.0 | 31.3 | 32.1 | -     |\\n| -w/ *DC*2        | 49.6 | 59.2 | 51.6 | 45.3 | 37.0 | 41.1 | 36.5 | 33.3 | 34.9 | +3.5  |\\n| -w/ *Zoom Eye*   | 83.5 | 82.9 | 83.3 | 67.8 | 38.8 | 53.3 | 65.5 | 36.0 | 50.8 | +23.5 |\\n| -w/ *RAP*        | 90.4 | 96.1 | 91.1 | 73.8 | 40.5 | 57.1 | 72.3 | 35.3 | 53.8 | +28.4 |\\n| -w/ *MRD* (ours) | 97.4 | 96.1 | 95.6 | 76.8 | 42.7 | 59.7 | 72.6 | 37.2 | 54.9 | +31.1 |\\n\\n\\n## C. Case Study\\n\\n### C.1. Single-instance Perception Task Examples\\n\\nFigure 10 shows two single-instance perception cases from each HR benchmarks using *RAP* and *MRD* on LLaVA-v1.5-7B. From the first to the last column, we show: the HR image, the *RAP* semantic similarity map, the object detection confidence map, the *MRD* semantic–detection fusion map, the *RAP* result and the *MRD* result. From the\\n\\n\\n\\n\\nvisualization of *RAP* semantic similarity maps, we can observe that due to the crop partitioning, a complete object may be divided across multiple crops, leading to inconsistencies in semantic similarity among different parts of the object. This inconsistency interferes with the subsequent retrieval process. For example, in the second case of *HR-Bench4K*, *RAP* only retrieves the right half of the speed-limit sign, resulting in an incorrect final prediction. In addition, the semantic similarity maps contain many false positives; for instance, in the first case of *HR-Bench8K*, the sky region—irrelevant to the query—shows undesirably high similarity scores.\\n\\n*MRD* addresses these issues by using multi-resolution semantic fusion to correct the semantic inconsistencies across different parts of the object, ensuring its completeness. Moreover, by incorporating an object detection model to directly localize the target, *MRD* reinforces the similarity of the true target region while suppressing false positives. As shown in the figure, the *MRD* semantic–detection fusion map exhibits much clearer contrast between the target and irrelevant regions compared with *RAP*, significantly reducing false positives and enabling more accurate retrieval of the target-related crops during the search process.\\n\\n## C.2. Cross-instance Perception Task Examples\\n\\nFigure 11 shows two cross-instance perception cases from each HR benchmarks using *RAP* and *MRD* on LLaVA-v1.5-7B. In the cross-instance task, the retrieval results show that *RAP* often retains only a subset of the target objects while ignoring others when multiple objects need to be localized. For example, in the first case of *V* * *Bench*, *RAP* completely misses the pink umbrella. This omission becomes more pronounced when there is a large size discrepancy between different target objects. As seen in the second case of *V* * *Bench* and the two cases from *HRBench-8K*, *RAP* tends to keep only the larger primary object while neglecting the smaller ones. Similar issues also appear in counting scenarios (e.g., the two cases in *HRBench-4K*), where *RAP* identifies only a few among multiple instances.\\n\\nIn contrast, *MRD* leverages object detection to simultaneously detect all target objects, ensuring that even small objects are preserved to the greatest extent. This gives *MRD* a clear advantage in cross-instance perception tasks.\\n\\n\\n\\n\\n\\n|                                                                              | HR Image                 | RAP Sim Map           | Det Conf Map           | MRD Fusion Map           | RAP Result           | MRD Result           |\\n| ---------------------------------------------------------------------------- | ------------------------ | --------------------- | ---------------------- | ------------------------ | -------------------- | -------------------- |\\n| V² Bench                                                                     | ![](street_scene.jpg)    | ![](rap_sim_map1.jpg) | ![](det_conf_map1.jpg) | ![](mrd_fusion_map1.jpg) | ![](rap_result1.jpg) | ![](mrd_result1.jpg) |\\n| Query: What is the color of the **helmet**?                                  |                          |                       |                        | *White* ✘                |                      | *Green* ✔            |\\n| V² Bench                                                                     | ![](building_facade.jpg) | ![](rap_sim_map2.jpg) | ![](det_conf_map2.jpg) | ![](mrd_fusion_map2.jpg) | ![](rap_result2.jpg) | ![](mrd_result2.jpg) |\\n| Query: What is the color of the **hat**?                                     |                          |                       |                        | *Black* ✘                |                      | *White* ✔            |\\n| HR-Bench-4K                                                                  | ![](dining_room.jpg)     | ![](rap_sim_map3.jpg) | ![](det_conf_map3.jpg) | ![](mrd_fusion_map3.jpg) | ![](rap_result3.jpg) | ![](mrd_result3.jpg) |\\n| Query: What\\'s the color of the **uniform** of the **figurine on the shelf**? |                          |                       |                        | *Brown* ✘                |                      | *White* ✔            |\\n| HR-Bench-4K                                                                  | ![](cathedral.jpg)       | ![](rap_sim_map4.jpg) | ![](det_conf_map4.jpg) | ![](mrd_fusion_map4.jpg) | ![](rap_result4.jpg) | ![](mrd_result4.jpg) |\\n| Query: What is the speed limit indicated on the **sign** in the image?       |                          |                       |                        | *20 km/h* ✘              |                      | *30 km/h* ✔          |\\n| HR-Bench-8K                                                                  | ![](landscape.jpg)       | ![](rap_sim_map5.jpg) | ![](det_conf_map5.jpg) | ![](mrd_fusion_map5.jpg) | ![](rap_result5.jpg) | ![](mrd_result5.jpg) |\\n| Query: What\\'s the primary color of the **person\\'s clothing** in the image?   |                          |                       |                        | *White* ✘                |                      | *Blue* ✔             |\\n| HR-Bench-8K                                                                  | ![](cityscape.jpg)       | ![](rap_sim_map6.jpg) | ![](det_conf_map6.jpg) | ![](mrd_fusion_map6.jpg) | ![](rap_result6.jpg) | ![](mrd_result6.jpg) |\\n| Query: What\\'s the color of the **trailer**?                                  |                          |                       |                        | *White* ✘                |                      | *Orange* ✔           |\\n\\n\\nFigure 10. Qualitative examples of **Single-instance Perception** task. We conduct experiments using LLaVA-v1.5-7B on three HR Benchmarks.\\n\\n\\n\\n\\n\\n|                                                                                           | HR Image                                                                          | RAP Sim Map        | Det Conf Map        | MRD Fusion Map        | RAP Result                                           | MRD Result                                              |\\n| ----------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------- | ------------------ | ------------------- | --------------------- | ---------------------------------------------------- | ------------------------------------------------------- |\\n| V\\\\* Bench                                                                                 | ![](beach_scene)                                                                  | ![](rap_sim_map_1) | ![](det_conf_map_1) | ![](mrd_fusion_map_1) | ![](rap_result_1)<br/>right side ✗                   | ![](mrd_result_1)<br/>left side ✓                       |\\n|                                                                                           | **Query: Is the yellow umbrella on the left or right side of the pink umbrella?** |                    |                     |                       |                                                      |                                                         |\\n|                                                                                           | ![](tower_scene)                                                                  | ![](rap_sim_map_2) | ![](det_conf_map_2) | ![](mrd_fusion_map_2) | ![](rap_result_2)<br/>right side ✗                   | ![](mrd_result_2)<br/>left side ✓                       |\\n| **Query: Is the dog on the left or right side of the golden tower?**                      |                                                                                   |                    |                     |                       |                                                      |                                                         |\\n| HR-Bench-kK                                                                               | ![](hallway_scene)                                                                | ![](rap_sim_map_3) | ![](det_conf_map_3) | ![](mrd_fusion_map_3) | ![](rap_result_3)<br/>One ✗                          | ![](mrd_result_3)<br/>Two ✓                             |\\n|                                                                                           | **Query: How many chairs are there in the image?**                                |                    |                     |                       |                                                      |                                                         |\\n|                                                                                           | ![](sailboat_scene)                                                               | ![](rap_sim_map_4) | ![](det_conf_map_4) | ![](mrd_fusion_map_4) | ![](rap_result_4)<br/>Two ✗                          | ![](mrd_result_4)<br/>Four ✓                            |\\n| **Query: How many people are there in the boat?**                                         |                                                                                   |                    |                     |                       |                                                      |                                                         |\\n| HR-Bench-8K                                                                               | ![](albert_hall_scene)                                                            | ![](rap_sim_map_5) | ![](det_conf_map_5) | ![](mrd_fusion_map_5) | ![](rap_result_5)<br/>Behind the bus ✗               | ![](mrd_result_5)<br/>To the left of the bus ✓          |\\n|                                                                                           | **Query: What is the position of the Royal Albert Hall relative to the bus?**     |                    |                     |                       |                                                      |                                                         |\\n|                                                                                           | ![](building_scene)                                                               | ![](rap_sim_map_6) | ![](det_conf_map_6) | ![](mrd_fusion_map_6) | ![](rap_result_6)<br/>In front of<br/>the building ✗ | ![](mrd_result_6)<br/>To the left of<br/>the building ✓ |\\n| **Query: What is the position of the car relative to the central tower of the building?** |                                                                                   |                    |                     |                       |                                                      |                                                         |\\n\\n\\nFigure 11. Qualitative examples of *Cross-instance Perception* task.We conduct experiments using LLaVA-v1.5-7B on three HR Benchmarks.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_input = \"data/test/raw/2512.02906v2.pdf\"\n",
    "md_output = \"data/test/processed/document.md\"\n",
    "\n",
    "os.makedirs(\"data/test/processed\", exist_ok=True)\n",
    "\n",
    "step1_llamaparse(pdf_input, md_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cd9155",
   "metadata": {},
   "source": [
    "## Extraction des metadonnée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb7dd8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata_with_llm(md_text_start):\n",
    "    \"\"\"\n",
    "    Sends the first 3000 characters of Markdown to Mistral (via local Ollama)\n",
    "    \"\"\"\n",
    "\n",
    "    text_sample = md_text_start[:3000] + \"\\n\\n... [CONTENU IGNORÉ] ...\\n\\n\"\n",
    "    prompt = \"\"\"You are an expert librarian assistant.\n",
    "    Analyze the beginning of this scientific document (Markdown format) and extract the following information in strict JSON format:\n",
    "    - \"title\": The complete title of the paper.\n",
    "    - \"authors\": The list of authors (separated by commas).\n",
    "    - \"year\": The publication year (if found, otherwise \"Unknown\").\n",
    "    \n",
    "    TEXT:\n",
    "    {text}\n",
    "    \n",
    "    Reply ONLY with the JSON, no additional explanations.\n",
    "    \"\"\".format(text=text_sample)\n",
    "    \n",
    "    response = query_ollama(MODEL, prompt)\n",
    "    \n",
    "    try:\n",
    "        content = response.strip()\n",
    "        metadata = json.loads(content)\n",
    "        if isinstance(metadata.get(\"authors\"), list):\n",
    "            metadata[\"authors\"] = \", \".join(metadata[\"authors\"])\n",
    "\n",
    "        return metadata\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"JSON Parse Error. Response was: {response[:100]}\")\n",
    "        return {\"title\": \"Unknown\", \"authors\": \"Unknown\", \"year\": \"Unknown\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4465e2",
   "metadata": {},
   "source": [
    "## Convertion markdown en donnée json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a3841f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def step2_markdown_to_json(md_path, json_output_path):\n",
    "    print(f\"Chargement du Markdown : {md_path}\")\n",
    "    \n",
    "    with open(md_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        md_text = f.read()\n",
    "\n",
    "    # Extraction métadonnées globales \n",
    "    print(\"Extraction des métadonnées globales via LLM...\")\n",
    "    meta_globales = extract_metadata_with_llm(md_text)\n",
    "    print(f\"   Titré détecté : {meta_globales['title']}\")\n",
    "    print(f\"   Auteurs : {meta_globales['authors']}\")\n",
    "    print(f\"   Année : {meta_globales['year']}\")\n",
    "    print(\"Nettoyage de la bibliographie...\")\n",
    "    # Suppression section References\n",
    "    if \"\\n# References\" in md_text:\n",
    "        md_text = md_text.split(\"\\n# References\")[0]\n",
    "    elif \"\\n## References\" in md_text:\n",
    "        md_text = md_text.split(\"\\n## References\")[0]\n",
    "    \n",
    "    \n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=[\n",
    "            (\"#\", \"Titre\"),\n",
    "            (\"##\", \"Section\"),\n",
    "            (\"###\", \"Sous_Section\"),\n",
    "    ]\n",
    "        )\n",
    "    md_header_splits = markdown_splitter.split_text(md_text)\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "        language=Language.MARKDOWN,\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        add_start_index=True,\n",
    "\n",
    "    )\n",
    "    final_docs = text_splitter.split_documents(md_header_splits)\n",
    "\n",
    "    json_output = []\n",
    "    for doc in final_docs:\n",
    "        chunk_data = {\n",
    "            \"page_content\": doc.page_content,\n",
    "            \"metadata\": {\n",
    "                \"source\": md_path,\n",
    "                # Récupération automatique des métadonnées du MarkdownSplitter\n",
    "                \"section\": doc.metadata.get(\"Section\", \"Introduction\"),\n",
    "                \"sous_section\": doc.metadata.get(\"Sous_Section\", \"\"),\n",
    "                \"titre\": doc.metadata.get(\"Titre\", \"\"),\n",
    "                \"meta_title\": str(meta_globales.get(\"title\", \"Unknown\")),\n",
    "                \"meta_authors\": str(meta_globales.get(\"authors\", \"Unknown\")),\n",
    "                \"meta_year\": str(meta_globales.get(\"year\", \"Unknown\")),\n",
    "            }\n",
    "        }\n",
    "        json_output.append(chunk_data)\n",
    "\n",
    "    # Sauvegarde\n",
    "    with open(json_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Terminé ! {len(json_output)} chunks prêts pour ChromaDB dans {json_output_path}\")\n",
    "\n",
    "    return json_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f378bce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du Markdown : data/test/processed/document.md\n",
      "Extraction des métadonnées globales via LLM...\n",
      "   Titré détecté : MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding\n",
      "   Auteurs : Fan Yang, Kaihao Zhang\n",
      "   Année : Unknown\n",
      "Nettoyage de la bibliographie...\n",
      "Terminé ! 63 chunks prêts pour ChromaDB dans data/test/processed/chunks_for_rag.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'page_content': '| Fan Yang<br/>Harbin Institute of Technology (Shenzhen)<br/>25b951055\\\\@stu.hit.edu.cn | Kaihao Zhang<br/>Harbin Institute of Technology (Shenzhen)<br/>super.khzhang\\\\@gmail.com |\\n| ------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------- |',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': 'Introduction',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': '*Understanding high-resolution images remains a significant challenge for multimodal large language models (MLLMs). Recent study address this issue by dividing the image into smaller crops and computing the semantic similarity between each crop and a query using a pretrained retrieval-augmented generation (RAG) model. The most relevant crops are then selected to localize the target object and suppress irrelevant information. However, such crop-based processing can fragment complete objects across multiple crops, thereby disrupting the computation of semantic similarity. In our experiments, we find that image crops of objects with different sizes are better handled at different resolutions. Based on this observation, we propose Multi-resolution Retrieval-Detection (MRD), a trainingfree framework for high-resolution image understanding. To address the issue of semantic similarity bias caused by objects being split across different image crops, we propose a multi-resolution semantic',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': 'Abstract',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': 'framework for high-resolution image understanding. To address the issue of semantic similarity bias caused by objects being split across different image crops, we propose a multi-resolution semantic fusion method, which integrates semantic similarity maps obtained at different resolutions to produce more accurate semantic information and preserve the integrity of target objects. Furthermore, to achieve direct localization of target objects at a global scale, we introduce an open-vocabulary object detection (OVD) model that identifies object regions using a sliding-window approach.Experiments on high-resolution image understanding benchmarks using different MLLMs demonstrate the effectiveness of our approach.*',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': 'Abstract',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': 'Multimodal Large Language Models (MLLMs) have demonstrated significant advancements in integrating and interpreting visual and linguistic information, enabling robust capabilities in vision-language understanding, reasoning, and interactive tasks [25]. By leveraging visual signals, these models can process and decipher complex visual information, forming a bridge between pixel-level data and  \\n| **Query: Is the red vehicle on the left or right side of the image?**<br/>**High-Resolution Image** |                                                                          | **Semantic Map**<br/>**RAG** |                   |\\n| --------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------ | ---------------------------- | ----------------- |',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '1. Introduction',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': '| MLLM                                                                                                | OVD                                                                      |                              | **Detection Map** |\\n|                                                                                                     | The red vehicle is on the<br/>right side of the image.<br/>**RE-Search** |                              |                   |  \\nFigure 1. Overview of the proposed Multi-resolution Retrieval-Detection framework, which uses RAG and OVD to obtain semantic similarity map and detection confidence map respectively. By integrating the two, the target objects can be localized more accurately.',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '1. Introduction',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': 'semantic interpretation [17, 20]. However, a common practice among most MLLMs is to process input images at fixed and pre-defined resolutions [1, 13, 14]. While this uniform input pipeline simplifies model architecture and reduces computational overhead, it introduces substantial limitations. Specifically, resizing high-resolution (HR) real-world images to a fixed low resolution often leads to shape distortion and blurring, which degrades the quality of fine-grained visual details. Recent studies [11, 19, 22, 24, 29, 30] indicate that existing methods remain unsatisfactory for highresolution image tasks. This is clearly demonstrated by their suboptimal results on dedicated high-resolution image understanding benchmarks [21, 24].',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '1. Introduction',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': 'To address this limitation, improving the high-resolution image perception capability of MLLMs has become an emerging research focus. A common \"locate-and-zoomin\" strategy is widely adopted to enhance detail perception in models. Although training-based approaches such as Supervised Fine-Tuning (SFT) [18] and Reinforcement',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '1. Introduction',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': \"Learning (RL) [30] can effectively identify relevant regions, they are hampered by critical limitations, including high computational costs, long training cycles, and poor cross-architecture transferability, which curtails their scalability and practical application. In contrast, training-free methods [11, 19, 29] automatically locate regions by using attention mechanisms or tree-based search, without requiring the construction of the dataset or the fine-tuning of the model. Despite employing a top-down search strategy from high to low resolution, these methods face significant limitations. A primary issue is the model's inadequate perception of small objects during the initial search stage [19, 21], which frequently leads to the generation of erroneous search paths.\",\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '1. Introduction',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': \"Recently, Inspired by the success of Retrieval-Augmented Generation (RAG) for enabling long-context understanding in general LLMs [9], Wang et al. [22] introduced Retrieval-Augmented Perception (RAP) — a training-free framework designed to enhance MLLMs' perception of high-resolution images. RAP extends this paradigm to the visual domain, achieving significant performance improvement on high-resolution benchmarks. The RAP framework consists of three key components: First, the Visual Retrieval module employs a pre-trained vision RAG model, VisRAG, to compute semantic similarity between the query and different image regions (crops), retrieving the most relevant ones to reduce noise. Next, the Spatial-Awareness Layout module preserves the original relative spatial relationships among the retrieved crops when composing them into the model input, maintaining spatial coherence. Finally, the Adaptive Retrieval-Exploration Search (RE-Search) module dynamically determines the optimal number of\",\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '1. Introduction',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': 'crops when composing them into the model input, maintaining spatial coherence. Finally, the Adaptive Retrieval-Exploration Search (RE-Search) module dynamically determines the optimal number of retrieved crops by constructing a Retrieval-Exploration Tree (RE-Tree), balancing information sufficiency with computational efficiency.',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '1. Introduction',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': 'Despite its promise, the RAP method suffers from several inherent limitations. First, the patching operation can fragment large objects across multiple disjointed crops, disrupting their holistic semantics and leading to biased similarity calculations. Our empirical observations confirm that some patches semantically irrelevant to the query can obtain abnormally high similarity scores. Second, the patch resolution is a critical yet difficult-to-tune hyperparameter: overly large patches introduce redundant background information, while overly small ones exacerbate object fragmentation. Experiments show that the choice of resolution significantly impacts performance. Third, in high-resolution images with cluttered backgrounds, the similarity measure is prone to false positives, where background regions may attain higher similarity than those containing the actual target objects, severely hampering recognition.',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '1. Introduction',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': 'To tackle these challenges, we propose a novel Multi-resolution Retrieval-Detection (MRD) framework, based on RAP to improve retrieval quality and localization accuracy through two key techniques:  \\n• Multi-resolution Semantic Fusion: To mitigate the bias inherent in single-resolution patching, we design a simple yet effective fusion strategy. It computes semantic similarities across multiple proportional resolutions and performs consistency-based fusion to calibrate the results, yielding a more robust and accurate relevance estimation that alleviates semantic deviations caused by object fragmentation.',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '1. Introduction',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': '• Open-vocabulary Detector Enhancement: For more precise target localization, we incorporate an advanced open-vocabulary object detector, LLMDet [7]. First, we leverage the in-context learning capability of LLMs to extract target concepts from the query, defining the categories for the detector. Subsequently, a sliding window mechanism is employed to traverse the entire high-resolution image, detecting target objects within each window to generate a confidence map indicating target presence.  \\nFinally, the calibrated multi-resolution semantic similarity is augmented by the object detection confidence. This synergistic fusion effectively amplifies the response in true target regions, enabling faster and more accurate localization of critical areas during subsequent retrieval, thereby guiding the MLLM toward more reliable inference.',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '1. Introduction',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': \"We conduct extensive experiments on several high-resolution benchmarks, including V* [24], HRBench-4K, and HRBench-8K [21], utilizing various MLLMs such as LLaVA-ov and LLaVA-v1.5. The results demonstrate that our MRD framework surpasses all existing training-free methods and achieves state-of-the-art performance on both single-object and multi-object retrieval and recognition tasks, with particularly notable gains on single-object tasks.  \\nOur contributions are summarized as follows:  \\n• To the best of our knowledge, this is the first work that systematically leverages an open-vocabulary object detector to enhance MLLMs' understanding of high-resolution images. Experiments validate that the detector provides precise target localization, effectively suppressing interference from irrelevant regions.\",\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '1. Introduction',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': '• We propose MRD, a training-free and generic framework. It innovatively corrects semantic similarity via a multi-resolution fusion strategy and integrates open-set detection results to enhance target regions, creating a synergistic effect.  \\n• Comprehensive experiments validate the effectiveness and generalization of our method. It achieves leading performance on both single-object and multi-object tasks across different MLLMs and high-resolution benchmarks.',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '1. Introduction',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': 'MLLMs have rapidly advanced as powerful foundation models capable of understanding and generating multimodal content across diverse vision-language tasks [25, 28]. Early MLLM architectures generally adopt fixed-resolution vision encoders—such as 224 × 224 or 448 × 448 ViTs [2, 12–14]. While this design simplifies training and computation, it inevitably requires resizing or cropping high-resolution (HR) images, thereby discarding fine-grained visual details crucial for tasks such as fine-grained recognition, dense reasoning, or detecting small objects.  \\nTo enhance high-resolution image understanding without proportionally increasing the computational burden from visual tokens, several studies have integrated high-resolution visual encoders into MLLMs. For example, Vary [23] and Deepseek-VL [15] incorporate the SAM encoder [10] to improve model performance on HR images.',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '2. Related Work',\n",
       "   'sous_section': '2.1. Multimodal Large Language Models',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': 'Alternatively, another line of work introduced Native/Dynamic-Resolution MLLMs that processes images at their native resolution. The core idea is to generate a variable-length sequence of visual tokens that adapts to the original dimensions of the input image, thereby preserving spatial fidelity and high-frequency details. These models employ various mechanisms to handle the resulting long sequences and computational complexity, including sliding window attention, dynamic masking, and patch-based encoding strategies. Representative works in this category have demonstrated significant progress. For instance, the InternVL series [4, 6, 31] adopts a strategy of splitting a high-resolution image into multiple fixed-size patches. In contrast, models like Qwen2.5-VL [2, 3] take an end-to-end approach by training a ViT directly on native-resolution images. This allows the model to embed the entire image into a single, coherent token sequence in one forward pass, potentially leading to better',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '2. Related Work',\n",
       "   'sous_section': '2.1. Multimodal Large Language Models',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': 'by training a ViT directly on native-resolution images. This allows the model to embed the entire image into a single, coherent token sequence in one forward pass, potentially leading to better global context understanding.',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '2. Related Work',\n",
       "   'sous_section': '2.1. Multimodal Large Language Models',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': \"Multimodal large language models (MLLMs) have made substantial progress in recent years; however, they continue to face challenges in accurately recognizing and interpreting fine-grained details within high-resolution (HR) images [21?]. To enhance the capability of MLLMs in high-resolution image understanding, existing studies generally follow two main directions. Training-based approaches rely on supervised fine-tuning (SFT) [18, 24] or reinforcement learning (RL) [30], but such methods often compromise the model's generalization ability on broad vision–language tasks. In contrast, training-free approaches [11, 19, 21, 22]typically perform hierarchical or tree-based search to localize target regions. However, these methods tend to suffer from low efficiency and may fail to retain all target objects during the search process, particularly in multi-object scenarios.\",\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '2. Related Work',\n",
       "   'sous_section': '2.2. High-Resolution Image Understanding',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': 'In this section, We first conduct an analysis of the relationship between the resolution of image crops and the performance of MLLMs in subsection 3.2. The experimental results indicate that using different resolution has a significant impact on MLLMs to analyze HR images. Objects of different sizes are suitable for different resolutions. Inspired by this we propose the MRD framework.',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '3. Preliminary',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': 'This section presents the pipeline for integrating Retrieval-Augmented Generation (RAG) into Multimodal Large Language Models (MLLMs) to calculate the semantic similarity scores between the query embedding and images crops from HR images. Given an HR image, we first partition it collection of image patches, denoted as $$P = \\\\{p_1, p_2, \\\\ldots, p_n\\\\}$$, where $$n$$ is the total number of image crops. Following the approach of Yu et al. [27], the textual query and each image crop are encoded independently using the text and image encoders of a Vision-Language Model (VLM), producing a sequence of hidden representations. The semantic similarity score between the query embedding and each image crop embedding is then computed. Specifically, the similarity score $$s(q, p_i)$$ for the i-th crop is calculated by the cosine similarity of the query and image crop embeddings:  \\n$$s(q, p_i) = \\\\frac{1}{2} \\\\cdot (1 + \\\\frac{q \\\\cdot p_i}{\\\\|q\\\\| \\\\cdot \\\\|p_i\\\\|})$$\\n(1)',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '3. Preliminary',\n",
       "   'sous_section': '3.1. Semantic Similarity',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': '$$s(q, p_i) = \\\\frac{1}{2} \\\\cdot (1 + \\\\frac{q \\\\cdot p_i}{\\\\|q\\\\| \\\\cdot \\\\|p_i\\\\|})$$\\n(1)  \\nBased on these scores, the top $$K$$ most relevant image crops are selected and provided to the MLLM to support detailed understanding of the high-resolution input.',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '3. Preliminary',\n",
       "   'sous_section': '3.1. Semantic Similarity',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': 'In this section, we conduct an analysis to investigates the relationship between the resolution of image crops and the performance of MLLMs in HR image understanding.  \\n**Experimental setting.** We analyze the relation between performance and the resolution of image crops, using LLaVA-ov and LLaVA-v1.5 on V * benmark.  \\n**Observations.** We visualize the relationship between the resolution of image crops and performance of MLLMs. As shown in Figure 3, from the overall accuracy obtained by using different resolutions, when the resolution of image crops is set to 112, using different MLLM achieved the highest accuracy rates in both the single-object task and the multi-object task. This indicates that setting the resolution to 112 might be the optimal choice. However, when we take',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '3. Preliminary',\n",
       "   'sous_section': '3.2. Impact of the Resolution of Image Crops',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': '| ![Street scene with half-timbered buildings and baby carriage](image1)                                                                                                                                                                   | ![Building facade detail](image2) | ![Building entrance with baby carriage](image3) | **Semantic Similarity**<br/>![Semantic similarity heatmap showing building in blue tones](heatmap) |                                                     | ![Building detail in green/blue tones](image4) |',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '3. Preliminary',\n",
       "   'sous_section': '3.2. Impact of the Resolution of Image Crops',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': '| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------- | ----------------------------------------------- | -------------------------------------------------------------------------------------------------- | --------------------------------------------------- | ---------------------------------------------- |',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '3. Preliminary',\n",
       "   'sous_section': '3.2. Impact of the Resolution of Image Crops',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': '| **What is the color of the baby carriage?**<br/>A. The color of the baby carriage is black.<br/>B. The color of the baby carriage is green.<br/>C. The color of the baby carriage is red.<br/>D. The color of the baby carriage is blue. |                                   | **Answer: B**<br/>**Resolution=96**             | **Answer: B**<br/>**Resolution=112**                                                               | **Resolution =112**<br/>![Scale from 0 to 1](scale) | ![Sad face emoji](sad_face)                    |',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '3. Preliminary',\n",
       "   'sous_section': '3.2. Impact of the Resolution of Image Crops',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': '|                                                                                                                                                                                                                                          | ![Street scene detail](image5)    | **Answer: B**<br/>**Resolution =144**           | **Answer: A**<br/>**Resolution =224**                                                              |                                                     |                                                |  \\nFigure 2. Setting the resolution of image crops to 112 causes complete objects to be split across different regions, which disrupts the semantic information of the target objects.  \\n| **Attribute**                                                                                      |   |   | **Spatial**                                |   |   |',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '3. Preliminary',\n",
       "   'sous_section': '3.2. Impact of the Resolution of Image Crops',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': '| **Attribute**                                                                                      |   |   | **Spatial**                                |   |   |\\n| -------------------------------------------------------------------------------------------------- | - | - | ------------------------------------------ | - | - |\\n| ![Two line graphs showing Accuracy vs Resolution of Image Crop for different LLaVA models](graphs) |   |   |                                            |   |   |\\n| LLaVA-ov-0.5B LLaVA-v1.5-7B LLaVA-v1.5-13B                                                         |   |   | LLaVA-ov-0.5B LLaVA-v1.5-7B LLaVA-v1.5-13B |   |   |  \\nFigure 3. The effect of the resolution of retrieved image crops on model performance. Attribute and Spatial represent the attribute recognition and spatial reasoning in $$V^*$$ Bench.',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '3. Preliminary',\n",
       "   'sous_section': '3.2. Impact of the Resolution of Image Crops',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': 'Figure 3. The effect of the resolution of retrieved image crops on model performance. Attribute and Spatial represent the attribute recognition and spatial reasoning in $$V^*$$ Bench.  \\na closer look at the results of each sample, we find that in some cases, choosing a different resolution actually leads to more accurate results compared to when the resolution is 112, as shown in Figure 3. The results of the image crops selected based on different resolutions and the visualization of the semantic similarity map can provide a very intuitive analysis of the reasons: Since the complete object is divided into different crops, some parts of it have a higher semantic similarity calculated by VisRAG, while other parts have a lower semantic similarity. After screening, only the parts with higher similarity are retained. However, this will damage the integrity of the target object and cause interference to the judgment of MLLM.',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '3. Preliminary',\n",
       "   'sous_section': '3.2. Impact of the Resolution of Image Crops',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': 'In this section, we propose a novel framework named Multi-resolution Retrieval Detection (MRD). The core design of MRD lies in its multi-resolution approach at different scales to better localize regions containing target objects. This enables subsequent search processes to more easily identify image crops corresponding to the target objects, eliminating irrelevant distractions and enhancing the perceptual understanding of HR images by MLLMs. Based on the findings in subsection 3.2, we argue that using different resolutions for semantic similarity computation is more suitable for objects of varying sizes and locations. Inspired by this idea, we first introduce a simple yet effective Multi-resolution Semantic Fusion method, which computes semantic similarity maps at different resolutions on a local scale and performs consistency-based fusion to refine the semantic similarity and improve its accuracy. To more directly localize target objects, we incorporate an Open-vocabulary object',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '4. Method',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': 'on a local scale and performs consistency-based fusion to refine the semantic similarity and improve its accuracy. To more directly localize target objects, we incorporate an Open-vocabulary object detection model that traverses the entire HR image globally using a sliding window approach, generating confidence scores for regions containing target objects. Finally, by integrating the detection confidence scores with the multi-resolution semantic similarity maps, our method not only improves localization of target regions but also distinguishes fine-grained differences among crops in these regions, thereby assisting subsequent search processes in more accurately identifying key areas. The following sections will provide detailed explanations of each component.',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '4. Method',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': 'In subsection 3.2, we observe that image crops of different resolutions are suitable for objects of varying sizes and locations in different cases. Compared to the semantic similarity map obtained using a single resolution, those derived from multiple resolutions exhibit respective advantages. Therefore, we first propose a Multi-Resolution Semantic Fusion method. As shown in the top part of Figure 4, we partition the HR image using proportional resolutions, with the low resolution set to $$l$$ and the high resolution set to $$\\\\hat{l}$$, where',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '4. Method',\n",
       "   'sous_section': '4.1. Multi-resolution Semantic Fusion',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': 'Figure 4. Detailed information of our proposed *MRD*. First, We use VisRAG with different resolution of image crops to obtain multi-resolution semantic similarity map. We then employ an open-set object detection model, LLMDet, to localize the target objects extracted from the query within the high-resolution image using a sliding-window approach, yielding a global detection confidence map. Finally, the obtained multi-resolution semantic similarity map is linearly fused with the detection confidence map, and the fused scores are used to guide the subsequent search to select image crops containing the target objects.',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '4. Method',\n",
       "   'sous_section': '4.1. Multi-resolution Semantic Fusion',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': '$$\\\\hat{l} = k \\\\cdot l$$. The set of image patches at high resolution is denoted as $$\\\\hat{P} = \\\\{\\\\hat{p}_1, \\\\hat{p}_2, \\\\ldots, \\\\hat{p}_m\\\\}$$, and at low resolution as $$P = \\\\{p_1, p_2, \\\\ldots, p_n\\\\}$$. Due to the proportional relationship between high and low resolutions, we have $$n = k^2 \\\\cdot m$$, and each high-resolution patch $$\\\\hat{p}_i$$ corresponds to $$k^2$$ low-resolution patches:  \\n$$\\\\hat{p}_i = \\\\begin{bmatrix} \\\\tilde{p}_{i,1} & \\\\tilde{p}_{i,2} & \\\\cdots & \\\\tilde{p}_{i,k} \\\\\\\\ \\\\tilde{p}_{i,(k+1)} & \\\\tilde{p}_{i,(k+2)} & \\\\cdots & \\\\tilde{p}_{i,(2k)} \\\\\\\\ \\\\vdots & \\\\vdots & \\\\ddots & \\\\vdots \\\\\\\\ \\\\tilde{p}_{i,(k(k-1)+1)} & \\\\tilde{p}_{i,(k(k-1)+2)} & \\\\cdots & \\\\tilde{p}_{i,k^2} \\\\end{bmatrix}$$ (2)  \\nwhere $$\\\\tilde{p}_{ij} \\\\in P, i \\\\in \\\\{1, 2, \\\\ldots, m\\\\}, \\\\quad j \\\\in \\\\{1, 2, \\\\ldots, k^2\\\\}$$.',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '4. Method',\n",
       "   'sous_section': '4.1. Multi-resolution Semantic Fusion',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': 'where $$\\\\tilde{p}_{ij} \\\\in P, i \\\\in \\\\{1, 2, \\\\ldots, m\\\\}, \\\\quad j \\\\in \\\\{1, 2, \\\\ldots, k^2\\\\}$$.  \\nThen, using Equation 1, we compute the cosine similarity between the query and image crop embeddings to obtain the semantic similarity scores for high and low resolutions, respectively: $$\\\\hat{S} = \\\\{\\\\hat{s}_1, \\\\hat{s}_2, \\\\ldots, \\\\hat{s}_m\\\\}$$ and $$S = \\\\{s_1, s_2, \\\\ldots, s_n\\\\}$$:  \\n$$\\\\hat{s}_i = s(f(q), g(\\\\hat{p}_i)), \\\\quad s_j = s(f(q), g(p_j))$$ (3)  \\nwhere $$f(\\\\cdot)$$ and $$g(\\\\cdot)$$ denote the embedding operations for the query and image crop, respectively, with $$i \\\\in \\\\{1, 2, \\\\ldots, m\\\\}$$ and $$j \\\\in \\\\{1, 2, \\\\ldots, n\\\\}$$. According to the mapping from $$\\\\hat{p}_i$$ to $$p_j$$ in (2), we can map the semantic similarity $$\\\\hat{s}$$ obtained from each high-resolution $$\\\\hat{p}$$ and the query to the corresponding $$k^2$$ low-resolution $$p$$ positions. The mapping operation can be expressed as:  \\n$$\\\\tilde{S} = H(\\\\hat{S})$$ (4)',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '4. Method',\n",
       "   'sous_section': '4.1. Multi-resolution Semantic Fusion',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': '$$\\\\tilde{S} = H(\\\\hat{S})$$ (4)  \\nwhere $$\\\\tilde{S} = \\\\{\\\\tilde{s}_1, \\\\tilde{s}_2, \\\\ldots, \\\\tilde{s}_n\\\\}$$. After obtaining $$\\\\tilde{S}$$ and $$S$$, we perform consistency fusion on the semantic similarities at corresponding positions to obtain the multi-resolution semantic similarity $$S^f = \\\\{s^f_1, s^f_2, \\\\ldots, s^f_n\\\\}$$, which can be expressed as:  \\n$$s^f_t = \\\\sqrt{\\\\tilde{s}_t \\\\cdot s_t}, \\\\quad t \\\\in 1, 2, \\\\ldots, n$$ (5)  \\nFinally, we can transform the semantic similarity scores into a two-dimensional semantic similarity map $$s^f(i, j)$$ with $$i \\\\in \\\\{1, 2, \\\\ldots, H\\\\}$$ and $$j \\\\in \\\\{1, 2, \\\\ldots, W\\\\}$$. The total number of low resolution image crop $$n = H \\\\times W$$.',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '4. Method',\n",
       "   'sous_section': '4.1. Multi-resolution Semantic Fusion',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': 'Fusing the multi-resolution semantic similarity scores from high and low resolutions enables correction of the low-resolution similarities when a complete object is split across different patches in the low-resolution view. This enhances the similarity of various parts of the object, thereby preserving the integrity of the object as much as possible during subsequent search processes and improving the recognition accuracy of the MLLM.',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '4. Method',\n",
       "   'sous_section': '4.1. Multi-resolution Semantic Fusion',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': 'VisRAG divides the HR image into patches and computes semantic similarity between the query and each image crop,',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '4.2. Open-vocabulary Detector Enhancement',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': 'enabling localized object retrieval at a small scale. However, it struggles to accurately localize larger objects. To address this limitation and achieve more direct and large-scale object localization, we introduce an advanced open-vocabulary object detection model—LLMDet—to directly locate regions containing target objects. First, we employ in-context learning with a Large Language Model (LLM) to extract the primary target objects from the query, which serve as the target categories for LLMDet. Due to the extremely high resolution of HR images in datasets such as HR-Bench, we adopt a sliding window strategy for object localization. To align with the semantic similarity map derived from image crops, we assign the detection confidence scores of the target bounding boxes to their corresponding image patches, thereby generating a detection map that reflects the confidence of target object presence in each patch. This detection map offers a more intuitive localization representation',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '4.2. Open-vocabulary Detector Enhancement',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': 'image patches, thereby generating a detection map that reflects the confidence of target object presence in each patch. This detection map offers a more intuitive localization representation compared to the semantic similarity map. In the following, we provide a detailed introduction to the proposed method.',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '4.2. Open-vocabulary Detector Enhancement',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': '**Object Extraction.** To enable open-vocabulary object detection, we first leverages Large Language Models (LLMs) to dynamically identify target objects from textual queries. Given an input query $Q$, we employ in-context learning to extract the primary object entities that serve as detection targets for our LLMDet framework.  \\nFormally, we define the object extraction process as:  \\n$$O = \\\\text{LLM}(\\\\mathcal{P}_{\\\\text{system}}, \\\\mathcal{E}_{\\\\text{examples}}, Q) \\\\tag{6}$$  \\nwhere $O$ represents the set of extracted objects, $\\\\mathcal{P}_{\\\\text{system}}$ denotes the system prompt containing extraction guidelines, and $\\\\mathcal{E}_{\\\\text{examples}}$ constitutes the demonstration examples.',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '4.2. Open-vocabulary Detector Enhancement',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': '**Sliding-window Object Detection.** In order to get a global detection confidence map align with the previously semantic similarity map, we similarly partition the HR image into a grid of $H \\\\times W$ non-overlapping patches where total number of patches $n = H \\\\times W$. A sliding window of size $h \\\\times w$ patches (where $h < H$ and $w < W$) traverses the entire image with a predefined stride. In this way, We can obtain $T$ sliding windows $W = \\\\{W_1, W_2, ..., W_T\\\\}$.  \\nAfter obtaining multiple sliding windows using the sliding window method, we use LLMDet to detect objects within each sliding window. The detector generates a set of bounding boxes $\\\\mathcal{B}_t = \\\\{b_1, b_2, \\\\ldots, b_{K_t}\\\\}$ and the corresponding confidence scores $s_k$ indicating the likelihood of containing a target object where $t$ denotes the $t$-th sliding window.  \\nWe apply a confidence threshold $\\\\tau$ to filter out low-quality detections:',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '4.2. Open-vocabulary Detector Enhancement',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': 'We apply a confidence threshold $\\\\tau$ to filter out low-quality detections:  \\n$$\\\\mathcal{B}_t^{\\\\text{filter}} = \\\\{b_k \\\\in \\\\mathcal{B}_t \\\\mid s_k > \\\\tau\\\\} \\\\tag{7}$$  \\nSubsequently, we generate a window detection confidence map $\\\\mathbf{c}_t^w \\\\in \\\\mathbb{R}^{h \\\\times w}$ for the current window. The value  \\nat patch coordinate $(p, q)$ within this local window is assigned the maximum confidence score among all bounding boxes in $\\\\mathcal{B}_t^{\\\\text{filter}}$ that contain this patch. If no box covers the patch, the confidence is set to 0:  \\n$$\\\\mathbf{c}_t^w(p, q) = \\\\max_{b_k \\\\in \\\\mathcal{B}_t^{\\\\text{filter}}} \\\\{s_k \\\\cdot \\\\mathbb{I}[(m, n) \\\\in b_k]\\\\} \\\\tag{8}$$  \\nwhere $\\\\mathbb{I}[\\\\cdot]$ is the indicator function that equals 1 if the patch $(p, q)$ is inside the bounding box $b_k$, and 0 otherwise.',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '4.2. Open-vocabulary Detector Enhancement',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': 'where $\\\\mathbb{I}[\\\\cdot]$ is the indicator function that equals 1 if the patch $(p, q)$ is inside the bounding box $b_k$, and 0 otherwise.  \\nTo aggregate information from all sliding windows and form a global, unified detection confidence map $\\\\mathbf{c}^g \\\\in \\\\mathbb{R}^{H \\\\times W}$ for the entire high-resolution image, we employ an averaging fusion strategy. For a global patch at coordinate $(p, q)$, its final confidence score is computed as the average of all confidence scores assigned to it from every sliding window that contained it.  \\nFor the patch $I(i, j)$ at position $(i, j)$ in the HR image, if it is contained in the $t$-th sliding window, we denote its position in the $t$-th sliding window $W_t$ as $(t_i, t_j)$, which can be expressed as  \\n$$I(i, j) = W_t(t_i, t_j), \\\\quad t \\\\in \\\\mathcal{T}_{i,j} \\\\tag{9}$$',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '4.2. Open-vocabulary Detector Enhancement',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': '$$I(i, j) = W_t(t_i, t_j), \\\\quad t \\\\in \\\\mathcal{T}_{i,j} \\\\tag{9}$$  \\nwhere $\\\\mathcal{T}_{ij}$ denotes the set of sliding windows that contain $I(i, j)$. Now, we can obtain the global detection confidence map of the whole HR image, which can be computed as:  \\n$$\\\\mathbf{c}^g(i, j) = \\\\frac{1}{|\\\\mathcal{T}_{i,j}|} \\\\sum_{t \\\\in \\\\mathcal{T}_{i,j}} \\\\mathbf{c}_t^w(t_i, t_j) \\\\tag{10}$$  \\nwhere $i \\\\in \\\\{1, 2, \\\\ldots, H\\\\}$ and $j \\\\in \\\\{1, 2, \\\\ldots, W\\\\}$.  \\nThe detection confidence map provides effective localization of target regions on a global scale, offering direct spatial guidance but lacking the ability to distinguish fine-grained differences within the target object. To address this limitation, we integrate the detection confidence with multi-resolution semantic similarity through linear combination, which can be expressed as:  \\n$$\\\\mathbf{s}^F(i, j) = (1 - w) \\\\cdot \\\\mathbf{s}^f(i, j) + w \\\\cdot \\\\mathbf{c}^g(i, j) \\\\tag{11}$$',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '4.2. Open-vocabulary Detector Enhancement',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': '$$\\\\mathbf{s}^F(i, j) = (1 - w) \\\\cdot \\\\mathbf{s}^f(i, j) + w \\\\cdot \\\\mathbf{c}^g(i, j) \\\\tag{11}$$  \\nThis synergistic fusion enables precise target localization while effectively highlighting intra-object variations, thereby facilitating more accurate extraction of key regions in subsequent search processes. The details of the subsequent Retrieved-Exploration Search process can be found in paper [22].',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '4.2. Open-vocabulary Detector Enhancement',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': '**Evaluated benchmark.** We evaluate our *MRD* on two high-resolution benchmarks. The first is $V^*$ *Bench* [24], with an average resolution of $2246 \\\\times 1582$, consists of two sub-tasks: attribute recognition and spatial reasoning. The Second is HRBench which includes two sub-task Fine-grained Single-instance Perception (FSP) and Fine-grained Cross-instance Perception (FCP).  \\nTable 1. Comparison of *MRD* with existing works on high-resolution benchmarks  \\n| Method                       | V\\\\* Bench<br/>Attribute | V\\\\* Bench<br/>Spatial | V\\\\* Bench<br/>Overall | HR-Bench 4K<br/>FSP | HR-Bench 4K<br/>FCP | HR-Bench 4K<br/>Overall | HR-Bench 8K<br/>FSP | HR-Bench 8K<br/>FCP | HR-Bench 8K<br/>Overall |\\n| ---------------------------- | ----------------------- | --------------------- | --------------------- | ------------------- | ------------------- | ----------------------- | ------------------- | ------------------- | ----------------------- |',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '5. Experiments',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': '| *Open-source MLLMs*          |                         |                       |                       |                     |                     |                         |                     |                     |                         |\\n| LLaVA-v1.6-7B \\\\[14]          | 60.9                    | 63.2                  | 61.8                  | 49.0                | 46.8                | 47.9                    | 37.3                | 44.3                | 40.8                    |\\n| LLaVA-v1.6-13B \\\\[14]         | 60.0                    | 64.5                  | 61.8                  | 49.8                | 41.3                | 45.5                    | 38.0                | 38.3                | 38.1                    |\\n| LLaVA-v1.6-34B \\\\[14]         | -                       | -                     | -                     | 55.3                | 50.5                | 52.9                    | 44.5                | 50.3                | 47.4                    |',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '5. Experiments',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': '| LLaVA-HR-X-13B \\\\[16]         | -                       | -                     | -                     | 61.3                | 46.0                | 53.6                    | 49.5                | 44.3                | 46.9                    |\\n| LLaVA-HR-X-7B \\\\[16]          | 51.3                    | 64.5                  | 56.5                  | 57.8                | 46.3                | 52.0                    | 42.0                | 41.3                | 41.6                    |\\n| InternVl-1.5-26B \\\\[5]        | -                       | -                     | -                     | 69.5                | 51.8                | 60.6                    | 69.3                | 48.5                | 57.9                    |\\n| Yi-VL-34B \\\\[26]              | -                       | -                     | -                     | 46.0                | 42.8                | 44.4                    | 39.5                | 38.5                | 39.0                    |',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '5. Experiments',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': '| *Closed-source MLLMs*        |                         |                       |                       |                     |                     |                         |                     |                     |                         |\\n| GPT-4o \\\\[8]                  | -                       | -                     | 66.0                  | 70.0                | 48.0                | 59.0                    | 62.0                | 49.0                | 55.5                    |\\n| Qwen-VL-max \\\\[2]             | -                       | -                     | -                     | 65.0                | **52.0**            | 58.5                    | 54.0                | **51.0**            | 52.5                    |\\n| *Baselines and MRD*          |                         |                       |                       |                     |                     |                         |                     |                     |                         |',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '5. Experiments',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': '| LLaVA-v1.5-7B \\\\[14]          | 43.5                    | 56.6                  | 48.7                  | 38.5                | 33.8                | 36.1                    | 33.0                | 31.3                | 32.1                    |\\n| LLaVA-v1.5-7B-Zoom Eye \\\\[19] | 83.5                    | 82.9                  | 83.3                  | 67.8                | 38.8                | 53.3                    | 65.5                | 36.0                | 50.8                    |\\n| LLaVA-v1.5-7B-RAP \\\\[22]      | 90.4                    | **96.1**              | 91.1                  | 73.8                | 40.5                | 57.1                    | 72.3                | 35.3                | 53.8                    |\\n| **LLaVA-v1.5-7B-MRD (ours)** | **97.4**                | **96.1**              | **95.6**              | **76.8**            | 42.7                | **59.7**                | **72.6**            | 37.2                | **54.9**                |',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '5. Experiments',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': '| LLaVA-ov-0.5B \\\\[14]          | 63.5                    | 64.5                  | 63.9                  | 63.5                | 39.5                | 51.5                    | 47.3                | 38.3                | 42.8                    |\\n| LLaVA-ov-0.5B-Zoom Eye\\\\[19]  | 85.2                    | 73.7                  | 80.6                  | 75.5                | 39.8                | 57.6                    | 68.5                | 38.3                | 53.4                    |\\n| LLaVA-ov-0.5B-RAP \\\\[22]      | 80.0                    | 84.2                  | 83.6                  | 80.3                | 42.3                | 61.3                    | **81.8**            | 45.3                | 63.5                    |\\n| **LLaVA-ov-0.5B-MRD (ours)** | **89.6**                | **82.9**              | **88.0**              | **84.0**            | **45.2**            | **64.6**                | **81.8**            | **47.3**            | **64.5**                |',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '5. Experiments',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': \"| **HR Image**                                                   | **Res = 112** | **Res = 224** | **Multi-res** |   | **Search Result<br/>(Res = 112)** | **Search Result<br/>(Multi-res)** |\\n| -------------------------------------------------------------- | ------------- | ------------- | ------------- | - | --------------------------------- | --------------------------------- |\\n| What is the color of the cyclist's box?                        |               |               |               |   |                                   |                                   |\\n| Is the green bucket on the left or right side of the red tent? |               |               |               |   |                                   |                                   |\\n| **HR Image**                                                   | **Res = 112** | **OVD**       | **RAG+OVD**   |   | **Search Result<br/>(Res = 112)** | **Search Result<br/>(RAG+OVD)**   |\",\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '5. Experiments',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': '| **HR Image**                                                   | **Res = 112** | **OVD**       | **RAG+OVD**   |   | **Search Result<br/>(Res = 112)** | **Search Result<br/>(RAG+OVD)**   |\\n| What is the color of the telephone?                            |               |               |               |   |                                   |                                   |\\n| What is the material of the stool?                             |               |               |               |   |                                   |                                   |  \\nFigure 5. Visualization of the Effects of Different Modules in MRD. Upper: Visualization of the Effects of the Multi-resolution Semantic Fusion Method. Lower: Visualization of the Effects of the Multi-resolution Semantic Fusion Method',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '5. Experiments',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': 'As shown in Table 1, compared with both the baseline MLLMs and previous baseline approaches, our proposed  \\nMRD framework consistently delivers substantial performance gains across all sub-tasks, datasets, and model configurations. The improvement is most pronounced on the V* dataset using the LaVA-v1.5-7B model, where MRD achieves a remarkable 46.9% absolute increase in accuracy—nearly doubling the original performance. Significant gains are also observed on HR-Bench 4K and HR-Bench 8K, with maximum improvements of 23.6% and 22.8%, respectively.  \\nIn comparison to the state-of-the-art baseline RAP, MRD achieves superior performance across all datasets and model settings, yielding an average improvement of 2.8%. When examining results across sub-task categories, MRD demonstrates particularly strong performance on single-object tasks. We attribute this advantage to the integration of a detection module, which provides more accurate localization for isolated objects.',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '5.1. Main Results',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': 'Overall, these results indicate that MRD markedly enhances the perception and understanding capabilities of MLLMs when operating on high-resolution images.',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '5.1. Main Results',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': 'Multi-resolution Semantic Fusion can obtain more accurate information by integrating semantic similarity maps from different resolutions. From the two cases shown in the upper part of Figure 5, we can clearly observe that incorporating multi-resolution semantic fusion allows the high-resolution semantic similarity map to correct the low-resolution map, alleviating semantic deviations caused by different parts of the target object being split across multiple patches. This helps better preserve the integrity of the target object. The results in the cases demonstrate that the approach is effective for both single-object and multi-object tasks. Overall, the experimental results indicate that Multi-resolution Semantic Fusion provides better adaptability to objects of different sizes compared to using a single resolution.',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '5.2. Effect of the Multi-resolution Semantic Fusion',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': \"To achieve more accurate and direct localization of the target object at a global scale, we introduce an open-set object detection model. As shown in lower part of Figure 5, sliding-window detection results effectively identify the target object's location. By combining the detection results with semantic similarity scores, MRD amplifies the scores of patches that contain the target object while suppressing false-positive patches that also exhibit high semantic similarity. This integration facilitates a more efficient and accurate patch retrieval process in subsequent searching.\",\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '5.3. Effect of Open-vocabulary Object Detection',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': 'To better understand the contributions of different modules in our *MRD* framework, we conduct ablation studies  \\nTable 2. Ablation study of different module in *MRD*.  \\n|                   | V\\\\* Bench<br/>Attribute | V\\\\* Bench<br/>Spatial | V\\\\* Bench<br/>Overall | ∆↑   |\\n| ----------------- | ----------------------- | --------------------- | --------------------- | ---- |\\n| RAP               | 80.0                    | 84.2                  | 83.6                  | -    |\\n| OVD               | 84.3                    | 81.6                  | 84.9                  | +1.3 |\\n| RAP+Multi-res     | 82.9                    | 85.2                  | 85.8                  | +2.2 |\\n| RAP+OVD           | 85.2                    | 84.2                  | 86.2                  | +2.6 |\\n| RAP+OVD+Multi-Res | 90.4                    | 85.5                  | 89.3                  | +5.7 |',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '5.4. Ablation Study',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': '| RAP+OVD+Multi-Res | 90.4                    | 85.5                  | 89.3                  | +5.7 |  \\non the V* dataset using the LLaVA-ov-0.5B model. As shown in Table 2, using the OVD model alone (second row) yields higher localization accuracy for single-object tasks, but its performance on multi-object tasks is inferior to RAP. When RAP employs multi-resolution semantic fusion (third row), performance improves on both single-object and multi-object tasks, indicating that multi-resolution semantic fusion can better handle objects of varying sizes across different scenarios.',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '5.4. Ablation Study',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': 'Fusing the semantic similarity map obtained from RAP with the detection confidence map from OVD (fourth row) significantly improves performance on single-object tasks; however, the performance on multi-object tasks is even worse than using OVD alone, suggesting that some target objects may be lost during the search. By further incorporating multi-resolution semantic fusion, performance improves on both single-object and multi-object tasks, demonstrating the effectiveness of this fusion strategy.',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '5.4. Ablation Study',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': \"In summary, introducing OVD helps localize single objects more accurately but may result in missed objects in multi-object scenarios. Multi-resolution semantic fusion corrects semantic similarity scores and preserves object completeness under different conditions, enhancing MLLM performance on both single- and multi-object tasks. The final model, which integrates all modules, achieves a 5.7% higher accuracy than RAP, demonstrating the effectiveness of MRD's design in improving high-resolution image understanding for MLLMs.\",\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '5.4. Ablation Study',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}},\n",
       " {'page_content': 'In this work, we propose a novel training-free method, Multi-resolution Retrieval-Detection (MRD), to enhance the understanding of high-resolution images by MLLMs. MRD employs multi-resolution semantic similarity to correct single-resolution similarity maps, ensuring the integrity of target objects. Moreover, to localize target objects more accurately and directly, we introduce an OVD model that identifies object regions using a sliding-window approach. We demonstrate the effectiveness of MRD across multiple high-resolution benchmarks with different MLLMs, showing its superior performance in HR image understanding.',\n",
       "  'metadata': {'source': 'data/test/processed/document.md',\n",
       "   'section': '6. Conclusion',\n",
       "   'sous_section': '',\n",
       "   'titre': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_title': 'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding',\n",
       "   'meta_authors': 'Fan Yang, Kaihao Zhang',\n",
       "   'meta_year': 'Unknown'}}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_int = \"data/test/processed/document.md\"\n",
    "json_output = \"data/test/processed/chunks_for_rag.json\"\n",
    "\n",
    "os.makedirs(\"data/test/processed\", exist_ok=True)\n",
    "\n",
    "step2_markdown_to_json(md_int, json_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e422ccc",
   "metadata": {},
   "source": [
    "## Fonction pour la baseline et RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ee4d63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "INPUT_DIR = \"data/test/raw\"\n",
    "MD_DIR = \"data/test/intermediate\"\n",
    "OUTPUT_JSON = \"data/test/processed/corpus_final.json\"\n",
    "\n",
    "def run_baseline():\n",
    "    print(f\"Démarrage du pipeline sur : {INPUT_DIR}\")\n",
    "\n",
    "    pdf_files = glob.glob(os.path.join(INPUT_DIR, \"*.pdf\"))\n",
    "    print(f\"{len(pdf_files)} fichiers PDF trouvés.\")\n",
    "\n",
    "    if not pdf_files:\n",
    "        print(\"Aucun PDF trouvé. Vérifie le dossier data/raw !\")\n",
    "        return\n",
    "\n",
    "    grand_corpus_chunks = []\n",
    "    os.makedirs(MD_DIR, exist_ok=True)\n",
    "\n",
    "    for pdf_path in tqdm(pdf_files, desc=\"Traitement des documents\"):\n",
    "        filename = os.path.basename(pdf_path)\n",
    "        base_name = os.path.splitext(filename)[0]\n",
    "        md_output_path = os.path.join(MD_DIR, f\"{base_name}.md\")\n",
    "\n",
    "        try:\n",
    "            if os.path.exists(md_output_path):\n",
    "                print(f\"Markdown existant trouvé pour {filename}, on saute le parsing.\")\n",
    "            else:\n",
    "                step1_llamaparse(pdf_path, md_output_path)\n",
    "\n",
    "            temp_json_path = os.path.join(MD_DIR, f\"{base_name}.json\")\n",
    "            chunks_du_fichier = step2_markdown_to_json(md_output_path, temp_json_path)\n",
    "            grand_corpus_chunks.extend(chunks_du_fichier)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nErreur critique sur {filename} : {e}\")\n",
    "            print(\"   On continue avec le fichier suivant...\")\n",
    "            continue\n",
    "\n",
    "    print(f\"\\nSauvegarde du corpus complet ({len(grand_corpus_chunks)} chunks)...\")\n",
    "    os.makedirs(os.path.dirname(OUTPUT_JSON), exist_ok=True)\n",
    "    with open(OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(grand_corpus_chunks, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Terminé ! Fichier prêt pour ChromaDB : {OUTPUT_JSON}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43e3c4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Démarrage du pipeline sur : data/test/raw\n",
      "1 fichiers PDF trouvés.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des documents:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Envoi du fichier data/test/raw/2512.02906v2.pdf à LlamaParse...\n",
      "Started parsing the file under job_id b1004670-410c-4dfb-a846-77bf7810899b\n",
      "Terminé ! Markdown sauvegardé dans : data/test/intermediate/2512.02906v2.md\n",
      "\n",
      "# MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding\n",
      "\n",
      "| Fan Yang<br/>Harbin Institute of Technology (Shenzhen)<br/>25b951055\\@stu.hit.edu.cn | Kaihao Zhang<br/>Harbin Institute of Technology (Shenzhen)<br/>super.khzhang\\@gmail.com |\n",
      "| ------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------- |\n",
      "\n",
      "\n",
      "## Abstract\n",
      "\n",
      "*Understanding high-resolution ima\n",
      "Chargement du Markdown : data/test/intermediate/2512.02906v2.md\n",
      "Extraction des métadonnées globales via LLM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des documents: 100%|██████████| 1/1 [00:31<00:00, 31.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Titré détecté : MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding\n",
      "   Auteurs : Fan Yang, Kaihao Zhang\n",
      "   Année : Unknown\n",
      "Nettoyage de la bibliographie...\n",
      "Terminé ! 63 chunks prêts pour ChromaDB dans data/test/intermediate/2512.02906v2.json\n",
      "\n",
      "Sauvegarde du corpus complet (63 chunks)...\n",
      "Terminé ! Fichier prêt pour ChromaDB : data/test/processed/corpus_final.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2889dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_baseline(question):\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Baseline (Sans contexte) : '{question}'\")\n",
    "    \n",
    "    prompt = f\"Réponds à cette question de manière concise :\\n\\n{question}\"\n",
    "    \n",
    "    response = query_ollama(MODEL, prompt)\n",
    "    duration = time.time() - start_time\n",
    "\n",
    "    print(\"\\n Réponse Baseline (Mistral) :\")\n",
    "    print(response)\n",
    "    print(f\"Temps total : {duration:.2f}s\")\n",
    "\n",
    "    return response, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6955d202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (Sans contexte) : 'Qu'est-ce que la méthode MRD propose ?'\n",
      "\n",
      " Réponse Baseline (Mistral) :\n",
      "La méthode MRD (Maximal Redundancy Decomposition) est une technique d'analyse multivariée qui permet d'extraire un sous-ensemble optimisé de variables indépendantes les unes des autres pour expliquer le plus possible de la variance totale dans un ensemble de données.\n",
      "Temps total : 19.18s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\"La méthode MRD (Maximal Redundancy Decomposition) est une technique d'analyse multivariée qui permet d'extraire un sous-ensemble optimisé de variables indépendantes les unes des autres pour expliquer le plus possible de la variance totale dans un ensemble de données.\",\n",
       " 19.175695657730103)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Qu'est-ce que la méthode MRD propose ?\"\n",
    "run_baseline(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26939a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "import shutil\n",
    "DB_PATH = \"./db_local\"\n",
    "INPUT_JSON = \"data/test/processed/chunks_for_rag.json\"\n",
    "\n",
    "def step3_create_vector_db():\n",
    "    print(f\"Chargement des chunks depuis : {INPUT_JSON}\")\n",
    "    print(f\"Base de données vectorielle sauvegardée avec succès dans : {DB_PATH}\")\n",
    "\n",
    "    if os.path.exists(DB_PATH):\n",
    "        print(f\"Suppression de l'ancienne DB corrompue...\")\n",
    "        shutil.rmtree(DB_PATH, ignore_errors=True)\n",
    "\n",
    "    os.makedirs(DB_PATH, exist_ok=True)\n",
    "    os.chmod(DB_PATH, 0o777)\n",
    "        \n",
    "        \n",
    "    if not os.path.exists(INPUT_JSON):\n",
    "        print(f\"Erreur : Le fichier {INPUT_JSON} n'existe pas. Lance le Step 2 d'abord.\")\n",
    "        return\n",
    "    \n",
    "\n",
    "    # Lire JSON\n",
    "    with open(INPUT_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "        chunks_data = json.load(f)\n",
    "\n",
    "    # Reconvertir JSON en objets \"Document\" LangChain\n",
    "    documents = []\n",
    "    for item in chunks_data:\n",
    "        doc = Document(\n",
    "            page_content=item[\"page_content\"],\n",
    "            metadata=item[\"metadata\"],\n",
    "            \n",
    "        )\n",
    "        documents.append(doc)\n",
    "    \n",
    "    print(f\"{len(documents)} documents prêts à être vectorisés.\")\n",
    "\n",
    "    # Embedding\n",
    "    print(f\"Initialisation de l'embedding ({EMBEDDING_MODEL})...\")\n",
    "    embeddings = OllamaEmbeddings(\n",
    "        model=EMBEDDING_MODEL,\n",
    "        base_url=SERVER,\n",
    "    )\n",
    "    vector = embeddings.embed_query(\"Test\")\n",
    "    \n",
    "    print(f\"Succes ! Ollama a renvoyé un vecteur de taille {len(vector)}.\")\n",
    "    # DB Chroma\n",
    "    print(\"Création des vecteurs et indexation (Cela peut prendre un moment)...\")\n",
    "    vector_db = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=DB_PATH,\n",
    "        collection_name=\"pdf_rag_collection\",\n",
    "    )\n",
    "    \n",
    "    print(f\"Base de données vectorielle sauvegardée avec succès dans : {DB_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fba33f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des chunks depuis : data/test/processed/chunks_for_rag.json\n",
      "Base de données vectorielle sauvegardée avec succès dans : ./db_local\n",
      "Suppression de l'ancienne DB corrompue...\n",
      "63 documents prêts à être vectorisés.\n",
      "Initialisation de l'embedding (nomic-embed-text)...\n",
      "Succes ! Ollama a renvoyé un vecteur de taille 768.\n",
      "Création des vecteurs et indexation (Cela peut prendre un moment)...\n",
      "Base de données vectorielle sauvegardée avec succès dans : ./db_local\n"
     ]
    }
   ],
   "source": [
    "step3_create_vector_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fc74900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rag(question):\n",
    "    \"\"\"\n",
    "    Exécute le RAG pour une question donnée sans boucle interactive.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    embeddings = OllamaEmbeddings(\n",
    "        model=EMBEDDING_MODEL, \n",
    "        base_url=SERVER\n",
    "    )\n",
    "    \n",
    "    vector_db = Chroma(\n",
    "        embedding_function=embeddings,\n",
    "        persist_directory=DB_PATH,\n",
    "        collection_name=\"pdf_rag_collection\",\n",
    "    )\n",
    "    \n",
    "    print(f\"RAG sur : '{question}'\")\n",
    "\n",
    "    # Recherche dans la DB\n",
    "    results = vector_db.similarity_search(question, k=5)\n",
    "    \n",
    "    if not results:\n",
    "        return \"Aucune info trouvée dans la base.\"\n",
    "\n",
    "    # Construction du contexte\n",
    "    context_str = \"\"\n",
    "    sources_list = []\n",
    "    \n",
    "    for doc in results:\n",
    "        meta = doc.metadata\n",
    "        titre = meta.get('meta_title', 'Doc')\n",
    "        section = meta.get('section', 'Section Inconnue')\n",
    "        source_info = f\"{titre} > {section}\"\n",
    "        \n",
    "        sources_list.append(source_info)\n",
    "        context_str += f\"---\\n[Source: {source_info}]\\n{doc.page_content}\\n\"\n",
    "\n",
    "    # Prompt final\n",
    "    final_prompt = f\"\"\"Tu es un assistant scientifique expert. \n",
    "    Utilise EXCLUSIVEMENT le contexte ci-dessous pour répondre à la question.\n",
    "    Si la réponse contient des éléments visuels (graphiques), décris-les d'après le texte.\n",
    "\n",
    "    CONTEXTE :\n",
    "    {context_str}\n",
    "\n",
    "    QUESTION : \n",
    "    {question}\n",
    "\n",
    "    RÉPONSE :\"\"\"\n",
    "\n",
    "    # Génération\n",
    "    print(\"Model : \" + MODEL)\n",
    "    reponse = query_ollama(MODEL, final_prompt)\n",
    "    \n",
    "    # Affichage et retour\n",
    "    print(\"\\n\" + reponse)\n",
    "    \n",
    "    # Stats\n",
    "    duration = time.time() - start_time\n",
    "    print(f\"\\nSources utilisées ({len(results)} chunks) :\")\n",
    "    for src in set(sources_list):\n",
    "        print(f\"    - {src}\")\n",
    "    print(f\"Temps total : {duration:.2f}s\")\n",
    "    \n",
    "    return reponse, duration, sources_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a14d9e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG sur : 'Qu'est-ce que la méthode MRD propose ?'\n",
      "Model : mistral\n",
      "\n",
      "La méthode MRD (Multi-resolution Retrieval-Detection Fusion) est une méthode de fusion de recherche et détection multi-résolution, conçue pour améliorer l'entendement des images à haute résolution par les réseaux de machines à learning multi-tâches (MLLMs).\n",
      "\n",
      "Elle utilise deux composants principaux : RAG (Retrieval Augmented Graph) et OVD (Object Vertex Detection), qui permettent respectivement d'obtenir une carte de similitude sémantique et une carte de confiance de détection. Par intégration des deux composants, les objets cibles peuvent être localisés plus précisément (Figure 1).\n",
      "\n",
      "OVD aide à localiser plus précisément un objet unique mais peut entraîner l'omission d'objets dans des scénarios avec plusieurs objets. Le fusion sémantique multi-résolution corrige les scores de similitude sémantique et préserve la complétude des objets sous différentes conditions, améliorant ainsi le rendement de MLLM sur les tâches à un objet unique et multiples. Le modèle final, qui intègre tous les modules, atteint une augmentation de 5.7% de précision par rapport à RAP (Retrieval Augmented Graph), démontrant ainsi l'efficacité de la conception de MRD pour améliorer l'entendement des images à haute résolution pour les réseaux de machines à learning multi-tâches.\n",
      "\n",
      "Les auteurs de cette méthode sont Fan Yang et Kaihao Zhang, tous deux du Harbin Institute of Technology (Shenzhen).\n",
      "\n",
      "Sources utilisées (5 chunks) :\n",
      "    - MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > 5.4. Ablation Study\n",
      "    - MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > 1. Introduction\n",
      "    - MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > Introduction\n",
      "    - MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > 5. Experiments\n",
      "Temps total : 248.88s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\"La méthode MRD (Multi-resolution Retrieval-Detection Fusion) est une méthode de fusion de recherche et détection multi-résolution, conçue pour améliorer l'entendement des images à haute résolution par les réseaux de machines à learning multi-tâches (MLLMs).\\n\\nElle utilise deux composants principaux : RAG (Retrieval Augmented Graph) et OVD (Object Vertex Detection), qui permettent respectivement d'obtenir une carte de similitude sémantique et une carte de confiance de détection. Par intégration des deux composants, les objets cibles peuvent être localisés plus précisément (Figure 1).\\n\\nOVD aide à localiser plus précisément un objet unique mais peut entraîner l'omission d'objets dans des scénarios avec plusieurs objets. Le fusion sémantique multi-résolution corrige les scores de similitude sémantique et préserve la complétude des objets sous différentes conditions, améliorant ainsi le rendement de MLLM sur les tâches à un objet unique et multiples. Le modèle final, qui intègre tous les modules, atteint une augmentation de 5.7% de précision par rapport à RAP (Retrieval Augmented Graph), démontrant ainsi l'efficacité de la conception de MRD pour améliorer l'entendement des images à haute résolution pour les réseaux de machines à learning multi-tâches.\\n\\nLes auteurs de cette méthode sont Fan Yang et Kaihao Zhang, tous deux du Harbin Institute of Technology (Shenzhen).\",\n",
       " 248.88378143310547,\n",
       " ['MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > 5. Experiments',\n",
       "  'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > 5.4. Ablation Study',\n",
       "  'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > 5.4. Ablation Study',\n",
       "  'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > 1. Introduction',\n",
       "  'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > Introduction'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Qu'est-ce que la méthode MRD propose ?\"\n",
    "run_rag(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81501df",
   "metadata": {},
   "source": [
    "## Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c9b10a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from sentence_transformers import CrossEncoder\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "import time\n",
    "\n",
    "def run_rag_with_reranking(question):\n",
    "    print(f\"RAG Reranking sur : '{question}'\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Retrieval large de documents (10)\n",
    "    embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=SERVER)\n",
    "    vector_db = Chroma(embedding_function=embeddings, persist_directory=DB_PATH, collection_name=\"pdf_rag_collection\")\n",
    "    initial_docs = vector_db.similarity_search(question, k=10)\n",
    "    \n",
    "    if not initial_docs:\n",
    "        return \"Pas de docs\", 0, []\n",
    "\n",
    "    # Reranking\n",
    "    reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "    pairs = [[question, doc.page_content] for doc in initial_docs]\n",
    "    scores = reranker.predict(pairs)\n",
    "    \n",
    "    # On trie du meilleur score au pire\n",
    "    ranked_results = sorted(zip(initial_docs, scores), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Selection du top 3\n",
    "    top_docs = [doc for doc, score in ranked_results[:3]]\n",
    "    \n",
    "    # Generer réponse\n",
    "    context_str = \"\\n\".join([d.page_content for d in top_docs])\n",
    "    prompt = f\"Tu es un expert. Contexte :\\n{context_str}\\n\\nQuestion : {question}\\nRéponse :\"\n",
    "    response = query_ollama(MODEL, prompt)\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    # Sources formatées\n",
    "    sources_format = []\n",
    "    for doc in top_docs:\n",
    "        meta = doc.metadata\n",
    "        titre = meta.get('meta_title', meta.get('source', 'Doc'))\n",
    "        if \"/\" in titre:\n",
    "            titre = titre.split(\"/\")[-1]\n",
    "            \n",
    "        section = meta.get('section', 'Section Inconnue')\n",
    "        \n",
    "        source_info = f\"{titre} > {section}\"\n",
    "        sources_format.append(source_info)\n",
    "    \n",
    "    return response, duration, sources_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbd65a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Reranking sur : 'Qu'est-ce que la méthode MRD propose ?'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\"La méthode MRD (Multi-Resolution Diffusion) propose un cadre pour améliorer la compréhension de images à haute résolution par les modèles de langage multimodal (MLLMs). Il intègre différents modules, chacun avec une fonction spécifique :\\n\\n1. OVD (Object Visual Detection) : Permet d'identifier plus précisément un objet isolé, mais peut manquer des objets dans les scénarios multi-objets.\\n2. Multi-resolution semantic fusion : Corrige les scores de similarité sématique et préserve l'intégrité des objets sous différentes conditions, améliorant ainsi le rendement du MLLM sur les tâches uni- et multi-objets.\\n3. RAG (Region Attention Generation) : Permet d'attribuer une attention à différentes régions de l'image pour faciliter la compréhension semantique globale.\\n4. HR Image : Utilise des images à haute résolution pour améliorer la performance du MLLM.\\n\\nL'intégration de tous les modules dans le modèle final permet d'atteindre une précision plus élevée de 5,7% par rapport au RAP (Rapid Attention Pooling), démontrant l'efficacité du cadre MRD pour améliorer la compréhension de images à haute résolution dans les MLLMs.\\nL'étude d'ablation montre que chaque module contribue à des progrès différents :\\n- OVD améliore les scores de 1,3% sur l'ensemble des évaluations (V\\\\* Bench Overall).\\n- La fusion multi-résolution permet une augmentation de 2,2% sur V\\\\* Bench Spatial et 2,6% sur V\\\\* Bench Overall.\\n- L'intégration de tous les modules améliore les scores d'environ 5,7%.\",\n",
       " 194.33643412590027,\n",
       " ['MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > 5.4. Ablation Study',\n",
       "  'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > 5.4. Ablation Study',\n",
       "  'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > 5. Experiments'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Qu'est-ce que la méthode MRD propose ?\"\n",
    "run_rag_with_reranking(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54d34a9",
   "metadata": {},
   "source": [
    "## Bonus Recherche hybride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27890f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "\n",
    "def run_rag_hybrid(question):\n",
    "    \"\"\"\n",
    "    RAG Hybride : combine recherche vectorielle et lexicale (BM25) manuellement\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=SERVER)\n",
    "    vector_db = Chroma(\n",
    "        embedding_function=embeddings,\n",
    "        persist_directory=DB_PATH,\n",
    "        collection_name=\"pdf_rag_collection\",\n",
    "    )\n",
    "    \n",
    "    print(f\"RAG Hybride sur : '{question}'\")\n",
    "    \n",
    "    all_docs_data = vector_db.get()\n",
    "    documents = []\n",
    "    for i, content in enumerate(all_docs_data['documents']):\n",
    "        doc = Document(\n",
    "            page_content=content,\n",
    "            metadata=all_docs_data['metadatas'][i]\n",
    "        )\n",
    "        documents.append(doc)\n",
    "        \n",
    "    bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "    bm25_retriever.k = 5\n",
    "    \n",
    "    results_bm25 = bm25_retriever.invoke(question)\n",
    "    results_vector = vector_db.similarity_search(question, k=5)\n",
    "    \n",
    "    seen = set()\n",
    "    results = []\n",
    "    \n",
    "    for doc in results_bm25 + results_vector:\n",
    "        doc_id = doc.page_content[:100]\n",
    "        if doc_id not in seen:\n",
    "            seen.add(doc_id)\n",
    "            results.append(doc)\n",
    "            if len(results) >= 5:\n",
    "                break\n",
    "    \n",
    "    if not results:\n",
    "        return \"Pas de docs\", 0, []\n",
    "    \n",
    "    context_str = \"\\n\".join([d.page_content for d in results])\n",
    "    prompt = f\"Tu es un expert. Contexte :\\n{context_str}\\n\\nQuestion : {question}\\nReponse :\"\n",
    "    \n",
    "    reponse = query_ollama(MODEL, prompt)\n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    sources_list = []\n",
    "    for doc in results:\n",
    "        meta = doc.metadata\n",
    "        titre = meta.get('meta_title', 'Doc')\n",
    "        if \"/\" in titre:\n",
    "            titre = titre.split(\"/\")[-1]\n",
    "        section = meta.get('section', 'Section Inconnue')\n",
    "        source_info = f\"{titre} > {section}\"\n",
    "        sources_list.append(source_info)\n",
    "    \n",
    "    return reponse, duration, sources_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04398112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Hybride sur : 'Qu'est-ce que la méthode MRD propose ?'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\"La méthode MRD (Multi-resolution Retrieval Detection) propose une méthode sans formation, appelée Multi-resolution Retrieval-Detection, pour améliorer la compréhension des images à haute résolution par les MLLMs (Modèles de langage large pré-entraînés). Elle utilise une similitude semantique multi-résolution pour corriger les cartes de similarité semantique uni-résolution, ce qui garantit l'intégrité des objets cibles. De plus, elle introduit un modèle OVD (Open-vocabulary object) pour identifier les régions d'objets à l'aide d'un approche fenêtre glissante. La méthode MRD améliore significativement la compréhension des images à haute résolution par les MLLMs en comparaison aux approches précédentes et aux modèles de base.\",\n",
       " 140.19953536987305,\n",
       " ['MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > 6. Conclusion',\n",
       "  'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > 3. Preliminary',\n",
       "  'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > 5.1. Main Results',\n",
       "  'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > 5.1. Main Results',\n",
       "  'MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > 4. Method'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Qu'est-ce que la méthode MRD propose ?\"\n",
    "run_rag_hybrid(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfc092e",
   "metadata": {},
   "source": [
    "# Test complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f06ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstration_complete():\n",
    "    \"\"\"\n",
    "    Démonstration complète avec sauvegarde détaillée pour visualisation Streamlit\n",
    "    Sauvegarde en CSV et JSON pour maximum de flexibilité\n",
    "    \"\"\"\n",
    "    questions = [\n",
    "        \"Qu'est-ce que la méthode MRD propose ?\",\n",
    "        \"Quels sont les résultats expérimentaux principaux ?\",\n",
    "        \"Quelle est la différence avec les méthodes existantes ?\",\n",
    "        \"Quels datasets sont utilisés pour l'évaluation ?\",\n",
    "        \"Quelles sont les limitations de l'approche ?\"\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for idx, q in enumerate(questions, 1):\n",
    "        print(f\"QUESTION {idx}/{len(questions)} : {q}\")\n",
    "        \n",
    "        # Baseline\n",
    "        print(\"\\n[BASELINE]\")\n",
    "        resp_baseline, time_baseline = run_baseline(q)\n",
    "        \n",
    "        # RAG\n",
    "        print(\"\\n[RAG]\")\n",
    "        resp_rag, time_rag, src_rag = run_rag(q)\n",
    "\n",
    "        # RAG Hybrid\n",
    "        print(\"\\n[RAG HYBRID]\")\n",
    "        resp_rag_hybrid, time_rag_hybrid, src_rag_hybrid = run_rag_hybrid(q)\n",
    "        \n",
    "        # Reranking\n",
    "        print(\"\\n[RERANKING]\")\n",
    "        resp_rag_rerank, time_rag_rerank, src_rag_rerank = run_rag_with_reranking(q)\n",
    "        \n",
    "        # Stockage complet des résultats\n",
    "        results.append({\n",
    "            'id': idx,\n",
    "            'question': q,\n",
    "            \n",
    "            # Baseline\n",
    "            'baseline_response': resp_baseline,\n",
    "            'baseline_time': round(time_baseline, 2),\n",
    "            'baseline_length': len(resp_baseline),\n",
    "            \n",
    "            # RAG\n",
    "            'rag_response': resp_rag,\n",
    "            'rag_time': round(time_rag, 2),\n",
    "            'rag_length': len(resp_rag),\n",
    "            'rag_sources_count': len(src_rag),\n",
    "            'rag_sources': ' | '.join(src_rag) if src_rag else 'Aucune',\n",
    "\n",
    "            # RAG Hybrid\n",
    "            'rag_hybrid_response': resp_rag_hybrid,\n",
    "            'rag_hybrid_time': round(time_rag_hybrid, 2),\n",
    "            'rag_hybrid_length': len(resp_rag_hybrid),\n",
    "            'rag_hybrid_sources_count': len(src_rag_hybrid),\n",
    "            'rag_hybrid_sources': ' | '.join(src_rag_hybrid) if src_rag_hybrid else 'Aucune',\n",
    "            \n",
    "            # Reranking\n",
    "            'rerank_response': resp_rag_rerank,\n",
    "            'rerank_time': round(time_rag_rerank, 2),\n",
    "            'rerank_length': len(resp_rag_rerank),\n",
    "            'rerank_sources_count': len(src_rag_rerank),\n",
    "            'rerank_sources': ' | '.join(src_rag_rerank) if src_rag_rerank else 'Aucune',\n",
    "        })\n",
    "    \n",
    "    # Sauvegarde DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Sauvegarde CSV (meilleur pour Streamlit - facile à lire)\n",
    "    df.to_csv('data/test/processed/demo_results.csv', index=False, encoding='utf-8')\n",
    "    print(f\"\\nResultats sauvegardes dans data/test/processed/demo_results.csv\")\n",
    "    \n",
    "    # Sauvegarde JSON (structure pour manipulation programmatique)\n",
    "    with open('data/test/processed/demo_results.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Resultats sauvegardes dans data/test/processed/demo_results.json\")\n",
    "    \n",
    "    # Sauvegarde Excel (pour lecture humaine facile)\n",
    "    df.to_excel('data/test/processed/demo_results.xlsx', index=False)\n",
    "    print(f\"Resultats sauvegardes dans data/test/processed/demo_results.xlsx\")\n",
    "    \n",
    "    # Afficher résumé\n",
    "    print(\"\\n\" + \"-\"*10)\n",
    "    print(\"Resume :\")\n",
    "    print(\"-\"*10)\n",
    "    print(f\"\\nTemps moyen :\")\n",
    "    print(f\"  Baseline  : {df['baseline_time'].mean():.2f}s\")\n",
    "    print(f\"  RAG       : {df['rag_time'].mean():.2f}s\")\n",
    "    print(f\"  RAG Hybrid: {df['rag_hybrid_time'].mean():.2f}s\")\n",
    "    print(f\"  Reranking : {df['rerank_time'].mean():.2f}s\")\n",
    "    \n",
    "    print(f\"\\nLongueur moyenne des reponses :\")\n",
    "    print(f\"  Baseline  : {df['baseline_length'].mean():.0f} caracteres\")\n",
    "    print(f\"  RAG       : {df['rag_length'].mean():.0f} caracteres\")\n",
    "    print(f\"  RAG Hybrid: {df['rag_hybrid_length'].mean():.0f} caracteres\")\n",
    "    print(f\"  Reranking : {df['rerank_length'].mean():.0f} caracteres\")\n",
    "    \n",
    "    print(f\"\\nSources moyennes :\")\n",
    "    print(f\"  RAG       : {df['rag_sources_count'].mean():.1f} sources\")\n",
    "    print(f\"  RAG Hybrid: {df['rag_hybrid_sources_count'].mean():.1f} sources\")\n",
    "    print(f\"  Reranking : {df['rerank_sources_count'].mean():.1f} sources\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc1cb685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "QUESTION 1/5 : Qu'est-ce que la méthode MRD propose ?\n",
      "============================================================\n",
      "\n",
      "[BASELINE]\n",
      "Baseline (Sans contexte) : 'Qu'est-ce que la méthode MRD propose ?'\n",
      "\n",
      " Réponse Baseline (Mistral) :\n",
      "La méthode MRD (Mean Residual Disease) est un outil utilisé en oncologie pour évaluer la réponse à un traitement anticancéreux. Il s'agit de mesurer le taux résiduel de maladie dans les cellules tumorales restantes après une première phase de traitement pour déterminer la réponse au traitement (complete response, partial response, stable disease ou progressive disease) et ainsi guider le choix du traitement suivant.\n",
      "Temps total : 28.08s\n",
      "\n",
      "[RAG]\n",
      "RAG sur : 'Qu'est-ce que la méthode MRD propose ?'\n",
      "Model : mistral\n",
      "\n",
      "La méthode MRD (Multi-resolution Retrieval-Detection Fusion) propose une méthode de fusion pour l'entendre des images à haute résolution par les réseaux de neurones profonds multi-échelon (MLLMs). Elle utilise deux modules principaux : RAG et OVD. RAG (Retrieval-Augmented Guidance) est utilisé pour obtenir une carte de similitude sémantique, tandis que OVD (Objectness Verification Detection) fournit une carte de confiance de détection d'objets. En intégrant ces deux modules, les objets cibles peuvent être localisés avec plus de précision. En plus, MRD améliore considérablement la compréhension des images à haute résolution pour les MLLMs en ajoutant un processus d'analyse multiresolution et en améliorant la localisation des objets similaires.\n",
      "\n",
      "Sources utilisées (5 chunks) :\n",
      "    - MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > 5.4. Ablation Study\n",
      "    - MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > 1. Introduction\n",
      "    - MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > Introduction\n",
      "    - MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > 5. Experiments\n",
      "Temps total : 200.27s\n",
      "\n",
      "[RAG HYBRID]\n",
      "RAG Hybride sur : 'Qu'est-ce que la méthode MRD propose ?'\n",
      "\n",
      "[RERANKING]\n",
      "RAG Reranking sur : 'Qu'est-ce que la méthode MRD propose ?'\n",
      "\n",
      "============================================================\n",
      "QUESTION 2/5 : Quels sont les résultats expérimentaux principaux ?\n",
      "============================================================\n",
      "\n",
      "[BASELINE]\n",
      "Baseline (Sans contexte) : 'Quels sont les résultats expérimentaux principaux ?'\n",
      "\n",
      " Réponse Baseline (Mistral) :\n",
      "Les résultats expérimentaux principaux dépendent du contexte de l'expérience en question. Il est donc impossible de fournir des réponses précises sans connaître le sujet particulier de la recherche. Cependant, en général, les résultats expérimentaux peuvent inclure des observations, des données statistiques, des tendances ou des prédictions basées sur des recherches ou des tests. Ils peuvent également s'étendre à des découvertes nouvelles, l'annonce de nouveaux produits, la validation ou la réfutation d'hypothèses et l'avancement du savoir dans un domaine donné.\n",
      "Temps total : 39.70s\n",
      "\n",
      "[RAG]\n",
      "RAG sur : 'Quels sont les résultats expérimentaux principaux ?'\n",
      "Model : mistral\n",
      "\n",
      "Les résultats expérimentaux principaux du travail \"Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding\" indiquent que MRD (MRD) sensiblement améliore les capacités de perception et d'intellection des réseaux de neurones profonds multi-couches (MLLMs) lorsqu'ils sont utilisés pour traiter des images à haute résolution. Ces résultats sont mentionnés dans la source suivante : [Source: MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > 5.1. Main Results].\n",
      "\n",
      "Sources utilisées (5 chunks) :\n",
      "    - MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > Introduction\n",
      "    - MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > 5.1. Main Results\n",
      "    - MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > 3. Preliminary\n",
      "    - MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > 5.4. Ablation Study\n",
      "    - MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > 5. Experiments\n",
      "Temps total : 175.34s\n",
      "\n",
      "[RAG HYBRID]\n",
      "RAG Hybride sur : 'Quels sont les résultats expérimentaux principaux ?'\n",
      "\n",
      "[RERANKING]\n",
      "RAG Reranking sur : 'Quels sont les résultats expérimentaux principaux ?'\n",
      "\n",
      "============================================================\n",
      "QUESTION 3/5 : Quelle est la différence avec les méthodes existantes ?\n",
      "============================================================\n",
      "\n",
      "[BASELINE]\n",
      "Baseline (Sans contexte) : 'Quelle est la différence avec les méthodes existantes ?'\n",
      "\n",
      " Réponse Baseline (Mistral) :\n",
      "Il est difficile de répondre de manière concise sans avoir connaissance précise des méthodes existantes que vous voulez comparer. En général, la principale différence des nouveaux développements dans le domaine du machine learning peut être l'utilisation de nouvelles techniques pour améliorer les résultats, réduire le temps d'exécution ou simplifier la configuration et l'entraînement des modèles. Par exemple, il y a eu un grand progrès dans l'utilisation de l'apprentissage profond (deep learning) et de l'intelligence artificielle (AI) pour résoudre des problèmes plus complexes. D'autres nouveaux développements incluent l'apprentissage sans étiquetage (unsupervised learning), l'apprentissage à partir de données imbalancées et l'utilisation d'architectures neurales différentes comme les réseaux neuronaux auto-encodants ou les réseaux neuronaux convolutifs déconvolutifs.\n",
      "Temps total : 63.69s\n",
      "\n",
      "[RAG]\n",
      "RAG sur : 'Quelle est la différence avec les méthodes existantes ?'\n",
      "Model : mistral\n",
      "\n",
      "La méthode MRD (Multi-resolution Retrieval-Detection Fusion) présente une amélioration par rapport aux méthodes existantes en termes de compréhension des images à haute résolution pour les réseaux de machines apprenants multi-scales (MLLMs). Selon l'article, la méthode MRD améliore la localisation précise d'objets individuels mais peut entraîner des objets manqués dans les scénarios à plusieurs objets. De plus, le fusionnement multi-résolutionnel de la sémanique corrige les scores de similarité semantique et conserve l'intégrité des objets sous différentes conditions, ce qui améliore les performances du MLLM sur les tâches uni-objet et multi-objet. Le modèle final intègre tous les modules et atteint une précision supérieure de 5,7% au RAP (Récupération Multi-résolutionnelle Approximative), démontrant ainsi l'efficacité de la conception MRD dans l'amélioration de l'entente des images à haute résolution pour les MLLMs.\n",
      "\n",
      "Sources utilisées (5 chunks) :\n",
      "    - MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > 5.4. Ablation Study\n",
      "    - MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > Introduction\n",
      "    - MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > 5. Experiments\n",
      "    - MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > 3. Preliminary\n",
      "Temps total : 246.48s\n",
      "\n",
      "[RAG HYBRID]\n",
      "RAG Hybride sur : 'Quelle est la différence avec les méthodes existantes ?'\n",
      "\n",
      "[RERANKING]\n",
      "RAG Reranking sur : 'Quelle est la différence avec les méthodes existantes ?'\n",
      "\n",
      "============================================================\n",
      "QUESTION 4/5 : Quels datasets sont utilisés pour l'évaluation ?\n",
      "============================================================\n",
      "\n",
      "[BASELINE]\n",
      "Baseline (Sans contexte) : 'Quels datasets sont utilisés pour l'évaluation ?'\n",
      "\n",
      " Réponse Baseline (Mistral) :\n",
      "Les datasets utilisés pour l'évaluation varient en fonction du contexte. Dans le domaine des modèles de langage naturel, les datasets communs comprennent GLUE, SuperGLUE, Stanford NLP, Ubuntu Dialog Corpus (UDC) et Common Crawled Corpus (CCC). Cependant, pour une évaluation spécifique, il est recommandé de vérifier le guide de référence ou de consulter la documentation du projet.\n",
      "Temps total : 30.62s\n",
      "\n",
      "[RAG]\n",
      "RAG sur : 'Quels datasets sont utilisés pour l'évaluation ?'\n",
      "Model : mistral\n",
      "\n",
      "Les datasets utilisés pour l'évaluation sont ceux du V* dataset, comme mentionné dans la partie 5.4 de l'article \"Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding\".\n",
      "\n",
      "Sources utilisées (5 chunks) :\n",
      "    - MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > 4.2. Open-vocabulary Detector Enhancement\n",
      "    - MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > Introduction\n",
      "    - MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > 1. Introduction\n",
      "    - MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > 3. Preliminary\n",
      "    - MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > 5.4. Ablation Study\n",
      "Temps total : 126.30s\n",
      "\n",
      "[RAG HYBRID]\n",
      "RAG Hybride sur : 'Quels datasets sont utilisés pour l'évaluation ?'\n",
      "\n",
      "[RERANKING]\n",
      "RAG Reranking sur : 'Quels datasets sont utilisés pour l'évaluation ?'\n",
      "\n",
      "============================================================\n",
      "QUESTION 5/5 : Quelles sont les limitations de l'approche ?\n",
      "============================================================\n",
      "\n",
      "[BASELINE]\n",
      "Baseline (Sans contexte) : 'Quelles sont les limitations de l'approche ?'\n",
      "\n",
      " Réponse Baseline (Mistral) :\n",
      "L'approche peut être limitée par des facteurs tels que :\n",
      "1. Simplicité : Les modèles peuvent présenter une simplicité excessive, ignorant certaines complexités réelles du monde.\n",
      "2. Validité : La validité des résultats dépend de l'exactitude avec laquelle le monde est représenté et des paramètres utilisés.\n",
      "3. Transfert : Les modèles peuvent ne pas représenter les relations entre les variables correctement, ce qui peut entraîner un transfert invalide vers des situations réelles différentes.\n",
      "4. Interprétation : Les résultats d'un modèle peuvent être difficiles à interpréter et peuvent nécessiter des compétences techniques avancées pour être utilisés efficacement.\n",
      "5. Coût : Le coût de la conception, de l'exécution et de l'entretien d'un modèle peut être élevé, particulièrement pour les modèles plus complexes.\n",
      "Temps total : 58.78s\n",
      "\n",
      "[RAG]\n",
      "RAG sur : 'Quelles sont les limitations de l'approche ?'\n",
      "Model : mistral\n",
      "\n",
      "En utilisant le contexte fourni, l'article présente l'approche MRD pour améliorer la compréhension et la perception des images à haute résolution par les MLLMs (Modèles de Langage de Haut Niveau).\n",
      "\n",
      "Selon l'ablation étudiée, il est mentionné que l'introduction du OVD (One-Vote Detection) aide à localiser plus précisément des objets individuels, mais peut entraîner la perte d'objets dans des scénarios multi-objet. De plus, lors de la fusion des scores de similarité semantique multiresolution, il est possible que les parties différentes d'un objet ne soient pas complètement intégrées, ce qui peut entraîner une perte de précision dans la compréhension de l'image à haute résolution.\n",
      "\n",
      "Ainsi, les limitations potentielles de l'approche MRD pourraient être :\n",
      "\n",
      "1. La présence d'objets perdus lors de situations multi-objet avec l'introduction du One-Vote Detection (OVD).\n",
      "2. Une perte de précision dans la compréhension de l'image à haute résolution due à une fusion incomplète des parties différentes d'un objet.\n",
      "\n",
      "Sources utilisées (5 chunks) :\n",
      "    - MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > 5.4. Ablation Study\n",
      "    - MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > 4. Method\n",
      "    - MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > 3. Preliminary\n",
      "    - MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding > 5.1. Main Results\n",
      "Temps total : 168.09s\n",
      "\n",
      "[RAG HYBRID]\n",
      "RAG Hybride sur : 'Quelles sont les limitations de l'approche ?'\n",
      "\n",
      "[RERANKING]\n",
      "RAG Reranking sur : 'Quelles sont les limitations de l'approche ?'\n",
      "\n",
      "Resultats sauvegardes dans data/test/processed/demo_results.csv\n",
      "Resultats sauvegardes dans data/test/processed/demo_results.json\n",
      "Resultats sauvegardes dans data/test/processed/demo_results.xlsx\n",
      "\n",
      "----------\n",
      "RESUME DES PERFORMANCES\n",
      "----------\n",
      "\n",
      "Temps moyen :\n",
      "  Baseline  : 44.17s\n",
      "  RAG       : 183.30s\n",
      "  Reranking : 169.10s\n",
      "\n",
      "Longueur moyenne des reponses :\n",
      "  Baseline  : 613 caracteres\n",
      "  RAG       : 684 caracteres\n",
      "  Reranking : 1105 caracteres\n",
      "\n",
      "Sources moyennes :\n",
      "  RAG       : 5.0 sources\n",
      "  Reranking : 3.0 sources\n"
     ]
    }
   ],
   "source": [
    "df_final = demonstration_complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69e75938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>baseline_response</th>\n",
       "      <th>baseline_time</th>\n",
       "      <th>baseline_length</th>\n",
       "      <th>rag_response</th>\n",
       "      <th>rag_time</th>\n",
       "      <th>rag_length</th>\n",
       "      <th>rag_sources_count</th>\n",
       "      <th>rag_sources</th>\n",
       "      <th>rag_hybrid_response</th>\n",
       "      <th>rag_hybrid_time</th>\n",
       "      <th>rag_hybrid_length</th>\n",
       "      <th>rag_hybrid_sources_count</th>\n",
       "      <th>rag_hybrid_sources</th>\n",
       "      <th>rerank_response</th>\n",
       "      <th>rerank_time</th>\n",
       "      <th>rerank_length</th>\n",
       "      <th>rerank_sources_count</th>\n",
       "      <th>rerank_sources</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Qu'est-ce que la méthode MRD propose ?</td>\n",
       "      <td>La méthode MRD (Mean Residual Disease) est un ...</td>\n",
       "      <td>28.08</td>\n",
       "      <td>419</td>\n",
       "      <td>La méthode MRD (Multi-resolution Retrieval-Det...</td>\n",
       "      <td>200.27</td>\n",
       "      <td>752</td>\n",
       "      <td>5</td>\n",
       "      <td>MRD: Multi-resolution Retrieval-Detection Fusi...</td>\n",
       "      <td>La méthode MRD (Multi-resolution Retrieval Det...</td>\n",
       "      <td>147.42</td>\n",
       "      <td>789</td>\n",
       "      <td>5</td>\n",
       "      <td>MRD: Multi-resolution Retrieval-Detection Fusi...</td>\n",
       "      <td>Le MRD (Multiscale Resolution Design) est une ...</td>\n",
       "      <td>159.91</td>\n",
       "      <td>996</td>\n",
       "      <td>3</td>\n",
       "      <td>MRD: Multi-resolution Retrieval-Detection Fusi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Quels sont les résultats expérimentaux princip...</td>\n",
       "      <td>Les résultats expérimentaux principaux dépende...</td>\n",
       "      <td>39.70</td>\n",
       "      <td>568</td>\n",
       "      <td>Les résultats expérimentaux principaux du trav...</td>\n",
       "      <td>175.34</td>\n",
       "      <td>527</td>\n",
       "      <td>5</td>\n",
       "      <td>MRD: Multi-resolution Retrieval-Detection Fusi...</td>\n",
       "      <td>Les résultats expérimentaux principaux indique...</td>\n",
       "      <td>258.45</td>\n",
       "      <td>1527</td>\n",
       "      <td>5</td>\n",
       "      <td>MRD: Multi-resolution Retrieval-Detection Fusi...</td>\n",
       "      <td>Les résultats expérimentaux principaux indique...</td>\n",
       "      <td>130.08</td>\n",
       "      <td>713</td>\n",
       "      <td>3</td>\n",
       "      <td>MRD: Multi-resolution Retrieval-Detection Fusi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Quelle est la différence avec les méthodes exi...</td>\n",
       "      <td>Il est difficile de répondre de manière concis...</td>\n",
       "      <td>63.69</td>\n",
       "      <td>880</td>\n",
       "      <td>La méthode MRD (Multi-resolution Retrieval-Det...</td>\n",
       "      <td>246.48</td>\n",
       "      <td>930</td>\n",
       "      <td>5</td>\n",
       "      <td>MRD: Multi-resolution Retrieval-Detection Fusi...</td>\n",
       "      <td>La methode MRD proposee dans ce travail est un...</td>\n",
       "      <td>156.11</td>\n",
       "      <td>670</td>\n",
       "      <td>5</td>\n",
       "      <td>MRD: Multi-resolution Retrieval-Detection Fusi...</td>\n",
       "      <td>La table montre des résultats de différents mo...</td>\n",
       "      <td>310.82</td>\n",
       "      <td>1911</td>\n",
       "      <td>3</td>\n",
       "      <td>MRD: Multi-resolution Retrieval-Detection Fusi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Quels datasets sont utilisés pour l'évaluation ?</td>\n",
       "      <td>Les datasets utilisés pour l'évaluation varien...</td>\n",
       "      <td>30.62</td>\n",
       "      <td>382</td>\n",
       "      <td>Les datasets utilisés pour l'évaluation sont c...</td>\n",
       "      <td>126.30</td>\n",
       "      <td>199</td>\n",
       "      <td>5</td>\n",
       "      <td>MRD: Multi-resolution Retrieval-Detection Fusi...</td>\n",
       "      <td>Les datasets utilisés pour l'évaluation sont V...</td>\n",
       "      <td>140.58</td>\n",
       "      <td>127</td>\n",
       "      <td>5</td>\n",
       "      <td>MRD: Multi-resolution Retrieval-Detection Fusi...</td>\n",
       "      <td>Les données utilisées pour l'évaluation n'appa...</td>\n",
       "      <td>86.32</td>\n",
       "      <td>392</td>\n",
       "      <td>3</td>\n",
       "      <td>MRD: Multi-resolution Retrieval-Detection Fusi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Quelles sont les limitations de l'approche ?</td>\n",
       "      <td>L'approche peut être limitée par des facteurs ...</td>\n",
       "      <td>58.78</td>\n",
       "      <td>816</td>\n",
       "      <td>En utilisant le contexte fourni, l'article pré...</td>\n",
       "      <td>168.09</td>\n",
       "      <td>1013</td>\n",
       "      <td>5</td>\n",
       "      <td>MRD: Multi-resolution Retrieval-Detection Fusi...</td>\n",
       "      <td>L'approche proposée, Multi-resolution Retrieva...</td>\n",
       "      <td>273.26</td>\n",
       "      <td>1954</td>\n",
       "      <td>5</td>\n",
       "      <td>MRD: Multi-resolution Retrieval-Detection Fusi...</td>\n",
       "      <td>Les limiteurs de l'approche sont les suivants ...</td>\n",
       "      <td>158.35</td>\n",
       "      <td>1515</td>\n",
       "      <td>3</td>\n",
       "      <td>MRD: Multi-resolution Retrieval-Detection Fusi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           question  \\\n",
       "0   1             Qu'est-ce que la méthode MRD propose ?   \n",
       "1   2  Quels sont les résultats expérimentaux princip...   \n",
       "2   3  Quelle est la différence avec les méthodes exi...   \n",
       "3   4   Quels datasets sont utilisés pour l'évaluation ?   \n",
       "4   5       Quelles sont les limitations de l'approche ?   \n",
       "\n",
       "                                   baseline_response  baseline_time  \\\n",
       "0  La méthode MRD (Mean Residual Disease) est un ...          28.08   \n",
       "1  Les résultats expérimentaux principaux dépende...          39.70   \n",
       "2  Il est difficile de répondre de manière concis...          63.69   \n",
       "3  Les datasets utilisés pour l'évaluation varien...          30.62   \n",
       "4  L'approche peut être limitée par des facteurs ...          58.78   \n",
       "\n",
       "   baseline_length                                       rag_response  \\\n",
       "0              419  La méthode MRD (Multi-resolution Retrieval-Det...   \n",
       "1              568  Les résultats expérimentaux principaux du trav...   \n",
       "2              880  La méthode MRD (Multi-resolution Retrieval-Det...   \n",
       "3              382  Les datasets utilisés pour l'évaluation sont c...   \n",
       "4              816  En utilisant le contexte fourni, l'article pré...   \n",
       "\n",
       "   rag_time  rag_length  rag_sources_count  \\\n",
       "0    200.27         752                  5   \n",
       "1    175.34         527                  5   \n",
       "2    246.48         930                  5   \n",
       "3    126.30         199                  5   \n",
       "4    168.09        1013                  5   \n",
       "\n",
       "                                         rag_sources  \\\n",
       "0  MRD: Multi-resolution Retrieval-Detection Fusi...   \n",
       "1  MRD: Multi-resolution Retrieval-Detection Fusi...   \n",
       "2  MRD: Multi-resolution Retrieval-Detection Fusi...   \n",
       "3  MRD: Multi-resolution Retrieval-Detection Fusi...   \n",
       "4  MRD: Multi-resolution Retrieval-Detection Fusi...   \n",
       "\n",
       "                                 rag_hybrid_response  rag_hybrid_time  \\\n",
       "0  La méthode MRD (Multi-resolution Retrieval Det...           147.42   \n",
       "1  Les résultats expérimentaux principaux indique...           258.45   \n",
       "2  La methode MRD proposee dans ce travail est un...           156.11   \n",
       "3  Les datasets utilisés pour l'évaluation sont V...           140.58   \n",
       "4  L'approche proposée, Multi-resolution Retrieva...           273.26   \n",
       "\n",
       "   rag_hybrid_length  rag_hybrid_sources_count  \\\n",
       "0                789                         5   \n",
       "1               1527                         5   \n",
       "2                670                         5   \n",
       "3                127                         5   \n",
       "4               1954                         5   \n",
       "\n",
       "                                  rag_hybrid_sources  \\\n",
       "0  MRD: Multi-resolution Retrieval-Detection Fusi...   \n",
       "1  MRD: Multi-resolution Retrieval-Detection Fusi...   \n",
       "2  MRD: Multi-resolution Retrieval-Detection Fusi...   \n",
       "3  MRD: Multi-resolution Retrieval-Detection Fusi...   \n",
       "4  MRD: Multi-resolution Retrieval-Detection Fusi...   \n",
       "\n",
       "                                     rerank_response  rerank_time  \\\n",
       "0  Le MRD (Multiscale Resolution Design) est une ...       159.91   \n",
       "1  Les résultats expérimentaux principaux indique...       130.08   \n",
       "2  La table montre des résultats de différents mo...       310.82   \n",
       "3  Les données utilisées pour l'évaluation n'appa...        86.32   \n",
       "4  Les limiteurs de l'approche sont les suivants ...       158.35   \n",
       "\n",
       "   rerank_length  rerank_sources_count  \\\n",
       "0            996                     3   \n",
       "1            713                     3   \n",
       "2           1911                     3   \n",
       "3            392                     3   \n",
       "4           1515                     3   \n",
       "\n",
       "                                      rerank_sources  \n",
       "0  MRD: Multi-resolution Retrieval-Detection Fusi...  \n",
       "1  MRD: Multi-resolution Retrieval-Detection Fusi...  \n",
       "2  MRD: Multi-resolution Retrieval-Detection Fusi...  \n",
       "3  MRD: Multi-resolution Retrieval-Detection Fusi...  \n",
       "4  MRD: Multi-resolution Retrieval-Detection Fusi...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
