[
  {
    "page_content": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image\nUnderstanding\nFan Yang\nHarbin Institute of Technology (Shenzhen) 25b951055@stu.hit.edu.cn\nKaihao Zhang\nHarbin Institute of Technology (Shenzhen) super.khzhang@gmail.com\nAbstract Understanding high-resolution images remains a significant challenge for multimodal large language models (MLLMs).\nRecent study address this issue by dividing the image into smaller crops and computing the semantic similarity between each crop and a query using a pretrained retrieval-augmented generation (RAG) model. The most relevant crops are then selected to localize the target object and suppress irrelevant information. However, such crop-based processing can fragment complete objects across multiple crops, thereby disrupting the computation of semantic similarity. In our experiments, we find that image crops of objects with different sizes are better handled at different resolutions. Based on this observation, we propose Multi-resolution Retrieval-Detection (MRD), a trainingfree framework for high-resolution image understanding.\nTo address the issue of semantic similarity bias caused by objects being split across different image crops, we propose a multi-resolution semantic fusion method, which integrates semantic similarity maps obtained at different resolutions to produce more accurate semantic information and preserve the integrity of target objects. Furthermore, to achieve direct localization of target objects at a global scale, we introduce an open-vocalbulary object detection (OVD) model that identifies object regions using a sliding-window approach. Experiments on high-resolution image understanding benchmarks using different MLLMs demonstrate the effectiveness of our approach.\n1. Introduction\nMultimodal Large Language Models (MLLMs) have demonstrated significant advancements in integrating and interpreting visual and linguistic information, enabling robust capabilities in vision-language understanding, reasoning, and interactive tasks [25]. By leveraging visual signals, these models can process and decipher complex visual information, forming a bridge between pixel-level data and Figure 1. Overview of the proposed Multi-resolution RetrievalDetection framework, which uses RAG and OVD to obtain semantic similarity map and detection confidence map respectively. By integrating the two, the target objects can be localized more accurately. semantic interpretation [17, 20]. However, a common practice among most MLLMs is to process input images at fixed and pre-defined resolutions [1, 13, 14]. While this uniform input pipeline simplifies model architecture and reduces computational overhead, it introduces substantial limitations. Specifically, resizing high-resolution (HR) real-world images to a fixed low resolution often leads to shape distortion and blurring, which degrades the quality of fine-grained visual details. Recent studies [11, 19, 22, 24, 29, 30] indicate that existing methods remain unsatisfactory for highresolution image tasks. This is clearly demonstrated by their suboptimal results on dedicated high-resolution image understanding benchmarks [21, 24].\nTo address this limitation, improving the high-resolution image perception capability of MLLMs has become an emerging research focus. A common ”locate-and-zoomin” strategy is widely adopted to enhance detail perception in models. Although training-based approaches such as Supervised Fine-Tuning (SFT) [18] and Reinforcement",
    "metadata": {
      "producer": "pikepdf 8.15.1",
      "creator": "arXiv GenPDF (tex2pdf:4177c2c)",
      "creationdate": "",
      "source": "data/raw/2512.02906v2.pdf",
      "file_path": "data/raw/2512.02906v2.pdf",
      "total_pages": 16,
      "format": "PDF 1.7",
      "title": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding",
      "author": "Fan Yang; Kaihao Zhang",
      "subject": "",
      "keywords": "",
      "moddate": "",
      "trapped": "",
      "modDate": "",
      "creationDate": "",
      "page": 0
    }
  },
  {
    "page_content": "Learning (RL) [30] can effectively identify relevant regions, they are hampered by critical limitations, including high computational costs, long training cycles, and poor cross-architecture transferability, which curtails their scalability and practical application. In contrast, training-free methods [11, 19, 29] automatically locate regions by using attention mechanisms or tree-based search, without requiring the construction of the dataset or the fine-tuning of the model. Despite employing a top-down search strategy from high to low resolution, these methods face significant limitations. A primary issue is the model’s inadequate perception of small objects during the initial search stage [19, 21], which frequently leads to the generation of erroneous search paths.\nRecently,\nInspired by the success of\nRetrievalAugmented Generation (RAG) for enabling long-context understanding in general LLMs [9], Wang et al. [22] introduced Retrieval-Augmented Perception (RAP) — a training-free framework designed to enhance MLLMs’ perception of high-resolution images.\nRAP extends this paradigm to the visual domain, achieving significant performance improvement on high-resolution benchmarks. The\nRAP framework consists of three key components: First, the Visual Retrieval module employs a pre-trained vision RAG model, VisRAG, to compute semantic similarity between the query and different image regions (crops), retrieving the most relevant ones to reduce noise.\nNext, the Spatial-Awareness Layout module preserves the original relative spatial relationships among the retrieved crops when composing them into the model input, maintaining spatial coherence. Finally, the Adaptive RetrievalExploration Search (RE-Search) module dynamically determines the optimal number of retrieved crops by constructing a Retrieval-Exploration Tree (RE-Tree), balancing information sufficiency with computational efficiency. Despite its promise, the RAP method suffers from several inherent limitations. First, the patching operation can fragment large objects across multiple disjointed crops, disrupting their holistic semantics and leading to biased similarity calculations. Our empirical observations confirm that some patches semantically irrelevant to the query can obtain abnormally high similarity scores. Second, the patch resolution is a critical yet difficult-to-tune hyperparameter: overly large patches introduce redundant background information, while overly small ones exacerbate object fragmentation. Experiments show that the choice of resolution significantly impacts performance. Third, in high-resolution images with cluttered backgrounds, the similarity measure is prone to false positives, where background regions may attain higher similarity than those containing the actual target objects, severely hampering recognition. To tackle these challenges, we propose a novel Multiresolution Retrieval-Detection (MRD) framework, based on RAP to improve retrieval quality and localization accuracy through two key techniques:\n• Multi-resolution Semantic Fusion: To mitigate the bias inherent in single-resolution patching, we design a simple yet effective fusion strategy. It computes semantic similarities across multiple proportional resolutions and performs consistency-based fusion to calibrate the results, yielding a more robust and accurate relevance estimation that alleviates semantic deviations caused by object fragmentation.\n• Open-vocabulary Detector Enhancement: For more precise target localization, we incorporate an advanced open-vocabulary object detector, LLMDet [7].\nFirst, we leverage the in-context learning capability of LLMs to extract target concepts from the query, defining the categories for the detector. Subsequently, a sliding window mechanism is employed to traverse the entire highresolution image, detecting target objects within each window to generate a confidence map indicating target presence. Finally, the calibrated multi-resolution semantic similarity is augmented by the object detection confidence. This synergistic fusion effectively amplifies the response in true target regions, enabling faster and more accurate localization of critical areas during subsequent retrieval, thereby guiding the MLLM toward more reliable inference. We conduct extensive experiments on several highresolution benchmarks, including V* [24], HRBench-4K, and HRBench-8K [21], utilizing various MLLMs such as LLaVA-ov and LLaVA-v1.5. The results demonstrate that our MRD framework surpasses all existing trainingfree methods and achieves state-of-the-art performance on both single-object and multi-object retrieval and recognition tasks, with particularly notable gains on single-object tasks. Our contributions are summarized as follows:\n• To the best of our knowledge, this is the first work that systematically leverages an open-vocabulary object detector to enhance MLLMs’ understanding of highresolution images. Experiments validate that the detector provides precise target localization, effectively suppressing interference from irrelevant regions.\n• We propose MRD, a training-free and generic framework. It innovatively corrects semantic similarity via a multi-resolution fusion strategy and integrates open-set detection results to enhance target regions, creating a synergistic effect.\n• Comprehensive experiments validate the effectiveness and generalization of our method. It achieves leading performance on both single-object and multi-object tasks across different MLLMs and high-resolution benchmarks",
    "metadata": {
      "producer": "pikepdf 8.15.1",
      "creator": "arXiv GenPDF (tex2pdf:4177c2c)",
      "creationdate": "",
      "source": "data/raw/2512.02906v2.pdf",
      "file_path": "data/raw/2512.02906v2.pdf",
      "total_pages": 16,
      "format": "PDF 1.7",
      "title": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding",
      "author": "Fan Yang; Kaihao Zhang",
      "subject": "",
      "keywords": "",
      "moddate": "",
      "trapped": "",
      "modDate": "",
      "creationDate": "",
      "page": 1
    }
  },
  {
    "page_content": "2. Related Work\n2.1. Multimodal Large Language Models\nMLLMs have rapidly advanced as powerful foundation models capable of understanding and generating multimodal content across diverse vision–language tasks [25, 28]. Early MLLM architectures generally adopt fixedresolution vision encoders—such as224×224 or 448×448\nViTs [2, 12–14].\nWhile this design simplifies training and computation, it inevitably requires resizing or cropping high-resolution (HR) images, thereby discarding finegrained visual details crucial for tasks such as fine-grained recognition, dense reasoning, or detecting small objects. To enhance high-resolution image understanding without proportionally increasing the computational burden from visual tokens, several studies have integrated highresolution visual encoders into MLLMs. For example, Vary [23] and Deepseek-VL [15] incorporate the SAM encoder [10] to improve model performance on HR images.\nAlternatively, another line of work introduced\nNative/Dynamic-Resolution\nMLLMs that processes images at their native resolution. The core idea is to generate a variable-length sequence of visual tokens that adapts to the original dimensions of the input image, thereby preserving spatial fidelity and high-frequency details.\nThese models employ various mechanisms to handle the resulting long sequences and computational complexity, including sliding window attention, dynamic masking, and patch-based encoding strategies. Representative works in this category have demonstrated significant progress. For instance, the InternVL series [4, 6, 31] adopts a strategy of splitting a high-resolution image into multiple fixed-size patches. In contrast, models like Qwen2.5-VL [2, 3] take an end-to-end approach by training a ViT directly on native-resolution images. This allows the model to embed the entire image into a single, coherent token sequence in one forward pass, potentially leading to better global context understanding.\n2.2. High-Resolution Image Understanding\nMultimodal large language models (MLLMs) have made substantial progress in recent years; however, they continue to face challenges in accurately recognizing and interpreting fine-grained details within high-resolution (HR) images [21? ].\nTo enhance the capability of MLLMs in high-resolution image understanding, existing studies generally follow two main directions. Training-based approaches rely on supervised fine-tuning (SFT) [18, 24] or reinforcement learning (RL) [30], but such methods often compromise the model’s generalization ability on broad vision–language tasks. In contrast, training-free approaches [11, 19, 21, 22]typically perform hierarchical or tree-based search to localize target regions. However, these methods tend to suffer from low efficiency and may fail to retain all target objects during the search process, particularly in multi-object scenarios.\n3. Preliminary In this section, We first conduct an analysis of the relationship between the resolution of image crops and the performance of MLLMs in subsection 3.2. The experimental results indicate that using different resolution has a significant impact on MLLMs to analyze HR images. Objects of different sizes are suitable for different resolutions. Inspired by this we propose the MRD framework.\n3.1. Semantic Similarity This section presents the pipeline for integrating RetrievalAugmented Generation (RAG) into Multimodal Large Language Models (MLLMs) to calculate the semantic similarity scores between the query embedding and images crops from HR images.\nGiven an HR image, we first partition it collection of image patches, denoted as P = {p1, p2, . . . , pn}, where n is the total number of image crops. Following the approach of Yu et al. [27], the textual query and each image crop are encoded independently using the text and image encoders of a Vision-Language Model (VLM), producing a sequence of hidden representations. The semantic similarity score between the query embedding and each image crop embedding is then computed. Specifically, the similarity score s(q, pi) for the i-th crop is calculated by the cosine similarity of the query and image crop embeddings: s(q, pi) = 1\n2 · (1 + q · pi ∥q∥· ∥pi∥) (1)\nBased on these scores, the top K most relevant image crops are selected and provided to the MLLM to support detailed understanding of the high-resolution input.\n3.2. Impact of the Resolution of Image Crops\nIn this section, we conduct an analysis to investigates the relationship between the resolution of image crops and the performance of MLLMs in HR image understanding. Experimental setting. We analyze the relation between performance and the resolution of image crops, using LLaVA-ov and LLaVA-v1.5 on V * benmark. Observations. We visualize the relationship between the resolution of image crops and performance of MLLMs. As shown in Figure 3, from the overall accuracy obtained by using different resolutions, when the resolution of image crops is set to 112, using different MLLM achieved the highest accuracy rates in both the single-object task and the multi-object task. This indicates that setting the resolution to 112 might be the optimal choice. However, when we take",
    "metadata": {
      "producer": "pikepdf 8.15.1",
      "creator": "arXiv GenPDF (tex2pdf:4177c2c)",
      "creationdate": "",
      "source": "data/raw/2512.02906v2.pdf",
      "file_path": "data/raw/2512.02906v2.pdf",
      "total_pages": 16,
      "format": "PDF 1.7",
      "title": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding",
      "author": "Fan Yang; Kaihao Zhang",
      "subject": "",
      "keywords": "",
      "moddate": "",
      "trapped": "",
      "modDate": "",
      "creationDate": "",
      "page": 2
    }
  },
  {
    "page_content": "Figure 2. Setting the resolution of image crops to 112 causes complete objects to be split across different regions, which disrupts the semantic information of the target objects. Figure 3. The effect of the resolution of retrieved image crops on model performance. Attribute and Spatial represent the attribute recognition and spatial reasoning in V ∗Bench. a closer look at the results of each sample, we find that in some cases, choosing a different resolution actually leads to more accurate results compared to when the resolution is 112, as shown in Figure 3. The results of the image crops selected based on different resolutions and the visualization of the semantic similarity map can provide a very intuitive analysis of the reasons: Since the complete object is divided into different crops, some parts of it have a higher semantic similarity calculated by VisRAG, while other parts have a lower semantic similarity. After screening, only the parts with higher similarity are retained. However, this will damage the integrity of the target object and cause interference to the judgment of MLLM.\n4. Method\nIn this section, we propose a novel framework named Multi-resolution Retrieval Detection (MRD). The core design of MRD lies in its multi-resolution approach at different scales to better localize regions containing target objects. This enables subsequent search processes to more easily identify image crops corresponding to the target objects, eliminating irrelevant distractions and enhancing the perceptual understanding of HR images by MLLMs. Based on the findings in subsection 3.2, we argue that using different resolutions for semantic similarity computation is more suitable for objects of varying sizes and locations. Inspired by this idea, we first introduce a simple yet effective\nMulti-resolution Semantic Fusion method, which computes semantic similarity maps at different resolutions on a local scale and performs consistency-based fusion to refine the semantic similarity and improve its accuracy. To more directly localize target objects, we incorporate an Openvocabulary object detection model that traverses the entire HR image globally using a sliding window approach, generating confidence scores for regions containing target objects.\nFinally, by integrating the detection confidence scores with the multi-resolution semantic similarity maps, our method not only improves localization of target regions but also distinguishes fine-grained differences among crops in these regions, thereby assisting subsequent search processes in more accurately identifying key areas. The following sections will provide detailed explanations of each component.\n4.1. Multi-resolution Semantic Fusion In subsection 3.2, we observe that image crops of different resolutions are suitable for objects of varying sizes and locations in different cases. Compared to the semantic similarity map obtained using a single resolution, those derived from multiple resolutions exhibit respective advantages. Therefore, we first propose a Multi-Resolution Semantic Fusion method. As shown in the top part of Figure 4, we partition the HR image using proportional resolutions, with the low resolution set to l and the high resolution set to ˆl, where",
    "metadata": {
      "producer": "pikepdf 8.15.1",
      "creator": "arXiv GenPDF (tex2pdf:4177c2c)",
      "creationdate": "",
      "source": "data/raw/2512.02906v2.pdf",
      "file_path": "data/raw/2512.02906v2.pdf",
      "total_pages": 16,
      "format": "PDF 1.7",
      "title": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding",
      "author": "Fan Yang; Kaihao Zhang",
      "subject": "",
      "keywords": "",
      "moddate": "",
      "trapped": "",
      "modDate": "",
      "creationDate": "",
      "page": 3
    }
  },
  {
    "page_content": "Figure 4. Detailed information of our propsoed MRD. First, We use VisRAG with different resolution of image crops to obtain multiresolution semantic similarity map. We then employ an open-set object detection model, LLMDet, to localize the target objects extracted from the query within the high-resolution image using a sliding-window approach, yielding a global detection confidence map. Finally, the obtained multi-resolution semantic similarity map is linearly fused with the detection confidence map, and the fused scores are used to guide the subsequent search to select image crops containing the target objects. ˆl = k · l. The set of image patches at high resolution is denoted as ˆP = {ˆp1, ˆp2, . . . , ˆpm}, and at low resolution as P = {p1, p2, . . . , pn}. Due to the proportional relationship between high and low resolutions, we have n = k2 · m, and each high-resolution patch ˆpi corresponds to k2 lowresolution patches: ˆpi =   ̃pi,1 ̃pi,2 · · · ̃pi,k ̃pi,(k+1) ̃pi,(k+2) · · · ̃pi,(2k) ... ... ... ... ̃pi,(k(k−1)+1) ̃pi,(k(k−1)+2) · · · ̃pi,k2   (2) where ̃pij ∈P, i ∈{1, 2, . . . , m}, j ∈{1, 2, . . . , k2}. Then, using Equation 1, we compute the cosine similarity between the query and image crop embeddings to obtain the semantic similarity scores for high and low resolutions, respectively: ˆS = {ˆs1, ˆs2, . . . , ˆsm} and S = {s1, s2, . . . , sn}: ˆsi = s(f(q), g(ˆpi)), sj = s(f(q), g(pj)) (3) where f(·) and g(·) denote the embedding operations for the query and image crop, respectively, with i ∈ {1, 2, . . . , m} and j ∈{1, 2, . . . , n}. According to the mapping from ˆpi to pj in (2), we can map the semantic similarity ˆs obtained from each high-resolution ˆp and the query to the corresponding k2 low-resolution p positions. The mapping operation can be expressed as: ̃S = H( ˆS) (4) where ̃S = { ̃s1, ̃s2, . . . , ̃sn}. After obtaining ̃S and S, we perform consistency fusion on the semantic similarities at corresponding positions to obtain the multi-resolution semantic similarity Sf = {sf\n1, sf 2, . . . , sf n}, which can be expressed as: sf t = p ̃st · st, t ∈1, 2, . . . , n (5)\nFinally, we can transform the semantic similarity scores into a two-dimensional semantic similarity map sf(i, j) with i ∈ {1, 2, . . . , H} and j ∈{1, 2, . . . , W}. The total number of low resolution image crop n = H × W .\nFusing the multi-resolution semantic similarity scores from high and low resolutions enables correction of the lowresolution similarities when a complete object is split across different patches in the low-resolution view. This enhances the similarity of various parts of the object, thereby preserving the integrity of the object as much as possible during subsequent search processes and improving the recognition accuracy of the MLLM.\n4.2. Open-vocabulary Detector Enhancement\nVisRAG divides the HR image into patches and computes semantic similarity between the query and each image crop,",
    "metadata": {
      "producer": "pikepdf 8.15.1",
      "creator": "arXiv GenPDF (tex2pdf:4177c2c)",
      "creationdate": "",
      "source": "data/raw/2512.02906v2.pdf",
      "file_path": "data/raw/2512.02906v2.pdf",
      "total_pages": 16,
      "format": "PDF 1.7",
      "title": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding",
      "author": "Fan Yang; Kaihao Zhang",
      "subject": "",
      "keywords": "",
      "moddate": "",
      "trapped": "",
      "modDate": "",
      "creationDate": "",
      "page": 4
    }
  },
  {
    "page_content": "enabling localized object retrieval at a small scale. However, it struggles to accurately localize larger objects. To address this limitation and achieve more direct and largescale object localization, we introduce an advanced openvocabulary object detection model—LLMDet—to directly locate regions containing target objects. First, we employ in-context learning with a Large Language Model (LLM) to extract the primary target objects from the query, which serve as the target categories for LLMDet. Due to the extremely high resolution of HR images in datasets such as\nHR-Bench, we adopt a sliding window strategy for object localization. To align with the semantic similarity map derived from image crops, we assign the detection confidence scores of the target bounding boxes to their corresponding image patches, thereby generating a detection map that reflects the confidence of target object presence in each patch. This detection map offers a more intuitive localization representation compared to the semantic similarity map. In the following, we provide a detailed introduction to the proposed method. Object Extraction. To enable open-vocabulary object detection, we first leverages Large Language Models (LLMs) to dynamically identify target objects from textual queries.\nGiven an input query Q, we employ in-context learning to extract the primary object entities that serve as detection targets for our LLMDet framework.\nFormally, we define the object extraction process as:\nO = LLM(Psystem, Eexamples, Q) (6) where O represents the set of extracted objects, Psystem denotes the system prompt containing extraction guidelines, and Eexamples constitutes the demonstration examples. Sliding-window Object Detection. In order to get a global detection confidence map align with the previously semantic similarity map, we similarly partition the HR image into a grid of H × W non-overlapping patches where total number of patches n = H ×W. A sliding window of size h×w patches (where h < H and w < W) traverses the entire image with a predefined stride. In this way, We can obtain T sliding windows W = {W1, W2, ..., WT }. After obtaining multiple sliding windows using the sliding window method, we use LLMDet to detect objects within each sliding window. The detector generates a set of bounding boxes Bt = {b1, b2, . . . , bKt} and the corresponding confidence scores sk indicating the likelihood of containing a target object where t denotes the t-th sliding window. We apply a confidence threshold τ to filter out lowquality detections:\nBfilter t = {bk ∈Bt | sk > τ} (7) Subsequently, we generate a window detection confidence map cw t ∈Rh×w for the current window. The value at patch coordinate (p, q) within this local window is assigned the maximum confidence score among all bounding boxes in Bfiltered t that contain this patch. If no box covers the patch, the confidence is set to 0: cw t (p, q) = max bk∈Bfilter t {sk · I[(m, n) ∈bk]} (8) where I[·] is the indicator function that equals 1 if the patch (p, q) is inside the bounding box bk, and 0 otherwise.\nTo aggregate information from all sliding windows and form a global, unified detection confidence map cg ∈\nRH×W for the entire high-resolution image, we employ an averaging fusion strategy. For a global patch at coordinate (p, q), its final confidence score is computed as the average of all confidence scores assigned to it from every sliding window that contained it.\nFor the patch I(i, j) at position (i, j) in the HR image, if it is contained in the t-th sliding window, we denote its position in the t-th sliding window Wt as (ti, tj), which can be expressed as\nI(i, j) = Wt(ti, tj), t ∈Ti,j (9) where Tij denotes the set of sliding windows that contain I(i, j). Now, we can obtain the global detection confidence map of the whole HR image, which can be computed as: cg(i, j) = 1 |Ti,j| X t∈Ti,j cw t (ti, tj) (10) where i ∈{1, 2, . . . , H} and j ∈{1, 2, . . . , W}. The detection confidence map provides effective localization of target regions on a global scale, offering direct spatial guidance but lacking the ability to distinguish finegrained differences within the target object. To address this limitation, we integrate the detection confidence with multiresolution semantic similarity through linear combination, which can be expressed as: sF(i, j) = (1 −w) · sf(i, j) + w · cg(i, j) (11) This synergistic fusion enables precise target localization while effectively highlighting intra-object variations, thereby facilitating more accurate extraction of key regions in subsequent search processes. The details of the subsequent Retrieved-Exploration Search process can be found in paper [22].\n5. Experiments Evaluated benchmark.\nWe evaluate our MRD on two high-resolution benchmarks. The first is V ∗Bench [24], with an average resolution of 2246 × 1582, consists of two sub-tasks: attribute recognition and spatial reasoning. The Second is HRBench which includes two sub-task Finegrained Single-instance Perception (FSP) and Fine-grained Cross-instance Perception (FCP)",
    "metadata": {
      "producer": "pikepdf 8.15.1",
      "creator": "arXiv GenPDF (tex2pdf:4177c2c)",
      "creationdate": "",
      "source": "data/raw/2512.02906v2.pdf",
      "file_path": "data/raw/2512.02906v2.pdf",
      "total_pages": 16,
      "format": "PDF 1.7",
      "title": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding",
      "author": "Fan Yang; Kaihao Zhang",
      "subject": "",
      "keywords": "",
      "moddate": "",
      "trapped": "",
      "modDate": "",
      "creationDate": "",
      "page": 5
    }
  },
  {
    "page_content": "Table 1. Comparison of MRD with existing works on high-resolution benchmarks\nMethod\nV* Bench\nHR-Bench 4K\nHR-Bench 8K\nAttribute\nSpatial\nOverall\nFSP\nFCP\nOverall\nFSP\nFCP\nOverall\nOpen-source MLLMs LLaVA-v1.6-7B [14]\n60.9\n63.2\n61.8\n49.0\n46.8\n47.9\n37.3\n44.3\n40.8 LLaVA-v1.6-13B [14]\n60.0\n64.5\n61.8\n49.8\n41.3\n45.5\n38.0\n38.3\n38.1 LLaVA-v1.6-34B [14]\n-\n-\n-\n55.3\n50.5\n52.9\n44.5\n50.3\n47.4 LLaVA-HR-X-13B [16]\n-\n-\n-\n61.3\n46.0\n53.6\n49.5\n44.3\n46.9 LLaVA-HR-X-7B [16]\n51.3\n64.5\n56.5\n57.8\n46.3\n52.0\n42.0\n41.3\n41.6 InternVl-1.5-26B [5]\n-\n-\n-\n69.5\n51.8\n60.6\n69.3\n48.5\n57.9 Yi-VL-34B [26]\n-\n-\n-\n46.0\n42.8\n44.4\n39.5\n38.5\n39.0\nClosed-source MLLMs GPT-4o [8]\n-\n-\n66.0\n70.0\n48.0\n59.0\n62.0\n49.0\n55.5 Qwen-VL-max [2]\n-\n-\n-\n65.0\n52.0\n58.5\n54.0\n51.0\n52.5\nBaselines and MRD LLaVA-v1.5-7B [14]\n43.5\n56.6\n48.7\n38.5\n33.8\n36.1\n33.0\n31.3\n32.1 LLaVA-v1.5-7B-Zoom Eye [19]\n83.5\n82.9\n83.3\n67.8\n38.8\n53.3\n65.5\n36.0\n50.8 LLaVA-v1.5-7B-RAP [22]\n90.4\n96.1\n91.1\n73.8\n40.5\n57.1\n72.3\n35.3\n53.8 LLaVA-v1.5-7B-MRD (ours)\n97.4\n96.1\n95.6\n76.8\n42.7\n59.7\n72.6\n37.2\n54.9 LLaVA-ov-0.5B [14]\n63.5\n64.5\n63.9\n63.5\n39.5\n51.5\n47.3\n38.3\n42.8 LLaVA-ov-0.5B-Zoom Eye[19]\n85.2\n73.7\n80.6\n75.5\n39.8\n57.6\n68.5\n38.3\n53.4 LLaVA-ov-0.5B-RAP [22]\n80.0\n84.2\n83.6\n80.3\n42.3\n61.3\n81.8\n45.3\n63.5 LLaVA-ov-0.5B-MRD (ours)\n89.6\n82.9\n88.0\n84.0\n45.2\n64.6\n81.8\n47.3\n64.5 Figure 5. Visualization of the Effects of Different Modules in MRD. Upper: Visualization of the Effects of the Multi-resolution Semantic Fusion Method. Lower: Visualization of the Effects of the Multi-resolution Semantic Fusion Method\n5.1. Main Results\nAs shown in Table 1, compared with both the baseline MLLMs and previous baseline approaches, our proposed",
    "metadata": {
      "producer": "pikepdf 8.15.1",
      "creator": "arXiv GenPDF (tex2pdf:4177c2c)",
      "creationdate": "",
      "source": "data/raw/2512.02906v2.pdf",
      "file_path": "data/raw/2512.02906v2.pdf",
      "total_pages": 16,
      "format": "PDF 1.7",
      "title": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding",
      "author": "Fan Yang; Kaihao Zhang",
      "subject": "",
      "keywords": "",
      "moddate": "",
      "trapped": "",
      "modDate": "",
      "creationDate": "",
      "page": 6
    }
  },
  {
    "page_content": "MRD framework consistently delivers substantial performance gains across all sub-tasks, datasets, and model configurations. The improvement is most pronounced on the V ∗dataset using the LaVA-v1.5-7B model, where MRD achieves a remarkable 46.9% absolute increase in accuracy—nearly doubling the original performance. Significant gains are also observed on HR-Bench 4K and HRBench 8K, with maximum improvements of 23.6% and\n22.8%, respectively.\nIn comparison to the state-of-the-art baseline RAP, MRD achieves superior performance across all datasets and model settings, yielding an average improvement of 2.8%. When examining results across sub-task categories, MRD demonstrates particularly strong performance on single-object tasks. We attribute this advantage to the integration of a detection module, which provides more accurate localization for isolated objects. Overall, these results indicate that MRD markedly enhances the perception and understanding capabilities of MLLMs when operating on high-resolution images.\n5.2. Effect of the Multi-resolution Semantic Fusion Multi-resolution Semantic Fusion can obtain more accurate information by integrating semantic similarity maps from different resolutions. From the two cases shown in the upper part of Figure 5, we can clearly observe that incorporating multi-resolution semantic fusion allows the high-resolution semantic similarity map to correct the lowresolution map, alleviating semantic deviations caused by different parts of the target object being split across multiple patches. This helps better preserve the integrity of the target object. The results in the cases demonstrate that the approach is effective for both single-object and multi-object tasks. Overall, the experimental results indicate that Multiresolution Semantic Fusion provides better adaptability to objects of different sizes compared to using a single resolution.\n5.3. Effect of Open-vocabulary Object Detection To achieve more accurate and direct localization of the target object at a global scale, we introduce an open-set object detection model. As shown in lower part of Figure 5, sliding-window detection results effectively identify the target object’s location. By combining the detection results with semantic similarity scores, MRD amplifies the scores of patches that contain the target object while suppressing false-positive patches that also exhibit high semantic similarity. This integration facilitates a more efficient and accurate patch retrieval process in subsequent searching.\n5.4. Ablation Study To better understand the contributions of different modules in our MRD framework, we conduct ablation studies Table 2. Ablation study of different module in MRD.\nV* Bench ∆↑\nAttribute\nSpatial\nOverall RAP\n80.0\n84.2\n83.6\n- OVD\n84.3\n81.6\n84.9 +1.3 RAP+Multi-res\n82.9\n85.2\n85.8 +2.2 RAP+OVD\n85.2\n84.2\n86.2 +2.6 RAP+OVD+Multi-Res\n90.4\n85.5\n89.3 +5.7 on the V ∗dataset using the LLaVA-ov-0.5B model. As shown in Table 2, using the OVD model alone (second row) yields higher localization accuracy for single-object tasks, but its performance on multi-object tasks is inferior to RAP. When RAP employs multi-resolution semantic fusion (third row), performance improves on both single-object and multi-object tasks, indicating that multi-resolution semantic fusion can better handle objects of varying sizes across different scenarios.\nFusing the semantic similarity map obtained from RAP with the detection confidence map from OVD (fourth row) significantly improves performance on single-object tasks; however, the performance on multi-object tasks is even worse than using OVD alone, suggesting that some target objects may be lost during the search. By further incorporating multi-resolution semantic fusion, performance improves on both single-object and multi-object tasks, demonstrating the effectiveness of this fusion strategy. In summary, introducing OVD helps localize single objects more accurately but may result in missed objects in multi-object scenarios.\nMulti-resolution semantic fusion corrects semantic similarity scores and preserves object completeness under different conditions, enhancing MLLM performance on both single- and multi-object tasks. The final model, which integrates all modules, achieves a 5.7% higher accuracy than RAP, demonstrating the effectiveness of MRD’s design in improving high-resolution image understanding for MLLMs.\n6. Conclusion\nIn this work, we propose a novel training-free method,\nMulti-resolution Retrieval-Detection (MRD), to enhance the understanding of high-resolution images by MLLMs. MRD employs multi-resolution semantic similarity to correct single-resolution similarity maps, ensuring the integrity of target objects. Moreover, to localize target objects more accurately and directly, we introduce an OVD model that identifies object regions using a sliding-window approach.\nWe demonstrate the effectiveness of MRD across multiple high-resolution benchmarks with different MLLMs, showing its superior performance in HR image understanding",
    "metadata": {
      "producer": "pikepdf 8.15.1",
      "creator": "arXiv GenPDF (tex2pdf:4177c2c)",
      "creationdate": "",
      "source": "data/raw/2512.02906v2.pdf",
      "file_path": "data/raw/2512.02906v2.pdf",
      "total_pages": 16,
      "format": "PDF 1.7",
      "title": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding",
      "author": "Fan Yang; Kaihao Zhang",
      "subject": "",
      "keywords": "",
      "moddate": "",
      "trapped": "",
      "modDate": "",
      "creationDate": "",
      "page": 7
    }
  },
  {
    "page_content": "References [1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,\nXiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint , 2023. 1 [2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond, 2023. 3, 7 [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin\nGe, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint , 2025. 3 [4] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. arXiv preprint , 2024. 3 [5] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng\nLuo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang\nYan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin,\nChao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang,\nBo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min\nDou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. How far are we to gpt-4v? closing the gap to commercial multimodal models with opensource suites. Sci. China Inf. Sci., 67(12), 2024. 7 [6] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen,\nSen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 24185–24198, 2024. 3 [7] Shenghao Fu, Qize Yang, Qijie Mo, Junkai Yan, Xihan Wei, Jingke Meng, Xiaohua Xie, and Wei-Shi Zheng.\nLlmdet:\nLearning strong open-vocabulary object detectors under the supervision of large language models. In Proceedings of the\nComputer Vision and Pattern Recognition Conference, pages 14987–14997, 2025. 2 [8] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint , 2024. 7 [9] Bowen Jin, Jinsung Yoon, Jiawei Han, and Sercan O Arik.\nLong-context llms meet rag: Overcoming challenges for long inputs in rag. arXiv preprint , 2024. 2 [10] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 4015–4026, 2023. 3 [11] Geng Li, Jinglin Xu, Yunzhen Zhao, and Yuxin Peng. Dyfo:\nA training-free dynamic focus visual search for enhancing lmms in fine-grained visual understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 9098–9108, 2025. 1, 2, 3 [12] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 19730–\n19742. PMLR, 2023. 3 [13] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 26296–26306, 2024. 1 [14] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llavanext: Improved reasoning, ocr, and world knowledge, 2024. 1, 3, 7 [15] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai\nDong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, et al. Deepseek-vl: towards real-world visionlanguage understanding. arXiv preprint ,\n2024. 3 [16] Gen Luo, Yiyi Zhou, Yuxin Zhang, Xiawu Zheng, Xiaoshuai Sun, and Rongrong Ji. Feast your eyes: Mixtureof-resolution adaptation for multimodal large language models. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. 7 [17] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748–8763. PmLR, 2021. 1 [18] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual cot: Advancing multi-modal language models with a comprehensive dataset and benchmark for chain-of-thought reasoning. Advances in Neural Information Processing Systems, 37:8612–8642, 2024. 1, 3 [19] Haozhan Shen, Kangjia Zhao, Tiancheng Zhao, Ruochen Xu, Zilun Zhang, Mingwei Zhu, and Jianwei Yin. Zoomeye: Enhancing multimodal llms with human-like zooming capabilities through tree-based image exploration. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 6613–6629, 2025. 1,\n2, 3, 7 [20] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. arXiv preprint , 2023. 1 [21] Wenbin Wang, Liang Ding, Minyan Zeng, Xiabin Zhou, Li Shen, Yong Luo, Wei Yu, and Dacheng Tao. Divide, conquer and combine: A training-free framework for high-resolution image perception in multimodal large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 7907–7915, 2025. 1, 2, 3 [22] Wenbin Wang, Yongcheng Jing, Liang Ding, Yingjie Wang, Li Shen, Yong Luo, Bo Du, and Dacheng Tao. Retrievalaugmented perception: High-resolution image perception meets visual rag. arXiv preprint , 2025. 1, 2, 3, 6, 7",
    "metadata": {
      "producer": "pikepdf 8.15.1",
      "creator": "arXiv GenPDF (tex2pdf:4177c2c)",
      "creationdate": "",
      "source": "data/raw/2512.02906v2.pdf",
      "file_path": "data/raw/2512.02906v2.pdf",
      "total_pages": 16,
      "format": "PDF 1.7",
      "title": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding",
      "author": "Fan Yang; Kaihao Zhang",
      "subject": "",
      "keywords": "",
      "moddate": "",
      "trapped": "",
      "modDate": "",
      "creationDate": "",
      "page": 8
    }
  },
  {
    "page_content": "[23] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng\nGe, Jinrong Yang, Jianjian Sun, Chunrui Han, and Xiangyu Zhang.\nVary: Scaling up the vision vocabulary for large vision-language model. In European Conference on Computer Vision, pages 408–424. Springer, 2024. 3 [24] Penghao Wu and Saining Xie. V?: Guided visual search as a core mechanism in multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13084–13094, 2024. 1, 2, 3, 6 [25] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen.\nA survey on multimodal large language models.\nNational Science Review, 11(12): nwae403, 2024. 1, 3 [26] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang,\nGuanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen,\nJing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue,\nSenbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao\nHuang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng\nNie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open foundation models by 01.ai. CoRR, abs/2403.04652, 2024. 7 [27] Shi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao\nRan, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han, Zhiyuan Liu, et al. Visrag: Vision-based retrieval-augmented generation on multi-modality documents. arXiv preprint , 2024. 3 [28] Duzhen Zhang, Yahan Yu, Jiahua Dong, Chenxing Li, Dan Su, Chenhui Chu, and Dong Yu. Mm-llms: Recent advances in multimodal large language models.\nIn Findings of the\nAssociation for Computational Linguistics ACL 2024, pages 12401–12430, 2024. 3 [29] Jiarui Zhang, Mahyar Khayatkhoei, Prateek Chhikara, and Filip Ilievski. Mllms know where to look: Training-free perception of small visual details with multimodal llms. arXiv preprint , 2025. 1, 2 [30] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing” thinking with images” via reinforcement learning. arXiv preprint , 2025. 1,\n2, 3 [31] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint , 2025. 3",
    "metadata": {
      "producer": "pikepdf 8.15.1",
      "creator": "arXiv GenPDF (tex2pdf:4177c2c)",
      "creationdate": "",
      "source": "data/raw/2512.02906v2.pdf",
      "file_path": "data/raw/2512.02906v2.pdf",
      "total_pages": 16,
      "format": "PDF 1.7",
      "title": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding",
      "author": "Fan Yang; Kaihao Zhang",
      "subject": "",
      "keywords": "",
      "moddate": "",
      "trapped": "",
      "modDate": "",
      "creationDate": "",
      "page": 9
    }
  },
  {
    "page_content": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image\nUnderstanding\nSupplementary Material A. Implement Details of MRD Following [22], given an input HR image I, it is first partitioned into smaller image crops based on a predefined crop size, which corresponds to the preferred resolution of the retriver. According to the Resolution of HR image, we set the crop resolutions as 112, 224 and 448 for V ∗Bench, HR-Bench-4K and HR-Bench-8K respectively. For multiresolution semantic fusion, the ratio between the high and low resolutions is set to k = 2 in all experiments. For\nSliding Window detection, we set window size and step as 1232 and 896 for V ∗Bench, 2240 and 1792 for HRBench-4K, 3136 and 2688 for for HR-Bench-8K to balance efficiency and accuracy. The weight w of the detection confidence map is 0.4 by default in semantic detection map fusion. For the following Retrieved-Exploration Search (RESearch) process, we adopt the same hyperparameters as the baseline method RAP [22]. In all experiments, the maximum search steps are set to 200, and the answering confidence threshold τ is set to 0.6. In the following hyperparameter studies, all other hyperparameters of our MRD use their default settings unless otherwise specified. B. More Experiment Result To analyze the impact of different hyperparameters on performance, we conduct experiments on MRD using various hyperparameter settings, including Crop Resolution, Maximum Search Steps, Detection Weight, and Detection Window Size. We perform these experiments on the V ∗ Bench using both the LLaVA-ov-0.5B and LLaVA-v1.5-7B models. Unless otherwise specified, all other hyperparameters follow the default settings mentioned in section A. B.1. Effect of Crop Resolution\nIn our experiments, we evaluate the effect of different crop resolutions on performance. The results are shown in Figure 6. For the Single Instance Task (Figure 6 (a)), we observe that MRD remains highly stable across different resolutions for both models, with only minor performance fluctuations, whereas RAP exhibits much larger variations, especially when using LLaVA-ov-0.5B. For the Cross Instance Task, the performance gap between MRD and RAP is relatively small when using LLaVA-v1.5-7B. However, with LLaVA-ov-0.5B, MRD is largely unaffected by resolution changes, further demonstrating its robustness. Overall, MRD consistently outperforms RAP across different resolutions and model settings, highlighting the advantages of our approach.\nIn summary, the Multi-resolution Semantic Fusion and Detector Enhancement modules in MRD effectively mitigate the interference caused by fragmenting complete objects across multiple crops when using different crop resolutions. As a result, our MRD performance is only weakly influenced by crop resolution and achieves notably better results in the Single Instance Task. B.2. Effect of Maximum Search Steps The performance of MRD and RAP under different maximum search steps is shown in Figure 7. In Figure 7 (a), for the single-instance task, MRD consistently outperforms RAP across different max step settings on both the LLaVAov-0.5B and LLaVA-v1.5-7B models.\nIn Figure 7 (b), for the Cross Instance Task, MRD is slightly inferior to RAP only when using LLaVA-v1.57B with small max steps. However, as the max step increases, MRD surpasses RAP and maintains better performance. Overall, MRD achieves superior results compared to RAP. Notably, MRD with LLaVA-ov-0.5B performs only marginally lower than RAP with the powerful LLaVA-v1.57B model.\nMost importantly, MRD reaches its peak performance with a significantly smaller number of maximum search steps (Max Step = 30). This means that in practical applications, MRD can operate effectively with fewer steps, achieving high accuracy while reducing search time and improving efficiency.” B.3. Effect of Detection Weight\nThe results of using different detection weights are shown in Figure 8. We observe that relying solely on the semantic similarity map (weight = 0) or solely on the detection map (weight = 1) does not yield optimal performance for either task. In contrast, fusing the two maps leads to better results, demonstrating that the semantic similarity map and detection map provide complementary information.\nOverall (Figure 8 (c)), the optimal detection weight varies slightly across models: LLaVA-ov-0.5B achieves its best performance at weight = 0.4, while LLaVA-v1.5-7B performs best at weight = 0.2. B.4. Effect of Window Size\nAs shown in Figure 9, adopting different sliding-window sizes for object detection also affects the results. Except for the Cross Instance Task with LLaVA-ov-0.5B, using a",
    "metadata": {
      "producer": "pikepdf 8.15.1",
      "creator": "arXiv GenPDF (tex2pdf:4177c2c)",
      "creationdate": "",
      "source": "data/raw/2512.02906v2.pdf",
      "file_path": "data/raw/2512.02906v2.pdf",
      "total_pages": 16,
      "format": "PDF 1.7",
      "title": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding",
      "author": "Fan Yang; Kaihao Zhang",
      "subject": "",
      "keywords": "",
      "moddate": "",
      "trapped": "",
      "modDate": "",
      "creationDate": "",
      "page": 10
    }
  },
  {
    "page_content": "Figure 6. The effect of the resolution of image crops on model performance. Single and Cross represent the attribute recognition and spatial reasoning in V ∗Bench. (a) Single-instance Task. (b) Cross-instance Task. (c) Overall Performance. . Figure 7. The effect of the maximum search steps of MRD and RAP. smaller sliding-window size (Window Size = 896) generally yields better performance. This is because a smaller window reduces background interference unrelated to the target object, leading to more accurate detection results.\nHowever, a smaller window size also means that more windows are required to scan the entire high-resolution image, resulting in increased computational complexity and longer processing time. Therefore, to balance accuracy and efficiency, we select a larger sliding-window size, Window Size = 1232, as the default setting. B.5. Compared with Other HR Methods\nWe compare our MRD approach with three high-resolution processing baselines RAP, DC2 and Zoom Eye. DC2 is a training-free framework that improves MLLM comprehension of HR images by dividing them into crops, generating textual descriptions for each region, and aggregating these descriptions to obtain a more complete understanding. Zoom Eye, on the other hand, uses a tree-search strategy to traverse the hierarchical visual structure of an image, enabling efficient identification and extraction of relevant information.\nAs shown in Table 3, all HR processing methods yield overall performance improvements compared with the baseline. Among them, our MRD achieves consistently stronger results across most tasks, demonstrating its clear advantage over existing approaches",
    "metadata": {
      "producer": "pikepdf 8.15.1",
      "creator": "arXiv GenPDF (tex2pdf:4177c2c)",
      "creationdate": "",
      "source": "data/raw/2512.02906v2.pdf",
      "file_path": "data/raw/2512.02906v2.pdf",
      "total_pages": 16,
      "format": "PDF 1.7",
      "title": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding",
      "author": "Fan Yang; Kaihao Zhang",
      "subject": "",
      "keywords": "",
      "moddate": "",
      "trapped": "",
      "modDate": "",
      "creationDate": "",
      "page": 11
    }
  },
  {
    "page_content": "Figure 8. The effect of the detection weight in MRD. Figure 9. The effect of the detection window size in MRD. Table 3. Comparison of MRD with existing works on high-resolution benchmarks. We conduct experiments on V ∗Bench and HR-Bench using LLaVA-v1.5 7B.\nMethod\nV* Bench\nHR-Bench 4K\nHR-Bench 8K ∆(↑)\nAttribute\nSpatial\nOverall\nFSP\nFCP\nOverall\nFSP\nFCP\nOverall LLaVA-v1.5-7B\n43.5\n56.6\n48.7\n38.5\n33.8\n36.1\n33.0\n31.3\n32.1\n-\n-w/ DC2\n49.6\n59.2\n51.6\n45.3\n37.0\n41.1\n36.5\n33.3\n34.9 +3.5\n-w/ Zoom Eye\n83.5\n82.9\n83.3\n67.8\n38.8\n53.3\n65.5\n36.0\n50.8 +23.5\n-w/ RAP\n90.4\n96.1\n91.1\n73.8\n40.5\n57.1\n72.3\n35.3\n53.8 +28.4\n-w/ MRD (ours)\n97.4\n96.1\n95.6\n76.8\n42.7\n59.7\n72.6\n37.2\n54.9 +31.1 C. Case Study C.1. Single-instance Perception Task Examples\nFigure 10 shows two single-instance perception cases from each HR benchmarks using RAP and MRD on LLaVAv1.5-7B. From the first to the last column, we show: the HR image, the RAP semantic similarity map, the object detection confidence map, the MRD semantic–detection fusion map, the RAP result and the MRD result. From the",
    "metadata": {
      "producer": "pikepdf 8.15.1",
      "creator": "arXiv GenPDF (tex2pdf:4177c2c)",
      "creationdate": "",
      "source": "data/raw/2512.02906v2.pdf",
      "file_path": "data/raw/2512.02906v2.pdf",
      "total_pages": 16,
      "format": "PDF 1.7",
      "title": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding",
      "author": "Fan Yang; Kaihao Zhang",
      "subject": "",
      "keywords": "",
      "moddate": "",
      "trapped": "",
      "modDate": "",
      "creationDate": "",
      "page": 12
    }
  },
  {
    "page_content": "visualization of RAP semantic similarity maps, we can observe that due to the crop partitioning, a complete object may be divided across multiple crops, leading to inconsistencies in semantic similarity among different parts of the object. This inconsistency interferes with the subsequent retrieval process. For example, in the second case of HRBench4K, RAP only retrieves the right half of the speedlimit sign, resulting in an incorrect final prediction. In addition, the semantic similarity maps contain many false positives; for instance, in the first case of HR-Bench8K, the sky region—irrelevant to the query—shows undesirably high similarity scores.\nMRD addresses these issues by using multi-resolution semantic fusion to correct the semantic inconsistencies across different parts of the object, ensuring its completeness. Moreover, by incorporating an object detection model to directly localize the target, MRD reinforces the similarity of the true target region while suppressing false positives. As shown in the figure, the MRD semantic–detection fusion map exhibits much clearer contrast between the target and irrelevant regions compared with RAP, significantly reducing false positives and enabling more accurate retrieval of the target-related crops during the search process. C.2. Cross-instance Perception Task Examples\nFigure 11 shows two cross-instance perception cases from each HR benchmarks using RAP and MRD on LLaVAv1.5-7B. In the cross-instance task, the retrieval results show that RAP often retains only a subset of the target objects while ignoring others when multiple objects need to be localized. For example, in the first case of V ∗Bench, RAP completely misses the pink umbrella. This omission becomes more pronounced when there is a large size discrepancy between different target objects. As seen in the second case of V ∗Bench and the two cases from HRBench8K, RAP tends to keep only the larger primary object while neglecting the smaller ones. Similar issues also appear in counting scenarios (e.g., the two cases in HRBench-4K), where RAP identifies only a few among multiple instances. In contrast, MRD leverages object detection to simultaneously detect all target objects, ensuring that even small objects are preserved to the greatest extent. This gives MRD a clear advantage in cross-instance perception tasks",
    "metadata": {
      "producer": "pikepdf 8.15.1",
      "creator": "arXiv GenPDF (tex2pdf:4177c2c)",
      "creationdate": "",
      "source": "data/raw/2512.02906v2.pdf",
      "file_path": "data/raw/2512.02906v2.pdf",
      "total_pages": 16,
      "format": "PDF 1.7",
      "title": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding",
      "author": "Fan Yang; Kaihao Zhang",
      "subject": "",
      "keywords": "",
      "moddate": "",
      "trapped": "",
      "modDate": "",
      "creationDate": "",
      "page": 13
    }
  },
  {
    "page_content": "Figure 10. Qualitative examples of Single-instance Perception task. We conduct experiments using LLaVA-v1.5-7B on three HR Benchmarks",
    "metadata": {
      "producer": "pikepdf 8.15.1",
      "creator": "arXiv GenPDF (tex2pdf:4177c2c)",
      "creationdate": "",
      "source": "data/raw/2512.02906v2.pdf",
      "file_path": "data/raw/2512.02906v2.pdf",
      "total_pages": 16,
      "format": "PDF 1.7",
      "title": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding",
      "author": "Fan Yang; Kaihao Zhang",
      "subject": "",
      "keywords": "",
      "moddate": "",
      "trapped": "",
      "modDate": "",
      "creationDate": "",
      "page": 14
    }
  },
  {
    "page_content": "Figure 11. Qualitative examples of Cross-instance Perception task. We conduct experiments using LLaVA-v1.5-7B on three HR Benchmarks",
    "metadata": {
      "producer": "pikepdf 8.15.1",
      "creator": "arXiv GenPDF (tex2pdf:4177c2c)",
      "creationdate": "",
      "source": "data/raw/2512.02906v2.pdf",
      "file_path": "data/raw/2512.02906v2.pdf",
      "total_pages": 16,
      "format": "PDF 1.7",
      "title": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding",
      "author": "Fan Yang; Kaihao Zhang",
      "subject": "",
      "keywords": "",
      "moddate": "",
      "trapped": "",
      "modDate": "",
      "creationDate": "",
      "page": 15
    }
  }
]