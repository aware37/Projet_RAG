[
  {
    "page_content": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image\nUnderstanding\nFan Yang\nHarbin Institute of Technology (Shenzhen)\n25b951055@stu.hit.edu.cn\nKaihao Zhang\nHarbin Institute of Technology (Shenzhen)\nsuper.khzhang@gmail.com\nAbstract\nUnderstanding high-resolution images remains a signif-\nicant challenge for multimodal large language models\n(MLLMs).\nRecent study address this issue by dividing\nthe image into smaller crops and computing the seman-\ntic similarity between each crop and a query using a pre-\ntrained retrieval-augmented generation (RAG) model. The\nmost relevant crops are then selected to localize the tar-\nget object and suppress irrelevant information. However,\nsuch crop-based processing can fragment complete objects\nacross multiple crops, thereby disrupting the computation\nof semantic similarity. In our experiments, we find that im-\nage crops of objects with different sizes are better handled at\ndifferent resolutions. Based on this observation, we propose\nMulti-resolution Retrieval-Detection (MRD), a training-\nfree framework for high-resolution image understanding.\nTo address the issue of semantic similarity bias caused by\nobjects being split across different image crops, we propose\na multi-resolution semantic fusion method, which integrates\nsemantic similarity maps obtained at different resolutions to\nproduce more accurate semantic information and preserve\nthe integrity of target objects. Furthermore, to achieve di-\nrect localization of target objects at a global scale, we in-\ntroduce an open-vocalbulary object detection (OVD) model\nthat identifies object regions using a sliding-window ap-\nproach.Experiments on high-resolution image understand-\ning benchmarks using different MLLMs demonstrate the ef-\nfectiveness of our approach.\n1. Introduction\nMultimodal Large Language Models (MLLMs) have\ndemonstrated significant advancements in integrating and\ninterpreting visual and linguistic information, enabling ro-\nbust capabilities in vision-language understanding, reason-\ning, and interactive tasks [25]. By leveraging visual signals,\nthese models can process and decipher complex visual in-\nformation, forming a bridge between pixel-level data and\nFigure 1. Overview of the proposed Multi-resolution Retrieval-\nDetection framework, which uses RAG and OVD to obtain se-\nmantic similarity map and detection confidence map respectively.\nBy integrating the two, the target objects can be localized more\naccurately.\nsemantic interpretation [17, 20]. However, a common prac-\ntice among most MLLMs is to process input images at fixed\nand pre-defined resolutions [1, 13, 14]. While this uniform\ninput pipeline simplifies model architecture and reduces\ncomputational overhead, it introduces substantial limita-\ntions. Specifically, resizing high-resolution (HR) real-world\nimages to a fixed low resolution often leads to shape distor-\ntion and blurring, which degrades the quality of fine-grained\nvisual details. Recent studies [11, 19, 22, 24, 29, 30] indi-\ncate that existing methods remain unsatisfactory for high-\nresolution image tasks. This is clearly demonstrated by their\nsuboptimal results on dedicated high-resolution image un-\nderstanding benchmarks [21, 24].\nTo address this limitation, improving the high-resolution\nimage perception capability of MLLMs has become an\nemerging research focus.\nA common ”locate-and-zoom-\nin” strategy is widely adopted to enhance detail percep-\ntion in models. Although training-based approaches such\nas Supervised Fine-Tuning (SFT) [18] and Reinforcement\narXiv:2512.02906v2  [cs.CV]  3 Dec 2025",
    "metadata": {
      "producer": "pikepdf 8.15.1",
      "creator": "arXiv GenPDF (tex2pdf:4177c2c)",
      "creationdate": "",
      "source": "data/raw/2512.02906v2.pdf",
      "file_path": "data/raw/2512.02906v2.pdf",
      "total_pages": 16,
      "format": "PDF 1.7",
      "title": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding",
      "author": "Fan Yang; Kaihao Zhang",
      "subject": "",
      "keywords": "",
      "moddate": "",
      "trapped": "",
      "modDate": "",
      "creationDate": "",
      "page": 0
    }
  },
  {
    "page_content": "Learning (RL)\n[30] can effectively identify relevant re-\ngions, they are hampered by critical limitations, including\nhigh computational costs, long training cycles, and poor\ncross-architecture transferability, which curtails their scal-\nability and practical application. In contrast, training-free\nmethods [11, 19, 29] automatically locate regions by using\nattention mechanisms or tree-based search, without requir-\ning the construction of the dataset or the fine-tuning of the\nmodel. Despite employing a top-down search strategy from\nhigh to low resolution, these methods face significant limi-\ntations. A primary issue is the model’s inadequate percep-\ntion of small objects during the initial search stage [19, 21],\nwhich frequently leads to the generation of erroneous search\npaths.\nRecently,\nInspired\nby\nthe\nsuccess\nof\nRetrieval-\nAugmented Generation (RAG) for enabling long-context\nunderstanding in general LLMs\n[9], Wang et al.\n[22]\nintroduced Retrieval-Augmented Perception (RAP) — a\ntraining-free framework designed to enhance MLLMs’ per-\nception of high-resolution images.\nRAP extends this\nparadigm to the visual domain, achieving significant perfor-\nmance improvement on high-resolution benchmarks. The\nRAP framework consists of three key components: First,\nthe Visual Retrieval module employs a pre-trained vision\nRAG model, VisRAG, to compute semantic similarity be-\ntween the query and different image regions (crops), re-\ntrieving the most relevant ones to reduce noise.\nNext,\nthe Spatial-Awareness Layout module preserves the origi-\nnal relative spatial relationships among the retrieved crops\nwhen composing them into the model input, maintain-\ning spatial coherence.\nFinally, the Adaptive Retrieval-\nExploration Search (RE-Search) module dynamically deter-\nmines the optimal number of retrieved crops by constructing\na Retrieval-Exploration Tree (RE-Tree), balancing informa-\ntion sufficiency with computational efficiency.\nDespite its promise, the RAP method suffers from sev-\neral inherent limitations. First, the patching operation can\nfragment large objects across multiple disjointed crops, dis-\nrupting their holistic semantics and leading to biased simi-\nlarity calculations. Our empirical observations confirm that\nsome patches semantically irrelevant to the query can ob-\ntain abnormally high similarity scores. Second, the patch\nresolution is a critical yet difficult-to-tune hyperparameter:\noverly large patches introduce redundant background infor-\nmation, while overly small ones exacerbate object fragmen-\ntation. Experiments show that the choice of resolution sig-\nnificantly impacts performance. Third, in high-resolution\nimages with cluttered backgrounds, the similarity measure\nis prone to false positives, where background regions may\nattain higher similarity than those containing the actual tar-\nget objects, severely hampering recognition.\nTo tackle these challenges, we propose a novel Multi-\nresolution Retrieval-Detection (MRD) framework, based\non RAP to improve retrieval quality and localization accu-\nracy through two key techniques:\n• Multi-resolution Semantic Fusion: To mitigate the bias\ninherent in single-resolution patching, we design a sim-\nple yet effective fusion strategy. It computes semantic\nsimilarities across multiple proportional resolutions and\nperforms consistency-based fusion to calibrate the results,\nyielding a more robust and accurate relevance estimation\nthat alleviates semantic deviations caused by object frag-\nmentation.\n• Open-vocabulary Detector Enhancement: For more\nprecise target localization, we incorporate an advanced\nopen-vocabulary object detector, LLMDet\n[7].\nFirst,\nwe leverage the in-context learning capability of LLMs\nto extract target concepts from the query, defining the\ncategories for the detector. Subsequently, a sliding win-\ndow mechanism is employed to traverse the entire high-\nresolution image, detecting target objects within each\nwindow to generate a confidence map indicating target\npresence.\nFinally, the calibrated multi-resolution semantic similar-\nity is augmented by the object detection confidence. This\nsynergistic fusion effectively amplifies the response in true\ntarget regions, enabling faster and more accurate localiza-\ntion of critical areas during subsequent retrieval, thereby\nguiding the MLLM toward more reliable inference.\nWe conduct extensive experiments on several high-\nresolution benchmarks, including V* [24], HRBench-4K,\nand HRBench-8K [21], utilizing various MLLMs such\nas LLaVA-ov and LLaVA-v1.5. The results demonstrate\nthat our MRD framework surpasses all existing training-\nfree methods and achieves state-of-the-art performance on\nboth single-object and multi-object retrieval and recognition\ntasks, with particularly notable gains on single-object tasks.\nOur contributions are summarized as follows:\n• To the best of our knowledge, this is the first work\nthat systematically leverages an open-vocabulary object\ndetector to enhance MLLMs’ understanding of high-\nresolution images. Experiments validate that the detector\nprovides precise target localization, effectively suppress-\ning interference from irrelevant regions.\n• We propose MRD, a training-free and generic frame-\nwork. It innovatively corrects semantic similarity via a\nmulti-resolution fusion strategy and integrates open-set\ndetection results to enhance target regions, creating a syn-\nergistic effect.\n• Comprehensive experiments validate the effectiveness\nand generalization of our method.\nIt achieves lead-\ning performance on both single-object and multi-object\ntasks across different MLLMs and high-resolution bench-\nmarks.",
    "metadata": {
      "producer": "pikepdf 8.15.1",
      "creator": "arXiv GenPDF (tex2pdf:4177c2c)",
      "creationdate": "",
      "source": "data/raw/2512.02906v2.pdf",
      "file_path": "data/raw/2512.02906v2.pdf",
      "total_pages": 16,
      "format": "PDF 1.7",
      "title": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding",
      "author": "Fan Yang; Kaihao Zhang",
      "subject": "",
      "keywords": "",
      "moddate": "",
      "trapped": "",
      "modDate": "",
      "creationDate": "",
      "page": 1
    }
  },
  {
    "page_content": "2. Related Work\n2.1. Multimodal Large Language Models\nMLLMs have rapidly advanced as powerful foundation\nmodels capable of understanding and generating multi-\nmodal content across diverse vision–language tasks [25,\n28].\nEarly MLLM architectures generally adopt fixed-\nresolution vision encoders—such as224×224 or 448×448\nViTs\n[2, 12–14].\nWhile this design simplifies training\nand computation, it inevitably requires resizing or crop-\nping high-resolution (HR) images, thereby discarding fine-\ngrained visual details crucial for tasks such as fine-grained\nrecognition, dense reasoning, or detecting small objects.\nTo enhance high-resolution image understanding with-\nout proportionally increasing the computational burden\nfrom visual tokens, several studies have integrated high-\nresolution visual encoders into MLLMs. For example, Vary\n[23] and Deepseek-VL [15] incorporate the SAM encoder\n[10] to improve model performance on HR images.\nAlternatively,\nanother\nline\nof\nwork\nintroduced\nNative/Dynamic-Resolution\nMLLMs\nthat\nprocesses\nimages at their native resolution. The core idea is to gener-\nate a variable-length sequence of visual tokens that adapts\nto the original dimensions of the input image, thereby\npreserving spatial fidelity and high-frequency details.\nThese models employ various mechanisms to handle the\nresulting long sequences and computational complexity,\nincluding sliding window attention, dynamic masking, and\npatch-based encoding strategies. Representative works in\nthis category have demonstrated significant progress. For\ninstance, the InternVL series [4, 6, 31] adopts a strategy\nof splitting a high-resolution image into multiple fixed-size\npatches.\nIn contrast, models like Qwen2.5-VL\n[2, 3]\ntake an end-to-end approach by training a ViT directly on\nnative-resolution images. This allows the model to embed\nthe entire image into a single, coherent token sequence\nin one forward pass, potentially leading to better global\ncontext understanding.\n2.2. High-Resolution Image Understanding\nMultimodal large language models (MLLMs) have made\nsubstantial progress in recent years; however, they con-\ntinue to face challenges in accurately recognizing and in-\nterpreting fine-grained details within high-resolution (HR)\nimages\n[21?\n].\nTo enhance the capability of MLLMs\nin high-resolution image understanding, existing studies\ngenerally follow two main directions. Training-based ap-\nproaches rely on supervised fine-tuning (SFT) [18, 24] or\nreinforcement learning (RL) [30], but such methods often\ncompromise the model’s generalization ability on broad vi-\nsion–language tasks. In contrast, training-free approaches\n[11, 19, 21, 22]typically perform hierarchical or tree-based\nsearch to localize target regions. However, these methods\ntend to suffer from low efficiency and may fail to retain\nall target objects during the search process, particularly in\nmulti-object scenarios.\n3. Preliminary\nIn this section, We first conduct an analysis of the relation-\nship between the resolution of image crops and the perfor-\nmance of MLLMs in subsection 3.2. The experimental re-\nsults indicate that using different resolution has a significant\nimpact on MLLMs to analyze HR images. Objects of dif-\nferent sizes are suitable for different resolutions. Inspired\nby this we propose the MRD framework.\n3.1. Semantic Similarity\nThis section presents the pipeline for integrating Retrieval-\nAugmented Generation (RAG) into Multimodal Large Lan-\nguage Models (MLLMs) to calculate the semantic sim-\nilarity scores between the query embedding and images\ncrops from HR images.\nGiven an HR image, we first\npartition it collection of image patches, denoted as P =\n{p1, p2, . . . , pn}, where n is the total number of image\ncrops. Following the approach of Yu et al.\n[27], the tex-\ntual query and each image crop are encoded independently\nusing the text and image encoders of a Vision-Language\nModel (VLM), producing a sequence of hidden represen-\ntations. The semantic similarity score between the query\nembedding and each image crop embedding is then com-\nputed. Specifically, the similarity score s(q, pi) for the i-th\ncrop is calculated by the cosine similarity of the query and\nimage crop embeddings:\ns(q, pi) = 1\n2 · (1 +\nq · pi\n∥q∥· ∥pi∥)\n(1)\nBased on these scores, the top K most relevant image\ncrops are selected and provided to the MLLM to support\ndetailed understanding of the high-resolution input.\n3.2. Impact of the Resolution of Image Crops\nIn this section, we conduct an analysis to investigates the\nrelationship between the resolution of image crops and the\nperformance of MLLMs in HR image understanding.\nExperimental setting. We analyze the relation between\nperformance and the resolution of image crops, using\nLLaVA-ov and LLaVA-v1.5 on V * benmark.\nObservations. We visualize the relationship between the\nresolution of image crops and performance of MLLMs. As\nshown in Figure 3, from the overall accuracy obtained by\nusing different resolutions, when the resolution of image\ncrops is set to 112, using different MLLM achieved the\nhighest accuracy rates in both the single-object task and the\nmulti-object task. This indicates that setting the resolution\nto 112 might be the optimal choice. However, when we take",
    "metadata": {
      "producer": "pikepdf 8.15.1",
      "creator": "arXiv GenPDF (tex2pdf:4177c2c)",
      "creationdate": "",
      "source": "data/raw/2512.02906v2.pdf",
      "file_path": "data/raw/2512.02906v2.pdf",
      "total_pages": 16,
      "format": "PDF 1.7",
      "title": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding",
      "author": "Fan Yang; Kaihao Zhang",
      "subject": "",
      "keywords": "",
      "moddate": "",
      "trapped": "",
      "modDate": "",
      "creationDate": "",
      "page": 2
    }
  },
  {
    "page_content": "Figure 2. Setting the resolution of image crops to 112 causes complete objects to be split across different regions, which disrupts the\nsemantic information of the target objects.\nFigure 3. The effect of the resolution of retrieved image crops on\nmodel performance. Attribute and Spatial represent the attribute\nrecognition and spatial reasoning in V ∗Bench.\na closer look at the results of each sample, we find that in\nsome cases, choosing a different resolution actually leads\nto more accurate results compared to when the resolution is\n112, as shown in Figure 3. The results of the image crops\nselected based on different resolutions and the visualization\nof the semantic similarity map can provide a very intuitive\nanalysis of the reasons: Since the complete object is divided\ninto different crops, some parts of it have a higher semantic\nsimilarity calculated by VisRAG, while other parts have a\nlower semantic similarity. After screening, only the parts\nwith higher similarity are retained. However, this will dam-\nage the integrity of the target object and cause interference\nto the judgment of MLLM.\n4. Method\nIn this section, we propose a novel framework named\nMulti-resolution Retrieval Detection (MRD). The core\ndesign of MRD lies in its multi-resolution approach at dif-\nferent scales to better localize regions containing target ob-\njects. This enables subsequent search processes to more\neasily identify image crops corresponding to the target ob-\njects, eliminating irrelevant distractions and enhancing the\nperceptual understanding of HR images by MLLMs. Based\non the findings in subsection 3.2, we argue that using dif-\nferent resolutions for semantic similarity computation is\nmore suitable for objects of varying sizes and locations. In-\nspired by this idea, we first introduce a simple yet effective\nMulti-resolution Semantic Fusion method, which computes\nsemantic similarity maps at different resolutions on a lo-\ncal scale and performs consistency-based fusion to refine\nthe semantic similarity and improve its accuracy. To more\ndirectly localize target objects, we incorporate an Open-\nvocabulary object detection model that traverses the en-\ntire HR image globally using a sliding window approach,\ngenerating confidence scores for regions containing target\nobjects.\nFinally, by integrating the detection confidence\nscores with the multi-resolution semantic similarity maps,\nour method not only improves localization of target regions\nbut also distinguishes fine-grained differences among crops\nin these regions, thereby assisting subsequent search pro-\ncesses in more accurately identifying key areas. The fol-\nlowing sections will provide detailed explanations of each\ncomponent.\n4.1. Multi-resolution Semantic Fusion\nIn subsection 3.2, we observe that image crops of different\nresolutions are suitable for objects of varying sizes and loca-\ntions in different cases. Compared to the semantic similarity\nmap obtained using a single resolution, those derived from\nmultiple resolutions exhibit respective advantages. There-\nfore, we first propose a Multi-Resolution Semantic Fusion\nmethod. As shown in the top part of Figure 4, we partition\nthe HR image using proportional resolutions, with the low\nresolution set to l and the high resolution set to ˆl, where",
    "metadata": {
      "producer": "pikepdf 8.15.1",
      "creator": "arXiv GenPDF (tex2pdf:4177c2c)",
      "creationdate": "",
      "source": "data/raw/2512.02906v2.pdf",
      "file_path": "data/raw/2512.02906v2.pdf",
      "total_pages": 16,
      "format": "PDF 1.7",
      "title": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding",
      "author": "Fan Yang; Kaihao Zhang",
      "subject": "",
      "keywords": "",
      "moddate": "",
      "trapped": "",
      "modDate": "",
      "creationDate": "",
      "page": 3
    }
  },
  {
    "page_content": "Figure 4. Detailed information of our propsoed MRD. First, We use VisRAG with different resolution of image crops to obtain multi-\nresolution semantic similarity map. We then employ an open-set object detection model, LLMDet, to localize the target objects extracted\nfrom the query within the high-resolution image using a sliding-window approach, yielding a global detection confidence map. Finally,\nthe obtained multi-resolution semantic similarity map is linearly fused with the detection confidence map, and the fused scores are used to\nguide the subsequent search to select image crops containing the target objects.\nˆl = k · l. The set of image patches at high resolution is\ndenoted as ˆP = {ˆp1, ˆp2, . . . , ˆpm}, and at low resolution as\nP = {p1, p2, . . . , pn}. Due to the proportional relationship\nbetween high and low resolutions, we have n = k2 · m,\nand each high-resolution patch ˆpi corresponds to k2 low-\nresolution patches:\nˆpi =\n\n\n˜pi,1\n˜pi,2\n· · ·\n˜pi,k\n˜pi,(k+1)\n˜pi,(k+2)\n· · ·\n˜pi,(2k)\n...\n...\n...\n...\n˜pi,(k(k−1)+1)\n˜pi,(k(k−1)+2)\n· · ·\n˜pi,k2\n\n\n(2)\nwhere ˜pij ∈P, i ∈{1, 2, . . . , m},\nj ∈{1, 2, . . . , k2}.\nThen, using Equation 1, we compute the cosine similar-\nity between the query and image crop embeddings to ob-\ntain the semantic similarity scores for high and low res-\nolutions, respectively:\nˆS = {ˆs1, ˆs2, . . . , ˆsm} and S =\n{s1, s2, . . . , sn}:\nˆsi = s(f(q), g(ˆpi)),\nsj = s(f(q), g(pj))\n(3)\nwhere f(·) and g(·) denote the embedding operations\nfor the query and image crop, respectively, with i\n∈\n{1, 2, . . . , m} and j ∈{1, 2, . . . , n}. According to the map-\nping from ˆpi to pj in (2), we can map the semantic similarity\nˆs obtained from each high-resolution ˆp and the query to the\ncorresponding k2 low-resolution p positions. The mapping\noperation can be expressed as:\n˜S = H( ˆS)\n(4)\nwhere ˜S = {˜s1, ˜s2, . . . , ˜sn}. After obtaining ˜S and S,\nwe perform consistency fusion on the semantic similarities\nat corresponding positions to obtain the multi-resolution se-\nmantic similarity Sf = {sf\n1, sf\n2, . . . , sf\nn}, which can be ex-\npressed as:\nsf\nt =\np\n˜st · st,\nt ∈1, 2, . . . , n\n(5)\nFinally, we can transform the semantic similarity scores into\na two-dimensional semantic similarity map sf(i, j) with i ∈\n{1, 2, . . . , H} and j ∈{1, 2, . . . , W}. The total number of\nlow resolution image crop n = H × W .\nFusing the multi-resolution semantic similarity scores\nfrom high and low resolutions enables correction of the low-\nresolution similarities when a complete object is split across\ndifferent patches in the low-resolution view. This enhances\nthe similarity of various parts of the object, thereby preserv-\ning the integrity of the object as much as possible during\nsubsequent search processes and improving the recognition\naccuracy of the MLLM.\n4.2. Open-vocabulary Detector Enhancement\nVisRAG divides the HR image into patches and computes\nsemantic similarity between the query and each image crop,",
    "metadata": {
      "producer": "pikepdf 8.15.1",
      "creator": "arXiv GenPDF (tex2pdf:4177c2c)",
      "creationdate": "",
      "source": "data/raw/2512.02906v2.pdf",
      "file_path": "data/raw/2512.02906v2.pdf",
      "total_pages": 16,
      "format": "PDF 1.7",
      "title": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding",
      "author": "Fan Yang; Kaihao Zhang",
      "subject": "",
      "keywords": "",
      "moddate": "",
      "trapped": "",
      "modDate": "",
      "creationDate": "",
      "page": 4
    }
  },
  {
    "page_content": "enabling localized object retrieval at a small scale. How-\never, it struggles to accurately localize larger objects. To\naddress this limitation and achieve more direct and large-\nscale object localization, we introduce an advanced open-\nvocabulary object detection model—LLMDet—to directly\nlocate regions containing target objects. First, we employ\nin-context learning with a Large Language Model (LLM)\nto extract the primary target objects from the query, which\nserve as the target categories for LLMDet. Due to the ex-\ntremely high resolution of HR images in datasets such as\nHR-Bench, we adopt a sliding window strategy for object\nlocalization. To align with the semantic similarity map de-\nrived from image crops, we assign the detection confidence\nscores of the target bounding boxes to their corresponding\nimage patches, thereby generating a detection map that re-\nflects the confidence of target object presence in each patch.\nThis detection map offers a more intuitive localization rep-\nresentation compared to the semantic similarity map. In the\nfollowing, we provide a detailed introduction to the pro-\nposed method.\nObject Extraction. To enable open-vocabulary object de-\ntection, we first leverages Large Language Models (LLMs)\nto dynamically identify target objects from textual queries.\nGiven an input query Q, we employ in-context learning to\nextract the primary object entities that serve as detection tar-\ngets for our LLMDet framework.\nFormally, we define the object extraction process as:\nO = LLM(Psystem, Eexamples, Q)\n(6)\nwhere O represents the set of extracted objects, Psystem\ndenotes the system prompt containing extraction guidelines,\nand Eexamples constitutes the demonstration examples.\nSliding-window Object Detection. In order to get a global\ndetection confidence map align with the previously seman-\ntic similarity map, we similarly partition the HR image into\na grid of H × W non-overlapping patches where total num-\nber of patches n = H ×W. A sliding window of size h×w\npatches (where h < H and w < W) traverses the entire\nimage with a predefined stride. In this way, We can obtain\nT sliding windows W = {W1, W2, ..., WT }.\nAfter obtaining multiple sliding windows using the slid-\ning window method, we use LLMDet to detect objects\nwithin each sliding window. The detector generates a set\nof bounding boxes Bt = {b1, b2, . . . , bKt} and the corre-\nsponding confidence scores sk indicating the likelihood of\ncontaining a target object where t denotes the t-th sliding\nwindow.\nWe apply a confidence threshold τ to filter out low-\nquality detections:\nBfilter\nt\n= {bk ∈Bt | sk > τ}\n(7)\nSubsequently, we generate a window detection confi-\ndence map cw\nt ∈Rh×w for the current window. The value\nat patch coordinate (p, q) within this local window is as-\nsigned the maximum confidence score among all bounding\nboxes in Bfiltered\nt\nthat contain this patch. If no box covers the\npatch, the confidence is set to 0:\ncw\nt (p, q) = max\nbk∈Bfilter\nt\n{sk · I[(m, n) ∈bk]}\n(8)\nwhere I[·] is the indicator function that equals 1 if the\npatch (p, q) is inside the bounding box bk, and 0 otherwise.\nTo aggregate information from all sliding windows and\nform a global, unified detection confidence map cg\n∈\nRH×W for the entire high-resolution image, we employ an\naveraging fusion strategy. For a global patch at coordinate\n(p, q), its final confidence score is computed as the average\nof all confidence scores assigned to it from every sliding\nwindow that contained it.\nFor the patch I(i, j) at position (i, j) in the HR image,\nif it is contained in the t-th sliding window, we denote its\nposition in the t-th sliding window Wt as (ti, tj), which\ncan be expressed as\nI(i, j) = Wt(ti, tj),\nt ∈Ti,j\n(9)\nwhere Tij denotes the set of sliding windows that contain\nI(i, j). Now, we can obtain the global detection confidence\nmap of the whole HR image, which can be computed as:\ncg(i, j) =\n1\n|Ti,j|\nX\nt∈Ti,j\ncw\nt (ti, tj)\n(10)\nwhere i ∈{1, 2, . . . , H} and j ∈{1, 2, . . . , W}.\nThe detection confidence map provides effective local-\nization of target regions on a global scale, offering direct\nspatial guidance but lacking the ability to distinguish fine-\ngrained differences within the target object. To address this\nlimitation, we integrate the detection confidence with multi-\nresolution semantic similarity through linear combination,\nwhich can be expressed as:\nsF(i, j) = (1 −w) · sf(i, j) + w · cg(i, j)\n(11)\nThis synergistic fusion enables precise target localiza-\ntion while effectively highlighting intra-object variations,\nthereby facilitating more accurate extraction of key regions\nin subsequent search processes. The details of the subse-\nquent Retrieved-Exploration Search process can be found\nin paper [22].\n5. Experiments\nEvaluated benchmark.\nWe evaluate our MRD on two\nhigh-resolution benchmarks. The first is V ∗Bench [24],\nwith an average resolution of 2246 × 1582, consists of\ntwo sub-tasks: attribute recognition and spatial reasoning.\nThe Second is HRBench which includes two sub-task Fine-\ngrained Single-instance Perception (FSP) and Fine-grained\nCross-instance Perception (FCP).",
    "metadata": {
      "producer": "pikepdf 8.15.1",
      "creator": "arXiv GenPDF (tex2pdf:4177c2c)",
      "creationdate": "",
      "source": "data/raw/2512.02906v2.pdf",
      "file_path": "data/raw/2512.02906v2.pdf",
      "total_pages": 16,
      "format": "PDF 1.7",
      "title": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding",
      "author": "Fan Yang; Kaihao Zhang",
      "subject": "",
      "keywords": "",
      "moddate": "",
      "trapped": "",
      "modDate": "",
      "creationDate": "",
      "page": 5
    }
  },
  {
    "page_content": "Table 1. Comparison of MRD with existing works on high-resolution benchmarks\nMethod\nV* Bench\nHR-Bench 4K\nHR-Bench 8K\nAttribute\nSpatial\nOverall\nFSP\nFCP\nOverall\nFSP\nFCP\nOverall\nOpen-source MLLMs\nLLaVA-v1.6-7B [14]\n60.9\n63.2\n61.8\n49.0\n46.8\n47.9\n37.3\n44.3\n40.8\nLLaVA-v1.6-13B [14]\n60.0\n64.5\n61.8\n49.8\n41.3\n45.5\n38.0\n38.3\n38.1\nLLaVA-v1.6-34B [14]\n-\n-\n-\n55.3\n50.5\n52.9\n44.5\n50.3\n47.4\nLLaVA-HR-X-13B [16]\n-\n-\n-\n61.3\n46.0\n53.6\n49.5\n44.3\n46.9\nLLaVA-HR-X-7B [16]\n51.3\n64.5\n56.5\n57.8\n46.3\n52.0\n42.0\n41.3\n41.6\nInternVl-1.5-26B [5]\n-\n-\n-\n69.5\n51.8\n60.6\n69.3\n48.5\n57.9\nYi-VL-34B [26]\n-\n-\n-\n46.0\n42.8\n44.4\n39.5\n38.5\n39.0\nClosed-source MLLMs\nGPT-4o [8]\n-\n-\n66.0\n70.0\n48.0\n59.0\n62.0\n49.0\n55.5\nQwen-VL-max [2]\n-\n-\n-\n65.0\n52.0\n58.5\n54.0\n51.0\n52.5\nBaselines and MRD\nLLaVA-v1.5-7B [14]\n43.5\n56.6\n48.7\n38.5\n33.8\n36.1\n33.0\n31.3\n32.1\nLLaVA-v1.5-7B-Zoom Eye [19]\n83.5\n82.9\n83.3\n67.8\n38.8\n53.3\n65.5\n36.0\n50.8\nLLaVA-v1.5-7B-RAP [22]\n90.4\n96.1\n91.1\n73.8\n40.5\n57.1\n72.3\n35.3\n53.8\nLLaVA-v1.5-7B-MRD (ours)\n97.4\n96.1\n95.6\n76.8\n42.7\n59.7\n72.6\n37.2\n54.9\nLLaVA-ov-0.5B [14]\n63.5\n64.5\n63.9\n63.5\n39.5\n51.5\n47.3\n38.3\n42.8\nLLaVA-ov-0.5B-Zoom Eye[19]\n85.2\n73.7\n80.6\n75.5\n39.8\n57.6\n68.5\n38.3\n53.4\nLLaVA-ov-0.5B-RAP [22]\n80.0\n84.2\n83.6\n80.3\n42.3\n61.3\n81.8\n45.3\n63.5\nLLaVA-ov-0.5B-MRD (ours)\n89.6\n82.9\n88.0\n84.0\n45.2\n64.6\n81.8\n47.3\n64.5\nFigure 5. Visualization of the Effects of Different Modules in MRD. Upper: Visualization of the Effects of the Multi-resolution Semantic\nFusion Method. Lower: Visualization of the Effects of the Multi-resolution Semantic Fusion Method\n5.1. Main Results\nAs shown in Table 1, compared with both the baseline\nMLLMs and previous baseline approaches, our proposed",
    "metadata": {
      "producer": "pikepdf 8.15.1",
      "creator": "arXiv GenPDF (tex2pdf:4177c2c)",
      "creationdate": "",
      "source": "data/raw/2512.02906v2.pdf",
      "file_path": "data/raw/2512.02906v2.pdf",
      "total_pages": 16,
      "format": "PDF 1.7",
      "title": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding",
      "author": "Fan Yang; Kaihao Zhang",
      "subject": "",
      "keywords": "",
      "moddate": "",
      "trapped": "",
      "modDate": "",
      "creationDate": "",
      "page": 6
    }
  },
  {
    "page_content": "MRD framework consistently delivers substantial perfor-\nmance gains across all sub-tasks, datasets, and model con-\nfigurations. The improvement is most pronounced on the\nV ∗dataset using the LaVA-v1.5-7B model, where MRD\nachieves a remarkable 46.9% absolute increase in accu-\nracy—nearly doubling the original performance.\nSignif-\nicant gains are also observed on HR-Bench 4K and HR-\nBench 8K, with maximum improvements of 23.6% and\n22.8%, respectively.\nIn comparison to the state-of-the-art baseline RAP, MRD\nachieves superior performance across all datasets and model\nsettings, yielding an average improvement of 2.8%. When\nexamining results across sub-task categories, MRD demon-\nstrates particularly strong performance on single-object\ntasks. We attribute this advantage to the integration of a de-\ntection module, which provides more accurate localization\nfor isolated objects.\nOverall, these results indicate that MRD markedly en-\nhances the perception and understanding capabilities of\nMLLMs when operating on high-resolution images.\n5.2. Effect of the Multi-resolution Semantic Fusion\nMulti-resolution Semantic Fusion can obtain more accu-\nrate information by integrating semantic similarity maps\nfrom different resolutions. From the two cases shown in\nthe upper part of Figure 5, we can clearly observe that\nincorporating multi-resolution semantic fusion allows the\nhigh-resolution semantic similarity map to correct the low-\nresolution map, alleviating semantic deviations caused by\ndifferent parts of the target object being split across multi-\nple patches. This helps better preserve the integrity of the\ntarget object. The results in the cases demonstrate that the\napproach is effective for both single-object and multi-object\ntasks. Overall, the experimental results indicate that Multi-\nresolution Semantic Fusion provides better adaptability to\nobjects of different sizes compared to using a single resolu-\ntion.\n5.3. Effect of Open-vocabulary Object Detection\nTo achieve more accurate and direct localization of the tar-\nget object at a global scale, we introduce an open-set ob-\nject detection model. As shown in lower part of Figure 5,\nsliding-window detection results effectively identify the tar-\nget object’s location. By combining the detection results\nwith semantic similarity scores, MRD amplifies the scores\nof patches that contain the target object while suppressing\nfalse-positive patches that also exhibit high semantic simi-\nlarity. This integration facilitates a more efficient and accu-\nrate patch retrieval process in subsequent searching.\n5.4. Ablation Study\nTo better understand the contributions of different mod-\nules in our MRD framework, we conduct ablation studies\nTable 2. Ablation study of different module in MRD.\nV* Bench\n∆↑\nAttribute\nSpatial\nOverall\nRAP\n80.0\n84.2\n83.6\n-\nOVD\n84.3\n81.6\n84.9\n+1.3\nRAP+Multi-res\n82.9\n85.2\n85.8\n+2.2\nRAP+OVD\n85.2\n84.2\n86.2\n+2.6\nRAP+OVD+Multi-Res\n90.4\n85.5\n89.3\n+5.7\non the V ∗dataset using the LLaVA-ov-0.5B model. As\nshown in Table 2, using the OVD model alone (second\nrow) yields higher localization accuracy for single-object\ntasks, but its performance on multi-object tasks is inferior to\nRAP. When RAP employs multi-resolution semantic fusion\n(third row), performance improves on both single-object\nand multi-object tasks, indicating that multi-resolution se-\nmantic fusion can better handle objects of varying sizes\nacross different scenarios.\nFusing the semantic similarity map obtained from RAP\nwith the detection confidence map from OVD (fourth row)\nsignificantly improves performance on single-object tasks;\nhowever, the performance on multi-object tasks is even\nworse than using OVD alone, suggesting that some target\nobjects may be lost during the search. By further incor-\nporating multi-resolution semantic fusion, performance im-\nproves on both single-object and multi-object tasks, demon-\nstrating the effectiveness of this fusion strategy.\nIn summary, introducing OVD helps localize single ob-\njects more accurately but may result in missed objects in\nmulti-object scenarios.\nMulti-resolution semantic fusion\ncorrects semantic similarity scores and preserves object\ncompleteness under different conditions, enhancing MLLM\nperformance on both single- and multi-object tasks. The fi-\nnal model, which integrates all modules, achieves a 5.7%\nhigher accuracy than RAP, demonstrating the effectiveness\nof MRD’s design in improving high-resolution image un-\nderstanding for MLLMs.\n6. Conclusion\nIn this work, we propose a novel training-free method,\nMulti-resolution Retrieval-Detection (MRD), to enhance\nthe understanding of high-resolution images by MLLMs.\nMRD employs multi-resolution semantic similarity to cor-\nrect single-resolution similarity maps, ensuring the integrity\nof target objects. Moreover, to localize target objects more\naccurately and directly, we introduce an OVD model that\nidentifies object regions using a sliding-window approach.\nWe demonstrate the effectiveness of MRD across multiple\nhigh-resolution benchmarks with different MLLMs, show-\ning its superior performance in HR image understanding.",
    "metadata": {
      "producer": "pikepdf 8.15.1",
      "creator": "arXiv GenPDF (tex2pdf:4177c2c)",
      "creationdate": "",
      "source": "data/raw/2512.02906v2.pdf",
      "file_path": "data/raw/2512.02906v2.pdf",
      "total_pages": 16,
      "format": "PDF 1.7",
      "title": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding",
      "author": "Fan Yang; Kaihao Zhang",
      "subject": "",
      "keywords": "",
      "moddate": "",
      "trapped": "",
      "modDate": "",
      "creationDate": "",
      "page": 7
    }
  },
  {
    "page_content": "References\n[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,\nXiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei\nHuang, et al.\nQwen technical report.\narXiv preprint\narXiv:2309.16609, 2023. 1\n[2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. Qwen-vl: A versatile vision-language model for un-\nderstanding, localization, text reading, and beyond, 2023. 3,\n7\n[3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin\nGe, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun\nTang, et al. Qwen2. 5-vl technical report. arXiv preprint\narXiv:2502.13923, 2025. 3\n[4] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhang-\nwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian,\nZhaoyang Liu, et al. Expanding performance boundaries of\nopen-source multimodal models with model, data, and test-\ntime scaling. arXiv preprint arXiv:2412.05271, 2024. 3\n[5] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhang-\nwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng\nLuo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang\nYan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin,\nChao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang,\nBo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min\nDou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao,\nJifeng Dai, and Wenhai Wang. How far are we to gpt-4v?\nclosing the gap to commercial multimodal models with open-\nsource suites. Sci. China Inf. Sci., 67(12), 2024. 7\n[6] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen,\nSen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu,\nLewei Lu, et al. Internvl: Scaling up vision foundation mod-\nels and aligning for generic visual-linguistic tasks. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 24185–24198, 2024. 3\n[7] Shenghao Fu, Qize Yang, Qijie Mo, Junkai Yan, Xihan Wei,\nJingke Meng, Xiaohua Xie, and Wei-Shi Zheng.\nLlmdet:\nLearning strong open-vocabulary object detectors under the\nsupervision of large language models. In Proceedings of the\nComputer Vision and Pattern Recognition Conference, pages\n14987–14997, 2025. 2\n[8] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perel-\nman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Weli-\nhinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card.\narXiv preprint arXiv:2410.21276, 2024. 7\n[9] Bowen Jin, Jinsung Yoon, Jiawei Han, and Sercan O Arik.\nLong-context llms meet rag: Overcoming challenges for\nlong inputs in rag. arXiv preprint arXiv:2410.05983, 2024.\n2\n[10] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. In Proceedings of the IEEE/CVF international confer-\nence on computer vision, pages 4015–4026, 2023. 3\n[11] Geng Li, Jinglin Xu, Yunzhen Zhao, and Yuxin Peng. Dyfo:\nA training-free dynamic focus visual search for enhancing\nlmms in fine-grained visual understanding. In Proceedings\nof the Computer Vision and Pattern Recognition Conference,\npages 9098–9108, 2025. 1, 2, 3\n[12] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models. In In-\nternational conference on machine learning, pages 19730–\n19742. PMLR, 2023. 3\n[13] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 26296–26306, 2024. 1\n[14] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan\nZhang, Sheng Shen, and Yong Jae Lee. Llavanext: Improved\nreasoning, ocr, and world knowledge, 2024. 1, 3, 7\n[15] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai\nDong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li,\nHao Yang, et al. Deepseek-vl: towards real-world vision-\nlanguage understanding. arXiv preprint arXiv:2403.05525,\n2024. 3\n[16] Gen Luo, Yiyi Zhou, Yuxin Zhang, Xiawu Zheng, Xi-\naoshuai Sun, and Rongrong Ji. Feast your eyes: Mixture-\nof-resolution adaptation for multimodal large language mod-\nels. In The Thirteenth International Conference on Learning\nRepresentations, ICLR 2025, Singapore, April 24-28, 2025.\nOpenReview.net, 2025. 7\n[17] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748–8763. PmLR, 2021. 1\n[18] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuo-\nfan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual\ncot: Advancing multi-modal language models with a com-\nprehensive dataset and benchmark for chain-of-thought rea-\nsoning. Advances in Neural Information Processing Systems,\n37:8612–8642, 2024. 1, 3\n[19] Haozhan Shen, Kangjia Zhao, Tiancheng Zhao, Ruochen\nXu, Zilun Zhang, Mingwei Zhu, and Jianwei Yin. Zoom-\neye: Enhancing multimodal llms with human-like zooming\ncapabilities through tree-based image exploration. In Pro-\nceedings of the 2025 Conference on Empirical Methods in\nNatural Language Processing, pages 6613–6629, 2025. 1,\n2, 3, 7\n[20] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue\nCao. Eva-clip: Improved training techniques for clip at scale.\narXiv preprint arXiv:2303.15389, 2023. 1\n[21] Wenbin Wang, Liang Ding, Minyan Zeng, Xiabin Zhou, Li\nShen, Yong Luo, Wei Yu, and Dacheng Tao. Divide, conquer\nand combine: A training-free framework for high-resolution\nimage perception in multimodal large language models. In\nProceedings of the AAAI Conference on Artificial Intelli-\ngence, pages 7907–7915, 2025. 1, 2, 3\n[22] Wenbin Wang, Yongcheng Jing, Liang Ding, Yingjie Wang,\nLi Shen, Yong Luo, Bo Du, and Dacheng Tao. Retrieval-\naugmented perception: High-resolution image perception\nmeets visual rag. arXiv preprint arXiv:2503.01222, 2025.\n1, 2, 3, 6, 7",
    "metadata": {
      "producer": "pikepdf 8.15.1",
      "creator": "arXiv GenPDF (tex2pdf:4177c2c)",
      "creationdate": "",
      "source": "data/raw/2512.02906v2.pdf",
      "file_path": "data/raw/2512.02906v2.pdf",
      "total_pages": 16,
      "format": "PDF 1.7",
      "title": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding",
      "author": "Fan Yang; Kaihao Zhang",
      "subject": "",
      "keywords": "",
      "moddate": "",
      "trapped": "",
      "modDate": "",
      "creationDate": "",
      "page": 8
    }
  },
  {
    "page_content": "[23] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng\nGe, Jinrong Yang, Jianjian Sun, Chunrui Han, and Xiangyu\nZhang.\nVary: Scaling up the vision vocabulary for large\nvision-language model. In European Conference on Com-\nputer Vision, pages 408–424. Springer, 2024. 3\n[24] Penghao Wu and Saining Xie. V?: Guided visual search as\na core mechanism in multimodal llms. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 13084–13094, 2024. 1, 2, 3, 6\n[25] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun,\nTong Xu, and Enhong Chen.\nA survey on multimodal\nlarge language models.\nNational Science Review, 11(12):\nnwae403, 2024. 1, 3\n[26] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang,\nGuanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen,\nJing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue,\nSenbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao\nHuang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng\nNie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu\nGu, Zhiyuan Liu, and Zonghong Dai. Yi: Open foundation\nmodels by 01.ai. CoRR, abs/2403.04652, 2024. 7\n[27] Shi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao\nRan, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han,\nZhiyuan Liu, et al. Visrag: Vision-based retrieval-augmented\ngeneration on multi-modality documents.\narXiv preprint\narXiv:2410.10594, 2024. 3\n[28] Duzhen Zhang, Yahan Yu, Jiahua Dong, Chenxing Li, Dan\nSu, Chenhui Chu, and Dong Yu. Mm-llms: Recent advances\nin multimodal large language models.\nIn Findings of the\nAssociation for Computational Linguistics ACL 2024, pages\n12401–12430, 2024. 3\n[29] Jiarui Zhang, Mahyar Khayatkhoei, Prateek Chhikara, and\nFilip Ilievski. Mllms know where to look: Training-free per-\nception of small visual details with multimodal llms. arXiv\npreprint arXiv:2502.17422, 2025. 1, 2\n[30] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao,\nGuohai Xu, Le Yang, Chao Shen, and Xing Yu.\nDeep-\neyes: Incentivizing” thinking with images” via reinforce-\nment learning. arXiv preprint arXiv:2505.14362, 2025. 1,\n2, 3\n[31] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shen-\nglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su,\nJie Shao, et al. Internvl3: Exploring advanced training and\ntest-time recipes for open-source multimodal models. arXiv\npreprint arXiv:2504.10479, 2025. 3",
    "metadata": {
      "producer": "pikepdf 8.15.1",
      "creator": "arXiv GenPDF (tex2pdf:4177c2c)",
      "creationdate": "",
      "source": "data/raw/2512.02906v2.pdf",
      "file_path": "data/raw/2512.02906v2.pdf",
      "total_pages": 16,
      "format": "PDF 1.7",
      "title": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding",
      "author": "Fan Yang; Kaihao Zhang",
      "subject": "",
      "keywords": "",
      "moddate": "",
      "trapped": "",
      "modDate": "",
      "creationDate": "",
      "page": 9
    }
  },
  {
    "page_content": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image\nUnderstanding\nSupplementary Material\nA. Implement Details of MRD\nFollowing [22], given an input HR image I, it is first parti-\ntioned into smaller image crops based on a predefined crop\nsize, which corresponds to the preferred resolution of the\nretriver. According to the Resolution of HR image, we set\nthe crop resolutions as 112, 224 and 448 for V ∗Bench,\nHR-Bench-4K and HR-Bench-8K respectively. For multi-\nresolution semantic fusion, the ratio between the high and\nlow resolutions is set to k = 2 in all experiments. For\nSliding Window detection, we set window size and step\nas 1232 and 896 for V ∗Bench, 2240 and 1792 for HR-\nBench-4K, 3136 and 2688 for for HR-Bench-8K to balance\nefficiency and accuracy. The weight w of the detection con-\nfidence map is 0.4 by default in semantic detection map fu-\nsion. For the following Retrieved-Exploration Search (RE-\nSearch) process, we adopt the same hyperparameters as the\nbaseline method RAP [22]. In all experiments, the maxi-\nmum search steps are set to 200, and the answering confi-\ndence threshold τ is set to 0.6. In the following hyperpa-\nrameter studies, all other hyperparameters of our MRD use\ntheir default settings unless otherwise specified.\nB. More Experiment Result\nTo analyze the impact of different hyperparameters on per-\nformance, we conduct experiments on MRD using various\nhyperparameter settings, including Crop Resolution, Max-\nimum Search Steps, Detection Weight, and Detection\nWindow Size. We perform these experiments on the V ∗\nBench using both the LLaVA-ov-0.5B and LLaVA-v1.5-7B\nmodels. Unless otherwise specified, all other hyperparame-\nters follow the default settings mentioned in section A.\nB.1. Effect of Crop Resolution\nIn our experiments, we evaluate the effect of different crop\nresolutions on performance. The results are shown in Fig-\nure 6. For the Single Instance Task (Figure 6 (a)), we ob-\nserve that MRD remains highly stable across different reso-\nlutions for both models, with only minor performance fluc-\ntuations, whereas RAP exhibits much larger variations, es-\npecially when using LLaVA-ov-0.5B.\nFor the Cross Instance Task, the performance gap be-\ntween MRD and RAP is relatively small when using\nLLaVA-v1.5-7B. However, with LLaVA-ov-0.5B, MRD is\nlargely unaffected by resolution changes, further demon-\nstrating its robustness. Overall, MRD consistently outper-\nforms RAP across different resolutions and model settings,\nhighlighting the advantages of our approach.\nIn summary, the Multi-resolution Semantic Fusion and\nDetector Enhancement modules in MRD effectively miti-\ngate the interference caused by fragmenting complete ob-\njects across multiple crops when using different crop reso-\nlutions. As a result, our MRD performance is only weakly\ninfluenced by crop resolution and achieves notably better\nresults in the Single Instance Task.\nB.2. Effect of Maximum Search Steps\nThe performance of MRD and RAP under different max-\nimum search steps is shown in Figure 7. In Figure 7 (a),\nfor the single-instance task, MRD consistently outperforms\nRAP across different max step settings on both the LLaVA-\nov-0.5B and LLaVA-v1.5-7B models.\nIn Figure 7 (b), for the Cross Instance Task, MRD is\nslightly inferior to RAP only when using LLaVA-v1.5-\n7B with small max steps. However, as the max step in-\ncreases, MRD surpasses RAP and maintains better perfor-\nmance. Overall, MRD achieves superior results compared\nto RAP. Notably, MRD with LLaVA-ov-0.5B performs only\nmarginally lower than RAP with the powerful LLaVA-v1.5-\n7B model.\nMost importantly, MRD reaches its peak performance\nwith a significantly smaller number of maximum search\nsteps (Max Step = 30). This means that in practical ap-\nplications, MRD can operate effectively with fewer steps,\nachieving high accuracy while reducing search time and im-\nproving efficiency.”\nB.3. Effect of Detection Weight\nThe results of using different detection weights are shown\nin Figure 8. We observe that relying solely on the seman-\ntic similarity map (weight = 0) or solely on the detection\nmap (weight = 1) does not yield optimal performance for\neither task. In contrast, fusing the two maps leads to better\nresults, demonstrating that the semantic similarity map and\ndetection map provide complementary information.\nOverall (Figure 8 (c)), the optimal detection weight\nvaries slightly across models: LLaVA-ov-0.5B achieves its\nbest performance at weight = 0.4, while LLaVA-v1.5-7B\nperforms best at weight = 0.2.\nB.4. Effect of Window Size\nAs shown in Figure 9, adopting different sliding-window\nsizes for object detection also affects the results. Except\nfor the Cross Instance Task with LLaVA-ov-0.5B, using a",
    "metadata": {
      "producer": "pikepdf 8.15.1",
      "creator": "arXiv GenPDF (tex2pdf:4177c2c)",
      "creationdate": "",
      "source": "data/raw/2512.02906v2.pdf",
      "file_path": "data/raw/2512.02906v2.pdf",
      "total_pages": 16,
      "format": "PDF 1.7",
      "title": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding",
      "author": "Fan Yang; Kaihao Zhang",
      "subject": "",
      "keywords": "",
      "moddate": "",
      "trapped": "",
      "modDate": "",
      "creationDate": "",
      "page": 10
    }
  },
  {
    "page_content": "Figure 6. The effect of the resolution of image crops on model performance. Single and Cross represent the attribute recognition and\nspatial reasoning in V ∗Bench. (a) Single-instance Task. (b) Cross-instance Task. (c) Overall Performance.\n.\nFigure 7. The effect of the maximum search steps of MRD and RAP.\nsmaller sliding-window size (Window Size = 896) gener-\nally yields better performance. This is because a smaller\nwindow reduces background interference unrelated to the\ntarget object, leading to more accurate detection results.\nHowever, a smaller window size also means that more\nwindows are required to scan the entire high-resolution im-\nage, resulting in increased computational complexity and\nlonger processing time. Therefore, to balance accuracy and\nefficiency, we select a larger sliding-window size, Window\nSize = 1232, as the default setting.\nB.5. Compared with Other HR Methods\nWe compare our MRD approach with three high-resolution\nprocessing baselines RAP, DC2 and Zoom Eye. DC2 is a\ntraining-free framework that improves MLLM comprehen-\nsion of HR images by dividing them into crops, generat-\ning textual descriptions for each region, and aggregating\nthese descriptions to obtain a more complete understand-\ning. Zoom Eye, on the other hand, uses a tree-search strat-\negy to traverse the hierarchical visual structure of an image,\nenabling efficient identification and extraction of relevant\ninformation.\nAs shown in Table 3, all HR processing methods yield\noverall performance improvements compared with the base-\nline. Among them, our MRD achieves consistently stronger\nresults across most tasks, demonstrating its clear advantage\nover existing approaches.",
    "metadata": {
      "producer": "pikepdf 8.15.1",
      "creator": "arXiv GenPDF (tex2pdf:4177c2c)",
      "creationdate": "",
      "source": "data/raw/2512.02906v2.pdf",
      "file_path": "data/raw/2512.02906v2.pdf",
      "total_pages": 16,
      "format": "PDF 1.7",
      "title": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding",
      "author": "Fan Yang; Kaihao Zhang",
      "subject": "",
      "keywords": "",
      "moddate": "",
      "trapped": "",
      "modDate": "",
      "creationDate": "",
      "page": 11
    }
  },
  {
    "page_content": "Figure 8. The effect of the detection weight in MRD.\nFigure 9. The effect of the detection window size in MRD.\nTable 3. Comparison of MRD with existing works on high-resolution benchmarks. We conduct experiments on V ∗Bench and HR-Bench\nusing LLaVA-v1.5 7B.\nMethod\nV* Bench\nHR-Bench 4K\nHR-Bench 8K\n∆(↑)\nAttribute\nSpatial\nOverall\nFSP\nFCP\nOverall\nFSP\nFCP\nOverall\nLLaVA-v1.5-7B\n43.5\n56.6\n48.7\n38.5\n33.8\n36.1\n33.0\n31.3\n32.1\n-\n-w/ DC2\n49.6\n59.2\n51.6\n45.3\n37.0\n41.1\n36.5\n33.3\n34.9\n+3.5\n-w/ Zoom Eye\n83.5\n82.9\n83.3\n67.8\n38.8\n53.3\n65.5\n36.0\n50.8\n+23.5\n-w/ RAP\n90.4\n96.1\n91.1\n73.8\n40.5\n57.1\n72.3\n35.3\n53.8\n+28.4\n-w/ MRD (ours)\n97.4\n96.1\n95.6\n76.8\n42.7\n59.7\n72.6\n37.2\n54.9\n+31.1\nC. Case Study\nC.1. Single-instance Perception Task Examples\nFigure 10 shows two single-instance perception cases from\neach HR benchmarks using RAP and MRD on LLaVA-\nv1.5-7B. From the first to the last column, we show: the\nHR image, the RAP semantic similarity map, the object de-\ntection confidence map, the MRD semantic–detection fu-\nsion map, the RAP result and the MRD result. From the",
    "metadata": {
      "producer": "pikepdf 8.15.1",
      "creator": "arXiv GenPDF (tex2pdf:4177c2c)",
      "creationdate": "",
      "source": "data/raw/2512.02906v2.pdf",
      "file_path": "data/raw/2512.02906v2.pdf",
      "total_pages": 16,
      "format": "PDF 1.7",
      "title": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding",
      "author": "Fan Yang; Kaihao Zhang",
      "subject": "",
      "keywords": "",
      "moddate": "",
      "trapped": "",
      "modDate": "",
      "creationDate": "",
      "page": 12
    }
  },
  {
    "page_content": "visualization of RAP semantic similarity maps, we can ob-\nserve that due to the crop partitioning, a complete object\nmay be divided across multiple crops, leading to inconsis-\ntencies in semantic similarity among different parts of the\nobject. This inconsistency interferes with the subsequent\nretrieval process. For example, in the second case of HR-\nBench4K, RAP only retrieves the right half of the speed-\nlimit sign, resulting in an incorrect final prediction. In addi-\ntion, the semantic similarity maps contain many false posi-\ntives; for instance, in the first case of HR-Bench8K, the sky\nregion—irrelevant to the query—shows undesirably high\nsimilarity scores.\nMRD addresses these issues by using multi-resolution\nsemantic fusion to correct the semantic inconsistencies\nacross different parts of the object, ensuring its complete-\nness. Moreover, by incorporating an object detection model\nto directly localize the target, MRD reinforces the similarity\nof the true target region while suppressing false positives.\nAs shown in the figure, the MRD semantic–detection fu-\nsion map exhibits much clearer contrast between the target\nand irrelevant regions compared with RAP, significantly re-\nducing false positives and enabling more accurate retrieval\nof the target-related crops during the search process.\nC.2. Cross-instance Perception Task Examples\nFigure 11 shows two cross-instance perception cases from\neach HR benchmarks using RAP and MRD on LLaVA-\nv1.5-7B. In the cross-instance task, the retrieval results\nshow that RAP often retains only a subset of the target ob-\njects while ignoring others when multiple objects need to be\nlocalized. For example, in the first case of V ∗Bench, RAP\ncompletely misses the pink umbrella. This omission be-\ncomes more pronounced when there is a large size discrep-\nancy between different target objects. As seen in the sec-\nond case of V ∗Bench and the two cases from HRBench-\n8K, RAP tends to keep only the larger primary object while\nneglecting the smaller ones. Similar issues also appear in\ncounting scenarios (e.g., the two cases in HRBench-4K),\nwhere RAP identifies only a few among multiple instances.\nIn contrast, MRD leverages object detection to simul-\ntaneously detect all target objects, ensuring that even small\nobjects are preserved to the greatest extent. This gives MRD\na clear advantage in cross-instance perception tasks.",
    "metadata": {
      "producer": "pikepdf 8.15.1",
      "creator": "arXiv GenPDF (tex2pdf:4177c2c)",
      "creationdate": "",
      "source": "data/raw/2512.02906v2.pdf",
      "file_path": "data/raw/2512.02906v2.pdf",
      "total_pages": 16,
      "format": "PDF 1.7",
      "title": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding",
      "author": "Fan Yang; Kaihao Zhang",
      "subject": "",
      "keywords": "",
      "moddate": "",
      "trapped": "",
      "modDate": "",
      "creationDate": "",
      "page": 13
    }
  },
  {
    "page_content": "Figure 10. Qualitative examples of Single-instance Perception task. We conduct experiments using LLaVA-v1.5-7B on three HR Bench-\nmarks.",
    "metadata": {
      "producer": "pikepdf 8.15.1",
      "creator": "arXiv GenPDF (tex2pdf:4177c2c)",
      "creationdate": "",
      "source": "data/raw/2512.02906v2.pdf",
      "file_path": "data/raw/2512.02906v2.pdf",
      "total_pages": 16,
      "format": "PDF 1.7",
      "title": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding",
      "author": "Fan Yang; Kaihao Zhang",
      "subject": "",
      "keywords": "",
      "moddate": "",
      "trapped": "",
      "modDate": "",
      "creationDate": "",
      "page": 14
    }
  },
  {
    "page_content": "Figure 11. Qualitative examples of Cross-instance Perception task.We conduct experiments using LLaVA-v1.5-7B on three HR Bench-\nmarks.",
    "metadata": {
      "producer": "pikepdf 8.15.1",
      "creator": "arXiv GenPDF (tex2pdf:4177c2c)",
      "creationdate": "",
      "source": "data/raw/2512.02906v2.pdf",
      "file_path": "data/raw/2512.02906v2.pdf",
      "total_pages": 16,
      "format": "PDF 1.7",
      "title": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding",
      "author": "Fan Yang; Kaihao Zhang",
      "subject": "",
      "keywords": "",
      "moddate": "",
      "trapped": "",
      "modDate": "",
      "creationDate": "",
      "page": 15
    }
  }
]